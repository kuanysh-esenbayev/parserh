"Представим себе, что в один прекрасный день вам пришла в голову идея процессора собственной, ни на что не похожей архитектуры, и вам очень захотелось эту идею реализовать «в железе». К счастью, в этом нет ничего невозможного. Немного верилога, и вот ваша идея реализована. Вам уже снятся прекрасные сны про то, как Intel разорилась, Microsoft спешно переписывает Windows под вашу архитектуру, а Linux-сообщество уже написало под ваш микропроцессор свежую версию системы с весьма нескучными обоями.  Однако, для всего этого не хватает одной мелочи: компилятора!   Да, я знаю, что многие не считают наличие компилятора чем-то важным, считая, что все должны программировать строго на ассемблере. Если вы тоже так считаете, я не буду с вами спорить, просто не читайте дальше.  Если вы хотите, чтобы для вашей оригинальной архитектуры был доступен хотя бы язык С, прошу под кат.  В статье будет рассматриваться применение инфраструктуры компиляторов LLVM для построения собственных решений на её основе.  Область применения LLVM не ограничивается разработкой компиляторов для новых процессоров, инфраструктура компиляторов LLVM также может применяться для разработки компиляторов новых языков программирования, новых алгоритмов оптимизации и специфических инструментов статического анализа программного кода (поиск ошибок, сбор статистики и т.п.).  Например, вы можете использовать какой-то стандартный процессор (например, ARM) в сочетании с специализированным сопроцессором (например, матричный FPU), в этом случае вам может понадобиться модифицировать существующий компилятор для ARM так, чтобы он мог генерировать код для вашего FPU.  Также интересным применением LLVM может быть генерация исходных текстов на языке высокого уровня («перевод» с одного языка на другой). Например, можно написать генератор кода на Verilog по исходному коду на С.        КДПВ      Почему LLVM?  На сегодняшний день существует только два реалистичных пути разработки компилятора для собственной архитектуры: использование GCC либо использование LLVM. Другие проекты компиляторов с открытым исходным кодом либо не достигли той степени развития, как GCC и LLVM, либо устарели и перестали развиваться, они не обладают развитыми алгоритмами оптимизации, и могут не обеспечивать полной совместимости даже со стандартом языка С, не говоря уже о поддержке других языков программирования. Разработка собственного компилятора “с нуля"", это весьма нерациональный путь, т.к. существующие опенсорсные решения уже реализуют фронтенд компилятора с множеством весьма нетривиальных алгоритмов оптимизации, которые, к тому же, хорошо протестированы и используются уже длительное время.  Какой из этих двух open-source проектов выбрать в качестве основы для своего компилятора? GCC (GNU Compiler Collection) является более старым проектом, первый релиз которого состоялся в 1987 году, его автором является Ричард Столлман, известный деятель open-source движения [4]. Он поддерживает множество языков программирования: C, C++, Objective C, Fortran, Java, Ada, Go. Также существуют фронтенды для многих других языков программирования, не включенных в основную сборку. Компилятор GCC поддерживает большое количество процессорных архитектур и операционных систем, и является в настоящее время наиболее распространённым компилятором. Сам GCC написан на языке С (в комментариях меня поправили, что он уже по большей части переписан на С++).  LLVM гораздо «моложе», его первый релиз состоялся в 2003 году, он (а точнее, его фронтенд Clang) поддерживает языки программирования C, C++, Objective-C and Objective-C++, и также имеет фронтенды для языков Common Lisp, ActionScript, Ada, D, Fortran, OpenGL Shading Language, Go, Haskell, Java bytecode, Julia, Swift, Python, Ruby, Rust, Scala, C# и Lua. Он разработан в университете Иллинойса, в США, и является основным компилятором для разработки под операционную систему OS X. LLVM написан на языке С++ (С++11 для последних релизов) [5].    Относительная «молодость» LLVM не является недостатком, он достаточно зрелый, чтобы в нём не было критических багов, и при этом он не несёт в себе огромного груза устаревших архитектурных решений, как GCC. Модульная структура компилятора позволяет использовать фронтенд LLVM-GCC, который обеспечивает полную поддержку стандартов GCC, при этом генерация кода целевой платформы будет осуществляться LLC (бэкенд LLVM). Также можно использовать Clang — оригинальный фронтенд LLVM.  LLVM хорошо документирован, и для него большое количество примеров кода.    Модульная архитектура компилятора  Инфраструктура компиляторов LLVM состоит из различных инструментов, рассматривать их все в рамках данной статьи не имеет смысла. Ограничимся основными инструментами, которые образуют как таковой компилятор.  Компилятор LLVM, как и некоторые другие компиляторы, состоит из фронтенда, оптимизатора (миддленда), и бэкенда. Такая структура позволяет разделить разработку компилятора нового языка программирования, разработку методов оптимизации и разработку кодогенератора для конкретного процессора (такие компиляторы называют «перенацеливаемыми», retargetable).   Связующим звеном между ними является промежуточный язык LLVM, ассемблер «виртуальной машины». Фронтенд (например, Clang) преобразует текст программы на языке высокого уровня в текст на промежуточном языке, оптимизатор (opt) производит над ним различные оптимизации, а бэкенд (llc) генерирует код целевого процессора (на ассемблере или непосредственно в виде бинарного файла). Помимо этого, LLVM может работать в режиме JIT (just-in-time) компиляции, когда компиляция происходит непосредственно во время исполнения программы.  Промежуточное представление программы может быть как в виде текстового файла на языке ассемблера LLVM, так и в виде двоичного формата, «биткода». По умолчанию clang генерирует именно биткод (файл .bc), но для отладки и изучения LLVM нам нужно будет генерировать текстовое промежуточное представление на ассемблере LLVM (он также называется IR-кодом, от слов Intermediate Representation, промежуточное представление).      Рис. 1. Модульная архитектура компилятора    Кроме вышеперечисленных программ, LLVM включает в себя и другие утилиты [6].  Итак, напишем простейшую программу на C.  int foo(int x, int y) {   return x + y; }   И скомпилируем её:  clang-3.5 -c add.c -O0 --target=xcore -emit-llvm -S -o add_o0.ll    Некоторые пояснения:  -c add.c — входной файл;  -O0 — уровень оптимизации 0, оптимизация отсутствует;  --target=xcore — ядро процессора xcore не имеет каких-либо сложных особенностей при компиляции в IR-код, поэтому является идеальным объектом для исследований. Это ядро имеет разрядность 32, и clang выравнивает все переменные по границам 32-битных слов;  -emit-llvm -S — указание clang-у сгенерировать файл llvm в текстовом виде (на ассемблере LLVM);  -o add_o0.ll — выходной файл  Посмотрим на результат:  ; ModuleID = 'add.c'  target datalayout = ""e-m:e-p:32:32-i1:8:32-i8:8:32-i16:16:32-i64:32-f64:32-a:0:32-n32""  target triple = ""xcore""    ; Function Attrs: nounwind  define i32 @foo(i32 %x, i32 %y) #0 {  %1 = alloca i32, align 4  %2 = alloca i32, align 4  store i32 %x, i32* %1, align 4  store i32 %y, i32* %2, align 4  %3 = load i32* %1, align 4  %4 = load i32* %2, align 4  %5 = add nsw i32 %3, %4  ret i32 %5  }    attributes #0 = { nounwind ""less-precise-fpmad""=""false"" ""no-frame-pointer-elim""=""false"" ""no-infs-fp-math""=""false"" ""no-nans-fp-math""=""false"" ""stack-protector-buffer-size""=""8"" ""unsafe-fp-math""=""false"" ""use-soft-float""=""false"" }    !llvm.ident = !{!0}  !xcore.typestrings = !{!1}    !0 = metadata !{metadata !""Ubuntu clang version 3.5.0-4ubuntu2~trusty2 (tags/RELEASE_350/final) (based on LLVM 3.5.0)""}  !1 = metadata !{i32 (i32, i32)* @foo, metadata !""f{si}(si,si)""}    Выглядит довольно сложно, не так ли? Однако давайте разберёмся, что тут написано. Итак:  target datalayout = «e-m:e-p:32:32-i1:8:32-i8:8:32-i16:16:32-i64:32-f64:32-a:0:32-n32»  описание разрядности переменных и самых основных особенностей архитектуры. e- little-endian архитектура. Для big-endian здесь была бы буква E. m:e — mangling, соглашение о преобразовании имён. Нам пока это не понадобится. p:32:32 — указатели имеют 32 бита и выровнены по границам 32-битных слов. i1:8:32 — булевые переменные (i1) выражаются 8-ибитными значениями и выровнены по границам 32-битных слов. Далее аналогично для целочисленных переменных i8 — i64 (от 8 до 64 бит соответственно), и f64 — для вещественных переменных двойной точности. a:0:32 — агрегатные переменные (т.е. массивы и структуры) имеют выравнивание 32 бита, n32 — разрядность чисел, обрабатываемых АЛУ процессора (native integer width).  Далее следует название таргета (т.е. целевого процессора): target triple = «xcore». Хотя код IR часто называют «машинно-независимым», на самом деле мы видим, это не так. Он отражает некоторые особенности целевой архитектуры. Это является одной из причин, по которой мы должны указывать целевую архитектуру не только для бэкенда, но и для фронтенда.  Далее следует код функции foo:  define i32 @foo(i32 %x, i32 %y) #0 {  %1 = alloca i32, align 4  %2 = alloca i32, align 4  store i32 %x, i32* %1, align 4  store i32 %y, i32* %2, align 4  %3 = load i32* %1, align 4  %4 = load i32* %2, align 4  %5 = add nsw i32 %3, %4  ret i32 %5  }    Код довольно громоздкий, хотя исходная функция очень проста. Вот что он делает.  Сразу отметим, что все имена переменных в LLVM имеют префикс либо % (для локальных переменных), либо @ — для глобальных. В данном примере все переменные локальные.  %1 = alloca i32, align 4 — выделяет на стеке 4 байта для переменной, указателем на эту область является указатель %1.  store i32 %x, i32* %1, align 4 — копирует в выделенную область один из аргументов функции (%x).  %3 = load i32* %1, align 4 — извлекает значение в переменную %3. Теперь в %3 хранится локальная копия %x.  Делает то же самое для аргумента %y  %5 = add nsw i32 %3, %4 — складывает локальные копии %x и %y, помещает результат в %5. Есть ещё атрибут nsw, но он пока для нас не важен.  Возвращает значение %5.  Из приведённого примера видно, что при нулевом уровне оптимизации clang генерирует код, буквально следуя исходному коду, создаёт локальные копии всех аргументов и не делает каких-либо попыток удалить избыточные команды. Это может показаться плохим свойством компилятора, но на самом деле это очень полезная особенность при отладке программ и при отладке кода самого компилятора.  Посмотрим, что произойдёт, если поменять уровень оптимизации на O1:  define i32 @foo(i32 %x, i32 %y) #0 {  %1 = add nsw i32 %y, %x  ret i32 %1  }    Мы видим, что ни одной лишней команды не осталось. Теперь программа складывает непосредственно аргументы функции и возвращает результат.  Есть и более высокие уровни оптимизации, но в этом конкретном случае они не дадут лучшего результата, т.к. максимальный уровень оптимизации уже достигнут.   Остальная часть LLVM-кода (атрибуты, метаданные), несёт в себе служебную информацию, которая пока нам неинтересна.    Структура кода LLVM  Структура кода LLVM очень проста. Код программы состоит из модулей, компилятор обрабатывает по одному модулю за раз. В модуле есть глобальные объявления (переменные, константы и объявления заголовков функций, находящихся в других модулях) и функции. Функции имеют аргументы и возвращаемые типы. Функции состоят из базовых блоков. Базовый блок — это последовательность команд ассемблера LLVM, имеющая одну точку входа и одну точку выхода. Базовый блок не содержит внутри себя никаких ветвлений и циклов, он выполняется строго последовательно от начала до конца и должен заканчиваться терминирующей командой (возвратом из функции или переходом на другой блок).  И, наконец, базовый блок состоит из команд ассемблера. Список команд приведён в документации на LLVM [7].  Итак, ещё раз: базовый блок имеет одну точку входа, помеченную меткой, и обязательно должен заканчиваться командой безусловного перехода br или командой возврата ret. Перед ними может быть команда условного перехода, в этом случае она должна быть непосредственно перед терминирующей командой. Базовый блок имеет список predecessors — базовых блоков, из которых управление может приходить на него, и successors — базовых блоков, которым он может передавать управление. На основе этой информации строится CFG — Control Flow Graph, граф потока управления, важнейшая структура, представляющая программу в компиляторе.  Рассмотрим тестовый пример на языке С:  Пусть исходный код на языке С имеет цикл:  //функция вычисляет сумму 10 элементов массива int for_loop(int x[]) {   int sum = 0;   for(int i = 0; i < 10; ++i) {     sum += x[i];   }   return sum; }   Его CFG имеет вид:    Ещё одним видом графов в LLVM является DAG — directed acyclic graph, направленный ациклический граф, представляющий собой базовый блок.  Он представляет команды ассемблера и зависимости между ними. На рисунке ниже приведён DAG базового блока, представляющий тело цикла в примере выше, при уровне оптимизации -O1:      SSA-форма  Ключевой особенностью IR-кода, отличающей его от языков высокого уровня является то, что он представлен в так называемой SSA-форме (Static Single Assignment form). Эта особенность очень важна для понимания при разработке компилятора на платформе LLVM, поэтому уделим ей некоторое внимание. Если формулировать кратко, то в SSA-форме каждой переменной значение присваивается ровно один раз и только в одной точке программы. Все алгоритмы оптимизации и преобразования IR-кода работают только с такой формой.  Однако, как преобразовать обычную программу на языке высокого уровня в такую форму? Ведь в обычных языках программирования значение переменной может присваиваться несколько раз в разных местах программы, или, например, в цикле.  Для реализации такого поведения программы используется один из двух приемов. Первый прием заключается в использовании пар команд load/store, как в вышеприведённом коде. Ограничение единственного присваивания распространяется только на именованные переменные LLVM, и не распространяется на ячейки памяти, на которые ссылаются указатели. То есть, можно неограниченное количество раз производить запись в ячейку памяти командой store, и формальное правило SSA будет соблюдено, так как указатель на эту ячейку не меняется. Этот способ используется обычно при уровне оптимизации -O0, и мы не будем на нём подробно останавливаться. Второй прием использует φ-функции, ещё одну интересную концепцию IR-кода.    Код в SSA-форме: load/store  Воспользуемся тестовым примером из предыдущего раздела.  Скомпилируем с уровнем оптимизации -O0:  define i32 @for_loop(i32* %x) #0 {  %1 = alloca i32*, align 4  %sum = alloca i32, align 4  %i = alloca i32, align 4  store i32* %x, i32** %1, align 4  store i32 0, i32* %sum, align 4  store i32 0, i32* %i, align 4  br label %2    ; :2 ; preds = %12, %0  %3 = load i32* %i, align 4  %4 = icmp slt i32 %3, 10  br i1 %4, label %5, label %15    ; :5 ; preds = %2  %6 = load i32* %i, align 4  %7 = load i32** %1, align 4  %8 = getelementptr inbounds i32* %7, i32 %6  %9 = load i32* %8, align 4  %10 = load i32* %sum, align 4  %11 = add nsw i32 %10, %9  store i32 %11, i32* %sum, align 4  br label %12    ; :12 ; preds = %5  %13 = load i32* %i, align 4  %14 = add nsw i32 %13, 1  store i32 %14, i32* %i, align 4  br label %2    ; :15 ; preds = %2  %16 = load i32* %sum, align 4  ret i32 %16  }    Здесь мы видим то, о чем было написано выше: переменная цикла, это просто ячейка памяти, на которую ссылается указатель %i.    Код в SSA-форме: φ-функции  Теперь попробуем уровень оптимизации O1:  define i32 @for_loop(i32* nocapture readonly %x) #0 {  br label %1    ; :1 ; preds = %1, %0  %i.02 = phi i32 [ 0, %0 ], [ %5, %1 ]  %sum.01 = phi i32 [ 0, %0 ], [ %4, %1 ]  %2 = getelementptr inbounds i32* %x, i32 %i.02  %3 = load i32* %2, align 4, !tbaa !2  %4 = add nsw i32 %3, %sum.01  %5 = add nsw i32 %i.02, 1  %exitcond = icmp eq i32 %5, 10  br i1 %exitcond, label %6, label %1    ; :6 ; preds = %1  ret i32 %4  }    Здесь мы видим, что переменной цикла является %i.02 (имена переменных в LLVM могут содержать точки), и это не указатель, а обычная 32-битная переменная, а присваивание значения происходит с помощью функции phi i32 [ 0, %0 ], [ %5, %1 ]. Это означает, что функция примет значение 0, если переход произошёл с базового блока %0 (первый базовый блок в функции), и значение переменной %5, если переход произошёл с базового блока %1 (т.е. с выходной точки этого же базового блока). Таким образом, генератор IR-кода избавился от двух присваиваний переменной, строго следуя формальным правилам SSA. Далее видно, что сходным образом происходит присваивание переменной %sum.01.  Итак, смысл φ-функции состоит в том, что её значение зависит от того, из какого базового блока был произведён переход на неё. φ-функции могут находиться только в начале базового блока. Если их несколько, они должны следовать непрерывно, начиная с первой инструкции базового блока.    Moar optimization!  Что будет при уровне оптимизации -O2?  define i32 @for_loop(i32* nocapture readonly %x) #0 {  %1 = load i32* %x, align 4, !tbaa !2  %2 = getelementptr inbounds i32* %x, i32 1  %3 = load i32* %2, align 4, !tbaa !2  %4 = add nsw i32 %3, %1  %5 = getelementptr inbounds i32* %x, i32 2  %6 = load i32* %5, align 4, !tbaa !2  %7 = add nsw i32 %6, %4  %8 = getelementptr inbounds i32* %x, i32 3  %9 = load i32* %8, align 4, !tbaa !2  %10 = add nsw i32 %9, %7  %11 = getelementptr inbounds i32* %x, i32 4  %12 = load i32* %11, align 4, !tbaa !2  %13 = add nsw i32 %12, %10  %14 = getelementptr inbounds i32* %x, i32 5  %15 = load i32* %14, align 4, !tbaa !2  %16 = add nsw i32 %15, %13  %17 = getelementptr inbounds i32* %x, i32 6  %18 = load i32* %17, align 4, !tbaa !2  %19 = add nsw i32 %18, %16  %20 = getelementptr inbounds i32* %x, i32 7  %21 = load i32* %20, align 4, !tbaa !2  %22 = add nsw i32 %21, %19  %23 = getelementptr inbounds i32* %x, i32 8  %24 = load i32* %23, align 4, !tbaa !2  %25 = add nsw i32 %24, %22  %26 = getelementptr inbounds i32* %x, i32 9  %27 = load i32* %26, align 4, !tbaa !2  %28 = add nsw i32 %27, %25  ret i32 %28  }    Оптимизатор развернул цикл. Вообще, оптимизатор IR-кода в LLVM очень интеллектуален, он умеет не только разворачивать циклы, но и упрощать нетривиальные конструкции, вычислять константные значения, даже если они не присутствуют в коде в явном виде, и делать другие сложные преобразования кода.     Компоновка IR-кода  Реальные программы состоят не из одного модуля. Традиционный компилятор компилирует модули по отдельности, превращая их в объектные файлы, а затем передаёт их компоновщику (линкеру), который объединяет их в один исполняемый файл. LLVM тоже позволяет так делать.  Но LLVM имеет также возможность компоновки IR-кода. Проще всего рассмотреть это на примере. Пусть есть два исходных модуля: foo.c и bar.c:  //bar.h #ifndef BAR_H #define BAR_H int bar(int x, int k); #endif  //bar.c int bar(int x, int k) {   return x * x * k; }  //foo.c #include ""bar.h"" int foo(int x, int y) {   return bar(x, 2) + bar(y, 3); }   Если программа будет скомпилирована «традиционным» образом, то оптимизатор не сможет сделать с ней практически ничего: при компиляции foo.c компилятор не знает, что находится внутри функции bar, и может поступить единственным очевидным способом, вставить вызовы bar().  Но если мы скомпонуем IR-код, то мы получим один модуль, который после оптимизации с уровнем -O2 будет выглядеть так (для ясности заголовок модуля и метаданные опущены):  define i32 @foo(i32 %x, i32 %y) #0 {  %1 = shl i32 %x, 1  %2 = mul i32 %1, %x  %3 = mul i32 %y, 3  %4 = mul i32 %3, %y  %5 = add nsw i32 %4, %2  ret i32 %5  }    ; Function Attrs: nounwind readnone  define i32 @bar(i32 %x, i32 %k) #0 {  %1 = mul nsw i32 %x, %x  %2 = mul nsw i32 %1, %k  ret i32 %2  }    Здесь видно, что в функции foo не происходит никаких вызовов, компилятор перенёс в неё содержимое bar() полностью, попутно подставив константные значения k. Хотя функция bar() осталась в модуле, она будет исключена при компиляции исполняемого файла, при условии, что она не вызывается нигде больше в программе.  Нужно отметить, что в GCC также есть возможность компоновки и оптимизации промежуточного кода (LTO, link-time optimization) [6].  Разумеется, оптимизация в LLVM не исчерпывается оптимизацией IR-кода. Внутри бэкенда также происходят различные оптимизации на разных стадиях преобразования IR-кода в машинное представление. Часть таких оптимизаций LLVM производит самостоятельно, но разработчик бэкенда может (и должен) разработать собственные алгоритмы оптимизации, которые позволят в полной мере использовать особенности архитектуры процессора.    Генерация кода целевой платформы  Разработка компилятора для оригинальной архитектуры процессора, это, в основном, разработка бэкенда. Вмешательство в алгоритмы фронтенда, как правило, не является необходимым, или, во всяком случае, требует весьма веских оснований. Если проанализировать исходный код Clang, можно увидеть, что большая часть «особенных» алгоритмов приходится на процессоры x86 и PowerPC с их нестандартными форматами вещественных чисел. Для большинства других процессоров нужно указать только размеры базовых типов и endianness (big-endian или little-endian). Чаще всего можно просто найти аналогичный (в плане разрядности) процессор среди множества поддерживаемых.  Генерация кода для целевой платформы происходит в бэкенде LLVM, LLC. LLC поддерживает множество различных процессоров, и вы можете на его основе сделать генератор кода для вашего собственного оригинального процессора. Эта задача упрощается ещё и благодаря тому, что весь исходный код, включая модули для каждой поддерживаемой архитектуры, полностью открыт и доступен для изучения.  Именно генератор кода для целевой платформы (таргета) является наиболее трудоёмкой задачей при разработке компилятора на основе инфрастуктуры LLVM. Я решил не останавливаться подробно здесь на особенностях реализации бэкенда, так как они существенным образом зависят от архитектуры целевого процессора. Впрочем, если у уважаемой аудитории хабра возникнет интерес к данной теме, я готов описать ключевые моменты разработки бэкенда в следующей статье.    Заключение  В рамках небольшой статьи нельзя рассмотреть подробно ни архитектуру LLVM, ни синтаксис языка LLVM IR, ни процесс разработки бэкенда. Однако эти вопросы подробно освещаются в документации. Автор скорее старался дать общее представление об инфраструктуре компиляторов LLVM, сделав упор на то, что эта платформа является современной, мощной, универсальной, и независимой ни от входного языка, ни от целевой архитектуры процессора, позволяя реализовать и то, и другое по желанию разработчика.  Помимо рассмотренных, LLVM содержит и другие утилиты, однако их рассмотрение выходит за рамки статьи.  LLVM позволяет реализовать бэкенд для любой архитектуры (см. примечание), включая архитектуры с конвейеризацией, с внеочередным выполнением команд, с различными вариантами параллелизации, VLIW, для классических и стековых архитектур, в общем, для любых вариантов.  Вне зависимости от того, насколько нестандартные решения лежат в основе процессорной архитектуры, это всего лишь вопрос написания большего или меньшего объёма кода.   примечаниедля любой, в пределах разумного. Вряд ли возможно реализовать компилятор языка С для 4-хбитной архитектуры, т.к. стандарт языка явно требует, чтобы разрядность была не меньше 8.      Литература    Компиляторы  [1] Книга дракона  [2] Вирт Н. Построение компиляторов  GCC  [3] gcc.gnu.org — сайт проекта GCC  [4] Richard M. Stallman and the GCC Developer Community. GNU Compiler Collection Internals  LLVM  [5] http://llvm.org/ — сайт проекта LLVM  [6] http://llvm.org/docs/GettingStarted.html Getting Started with the LLVM System  [7] http://llvm.org/docs/LangRef.html LLVM Language Reference Manual  [8] http://llvm.org/docs/WritingAnLLVMBackend.html Writing An LLVM Backend  [9] http://llvm.org/docs/WritingAnLLVMPass.html Writing An LLVM Pass  [10] Chen Chung-Shu. Creating an LLVM Backend for the Cpu0 Architecture  [11] Mayur Pandey, Suyog Sarda. LLVM Cookbook  [12] Bruno Cardoso Lopes. Getting Started with LLVM Core Libraries  [13] Suyog Sarda, Mayur Pandey. LLVM Essentials  Автор будет рад ответить на ваши вопросы в комментариях и в личке.   Просьба обо всех замеченных опечатках сообщать в личку. Заранее спасибо.","llvm, компиляторы, процессоры",LLVM: компилятор своими руками. Введение
"В этом материале мы собрали полезные статьи об игровой разработке, создании озвучки и арта. Все самое интересное — в нашем дайджесте.        Создание игр     Создаем первую игру с Game Maker: Studio.   Кодим простые состояния игры.   Как написать шейдер дыма?   Руководство для начинающих по созданию уровней в игре.   Что понадобится для создания первой мобильной игры.   Написание игрового движка с нуля: графические библиотеки.   Разрушающийся тигр, затаившийся каньон – заставляем персонажей растворяться в воздухе.   Введение в технологию Intel RealSense для разработчиков игр.   Быстрая подсказка: как рендерить в текстуру в Three.js.   Модель взаимодействия воды с лодками в видеоиграх (часть 2).   Дизайн эпизодических игр (часть 1, часть 2).   Список действий: простой, гибкий, расширяемый искусственный интеллект.   Как сделать пошаговую RPG при помощи Phaser (часть 2).   Управление поведением персонажей игры с течением времени.   Обучающие 60-секундные ролики: веб-дизайн, фото и видео, дизайн и иллюстрация, код.   Метод Super Mario World: понимание умений персонажа.   Основы работы с архитектурой приложений.   Как использовать тайловое побитовое маскирование для автоматического формирования макетов уровней.   Видео: дизайн игр с бесконечным геймплеем.   Основы игрового баланса: случайность и вероятность наступления разных событий.   Разработчики из компании Crytek сделали небольшое обновление игрового 3D движка CryEngine до версии 3.8.6.   Что нового в Ungine SDK 2.1.1.   Варианты применения процедурной генерации.   Вышло обновление Xenko Game Engine.   RAPTOR Game Engine был обновлен до версии 0.5.   Вышла вторая альфа-версия среды DevelNext.   План обновлений CryEngine на 2016 год.   Релиз Blend4Web 16.01.   Новые фундаментальные книги для разработчиков 3D движков.   Планы разработчиков Blend4Web на 2016 год.   Релиз стабильной сборки Construct 2 r221.    Аудио     Расшифровка аудио для XAudio2 при помощи Microsoft Media Foundation.   Взаимосвязь игровой музыки и живого исполнения.   Адаптивная музыка в WWISE и FMOD.   VR для игрового композитора – мастерство и рабочий процесс.   Создание звука и музыки в играх жанра хоррор.   Влияние музыки на восприятие сложности.   Минималистичный саунд-дизайн игры The Witness.   Почему вы не должны экономить на аудио вашей игры – взгляд продюсера.    Это интересно     Основы аналитики и исследования рынка для инди-разработчиков.   Джонатан Блоу о дизайне игры The Witness и состоянии рынка инди-игр.   Релиз инди-игры на трех консолях одновременно и финансовый провал.   Что такое пиксель-арт?   Рецензия на Resident Evil Zero.   Ведущий программист: меньше кода, больше работы с людьми.   Эффект Кулешова: Бэтмен голоден?   7 применений процедурной генерации, которые должны изучить все разработчики.   Рецензия на игру The Witness.   Сторителлинг в синематиках Middle-earth: Shadow of Mordor.   Видео: создание подвижной и сильной анимации для игры Skullgirls.   Два важных отчета от SuperDataResearch и Sensortower о доходах топ игр в 2015 году.   Игровой аудит, или Как вытащить на гору «велосипед с квадратными колесами».   Пиксельный Indie Battle между Dropsy и Superbrothers: Sword & Sworcery EP.   Менеджер киберспортивного направления Riot Games Владимир Торцов: «Доход киберспортивной команды СНГ по League of Legends в суперуспешный сезон превысит 90 млн рублей».   «Три года разработки окупились за первые 49 часов продаж», – заявил Владимир Пискунов, основатель студии Bitbox, создававшей популярный в Steam проект Life is Feudal: Your Own.   Исследование процесса подготовки к выходу на Steam.   Сколько зарабатывают сотрудники игровой индустрии в России, Украине и Белоруссии – опрос от блогера Сергея Галёнкина.   Гейминг без видеокарты всё еще откладывается.   60 фактов о Metal Gear Solid V: The Phantom Pain.   Возвращаясь к классике: как создавалась игра Half-Life 2.   Доходы игровой индустрии достигнут $115 млрд к 2020 году.   История проекта Punch Club: от шутливого концепта до топа продаж в Steam.   7 признаков успешной игровой студии от директора по дизайну Zynga Михаила Каткофа.   Реалити-шоу по разработке игр.   Итоги и прогнозы в игровой индустрии от Сергея Галёнкина.   Интервью: Алекс Ничипорчик, издатель игры Punch Club, заработавшей больше $1 млн за 10 дней.   Как и почему растет индустрия киберспорта.   10 самых странных консолей коллекционера с reddit.com.   Леонид Сиротин: «Геймдизайнеров не существует».   Обзор новой игры от Supercell: MOBA-элементы, монетизация и удержание пользователей в Clash Royale.   Выход Google Play в Китай, падение продаж консолей и предзаказ мобильных игр.   Рецензия на Punch Club от «Отвратительных мужиков».   Почему Fallout 4 мог стать игрой года, но не стал.    Подкасты     Видеоигры и наше здоровье.   Подкаст Сергея Галёнкина с создателями Punch Club о пути игры от идеи и прототипа до успешного релиза.   Сергей Галёнкин об управлении проектами.","game development, дайджест, игры, игровая индустрия, новости, январь, обзоры, новинки",Дайджест игровой разработки
"Лет 10 назад, когда я только начинал свой путь в игровой индустрии, слово инди ещё никто не произносил, а игровые корпорации на постсоветском пространстве казались всем мифической сказкой. В те прекрасные времена, когда на игровом рынке только расцветали первые российские браузерки, мы с другом начали делать свой проект. Мы не считали себя предпринимателями или стартаперами. Нет. Вчерашние студенты, обычные игроки, фанатеющие по Варкрафту, Героям и другим классическим играм. Сегодня я хочу поделиться с вами личным опытом, полученным за время инди-разработки браузерной игры с нуля. Начинающие разработчики, это статья для вас.            Итак, два молодых студента, обладающие базовыми навыками программирования, вдохновлённые крутыми играми, решили создать свой проект. Мы задумали игру колоссальных масштабов. Эдакий World of Warcraft в браузере.    Опыт № 1. Трезво оценивайте свои силы.    Мы не только с энтузиазмом взялись за работу, которую нам никогда не осилить, но и были абсолютно уверены в её успехе и своих возможностях! 100500 абилок, 1000 фичей и конечно же овер 9000 героев – это то, что больше всего не нужно инди-проекту. Не осознавая масштабов и никогда не слышав о слове «планирование», мы принялись за работу. Будучи опытными игроками, мы были уверены, что этого достаточно чтобы сделать свою игру. Дима, мой партнёр по разработке, на тот момент уже слышал такие слова как альфа-тест и имел в своей голове чёткую методологию – запустим альфу, а пилим контент, потом бета и деньги. Обречённость на успех.    Опыт № 2. Нельзя недооценивать важность методологии.    Начитавшись статей, я уже знал, что гейм-дизайн начинается с диздоков. К концу первого года разработки они у нас появились.  Мы работали много и усердно. И через месяц получили играбельный прототип: перемещение по карте локации, текстовый замок на менюшках и пошаговую боёвку без графики. Сделано всё было на коленках и в целом это было оправдано. Теперь я понимаю, что так и нужно писать прототипы. Вот только потом их выкидывают, чтобы не тащить в основной код костылей и велосипедов. Момент принятия прототипа ознаменует начала этапа, которым сейчас принято называть Vertical Slice. К слову, совсем скоро 2 марта я расскажу в деталях о документации и методологии на открытой лекции. Подробнее об этом в другой статье на хабре.    Опыт № 3. Не бойтесь отправить прототип в корзину.    Ну а много лет назад вместо того, чтобы сказать: «Отлично! Мы придумали крутую игру», — выкинуть прототип и начать разработку, мы продолжили работу над ним, превращая его в боевую версию.  Работа продолжалась ещё много месяцев: графика, контент, клиентская часть на HTML / JS и серверная на PHP. И наконец 7 января 2008 был запущен долгожданный альфа-тест. Мы разослали приглашения всем друзьям, распостили информацию о запуске по известным нам форумам, и ждали наплыва игроков. К вечеру MAU проекта достигло 30.        Опыт № 4. Готовьте трафик к Soft Launch.    Быстро осознав ошибки своего пути, мы принялись делать проекту PR и работать с комьюнити. Социальные сети тогда ещё не так плотно вошли в нашу жизнь и потенциальные игроки больше сидели на форумах. Через какое-то время мы узнали о рекламных сетях и начали закупу трафика в Google AdWords. К тому моменту к нашей команде присоединился первый контент-менеджер и профессиональный художник. Каково же было наше удивление, когда 80% регистраций не превращалось ни во что! Люди уходили из игры, даже не заплатив денег. Мы начали думать о туториале и монетизации.    Опыт № 5. Монетизационные механизмы должны быть заложены в core gameplay.    Проведя анализ конкурентов, включая всем известный БК и пару аналогичных бразерок, мы выделили ряд ключевых статей дохода и поняли, что хотим продавать в нашей игре.  Мы постоянно общались со своими игроками на форуме и в чате проекта, получая от них обратную связь и оперативно дорабатывая проект, основываясь на их подсказках. Но денег не стало больше, как и игроков онлайн. В этот момент мы задумались об аналитике.    Опыт № 6. Система сбора и анализа статистики должны быть готова к Soft Launch.    Мы научились считать ретенш, поняли, что такое конверсии и проанализировали, где отваливаются наши игроки. Мы начали работу с трафиком и борьбу за низкие CPA.  Очевидные косяки были закрыты, а игра была обложена метриками. Время жизни активного игрока превышало многие месяцы. Процент платящих иногда доходил до 20. Онлайн стабильно держался на уровне 100. Но чем больше мы вводили новых фичей, чем больше рецептов, талантов и кветов появлялось в игре, тем больше жалоб появлялось на форуме. В определённый момент объём нового контента и его корреляция со старым стали выше чем то, что мы могли анализировать. Новый квест добавлял не фан, а всё большее недовольство. Второй художник, которого мы наняли на растущие доходы проекта, ситуации не спасал.    Опыт № 7. Не пренебрегайте контролем качества.    Много недель ушло на то, чтобы создать систему правильной выливки обновлений вместо ручного копирования на сервер. Появились тест-кейсы и чек-листы обновлений. Патчи более не накатывались на продакшн без тестирования на PTR (Public Test Realm). Все игроки заранее оповещались о готовящихся обновлениях и принимали участие в их обсуждениях. У проекта появилась энциклопедия, тестировщики и комьюнити. Мы создали институт модераторов из числа игроков. Техподдержка переродилась из вечно заваленного болью топика на форуме в отдельную удобную страницу сайта.  Мы собрали своё комьюнити – преданное и безумно влюблённое в наш проект. Но дальнейший рост аудитории и выручки шёл очень медленно. У проекта появился красивый лэндинг, был хороший ARPPU  – 1800 рублей. Но конверсии оставались низкими. Только через год после запуска альфы два человека с техническим складом ума поняли почему.        Опыт № 8. Продукт должен быть красиво упакован интерфейсами с самого начала.    Интерфейсы Возмездия были ужасны. Нам они казались удобными и вполне допустимыми. Но разбираться с нашими HTML таблицами были готовы только достаточно хардкорные пользователи. Мы наняли дизайнера интерфейсов, и я взялся за юзабилити. Осознание того, что один действительно удобный крутой интерфейс делается за месяц, а в игре таких нужно сотню привело нас в уныние. То, что должно было разрабатываться с самого начала, за год до запуска, появилось лишь в ОБТ. К слову, мы так и не поменяли все интерфейсы игры на новые и кое-где оставались «артефакты» древних времён.  Наши художники прокачались. Появился простой и увлекательный туториал. Игра начала накапливать аудиторию. У нас появились большие кланы, а число регистраций перевалило за 100 000. Бутылочным горлышком для расширения стала монетизация. Мы посмотрели на конкурентов и ввели артефакты.    Опыт № 9. Прямая продажа мощи негативно влияет на игру.    Процент платящих подскакивал до 37! Ежемесячная выручка исчислялась сотнями тысяч рублей. И всё, казалось, шло просто замечательно. Но закупившись премиум-аккаунтами, мощными шмотками и бустами, наши любимые преданные игроки начали поглощать контент с огромной скоростью. То, что мы запланировали на полгода, она освоили за месяц. В начале мы ускорили выпуск контента, но осознания тупиковости этого пути пришло к нам очень быстро.  Затем был болезненный нерф. Удар по выручке проекта оказался несильным, а баланс слегка подправился. Однако большинство игроков уже закупили всё, что хотели. И мы всё равно были обречены вводить новый контент: фракции, территории, боссов, подземелья, ресурсы, поля боя, рецепты… Мы бросили все силы на производство контента. Но игра медленно шла на спад.    Опыт № 10. Контента без новых механик недостаточно для поддержания проекта.    На рынке жгли сильные конкуренты: «Легенда: наследие драконов», «Троецарствие», «Территория», «Техномагия» и другие известные браузерные игры. В один тёплый августовский день мы приостановили дальнейшую поддержку проекта.    Прошло много лет. За это время я принял участие в разработке еще нескольких браузерок, action-rpg Panzar, мобильной игры «Эволюция: Герои Утопии» и других мобильных и социальных проектов. Стал директором по гейм-дизайну крупной российской игровой компании. 5 лет преподавал игровую логику в школе RealTime, а два года назад запустил с коллегами по цеху первое в России государственное образование в области игровой индустрии на базе Высшей школы экономики. Все это время работа в геймдеве была и продолжает оставаться для меня интереснейшим делом жизни.    В заключении хочу сказать, что игровая индустрия – это место, открывающее для людей с горящими глазами перспективы работы, приносящей удовольствие. А когда занимаешься любимым делом, то и деньги, и успех в итоге приходят. Приходят через ошибки, через собственные «грабли» и недосыпы. Но если глаза горят – то все это преодолимо!","инди-игра, геймдизайн, геймдев, игровая индустрия, оценка трудозатрат, методология разработки, прототипирование, монетизация игр, аналитика, qa, интерфейсы и юзабилити","10 тезисов инди-разработки, которые привели к успеху"
"Недавно  ZlodeiBaal опубликовал статью «Нейрореволюция в головах и сёлах», в которой привел обзор возможностей современных нейронных сетей. Самым интересным, на мой взгляд, является подход с использованием сверточных сетей для сегментации изображений, про этот подход и пойдет речь в статье.         Уже давно появилось желание изучить сверточные сети и узнать что-то новое, к тому же под рукой есть несколько последних Tesla K40 с 12Гб памяти, Tesla c2050, обычные видеокарты, Jetson TK1 и ноутбук с мобильной GT525M, интереснее всего конечно попробовать на TK1, так как его можно использовать практически везде, хоть на столб фонарный повесить. Самое первое с чего начал, это распознавание цифр, тут конечно удивить нечем, цифры уже давно неплохо распознаются сетями, но при этом постоянно возникает потребность в новых приложениях, которые должны что-то распознавать: номера домов, номера автомобилей, номера вагонов и т.д. Все бы хорошо, но задача распознавания цифр является лишь частью более общих задач.      Свёрточные сети бывают разные. Какие-то умеют только распознавать объекты на изображении. Какие-то умеют выделить прямоугольник с объектом (RCNN, например). А какие-то могут профильтровать изображение и превратить его в некоторую логическую картинку. Мне больше всего понравились последние: они самые быстрые и красивее всего работают. Для тестирования была выбрана одна из самых последних сетей на этом фронте — SegNet, подробнее можно прочитать в статье. Основная идея этого метода заключается в том, что вместо lable подается не число, а изображение, добавляется новый слой «Upsample» для увеличения размерности слоя.     layer {   	name: ""data""   	type: ""DenseImageData""   	top: ""data""   	top: ""label""   	dense_image_data_param {    		source: ""/path/train.txt""	// файл обучения: image1.png  label1.png     		batch_size: 4   			       		shuffle: true   	} }     В конце развернутое изображение и маска из lable подается в слой loss, где каждому классу назначается его вес в функции потерь.    layer {     	name: ""loss""     	type: ""SoftmaxWithLoss""    	 bottom: ""conv_1D""    	 bottom: ""label""    	 top: ""loss""     	softmax_param {engine: CAFFE}     	loss_param: {     	  	weight_by_label_freqs: true     	  	class_weighting: 1     	  	class_weighting: 80   	} }     Правильно распознать цифры это лишь часть задачи распознавания номеров и далеко не самая сложная, надо для начала этот номер найти, затем найти где примерно располагаются цифры, а затем уже их распознавать. Довольно часто на первых этапа появляются большие ошибки, и в итоге получить высокую достоверность распознавания номеров довольно сложно. Грязные и затертые номера плохо детектируются и с большими погрешностями, шаблон номера плохо накладывается, в результате возникает много неточностей и сложностей. Номер может быть вообще нестандартным с произвольными интервалами и т.д.   Например, номера вагонов имеют множество вариаций написания. Если правильно выделить границы номера, то можно получить хоть 99.9% на каждой цифре. А если цифры переплетены? Если сегментация будет давать разные цифры в разных частях вагона?        Или, например, задача обнаружения автомобильного номера. Конечно, её можно решить и через Haar и через Hog. Но почему бы не попробовать другой метод и сравнить? Тем более когда есть готовая для обучения и разметки база?   На вход сверточной сети подается изображение с автомобильным номером и маска, на которой единицей залит прямоугольник с номером, а все остальное нулем. После обучения проверяем работу на тестовой выборке, где на каждое входное изображение сеть выдает такого же размера маску, на которой закрашивает те пиксели, где по ее мнению есть номер. Результат на рисунках ниже.                 Просмотрев тестовую выборку можно понять, что этот метод работает достаточно хорошо и почти не дает сбоев, все зависит от качества обучения и настроек. Так как у  Vasyutka и  ZlodeiBaal была размеченная база номеров, то обучились именно на ней и проверили насколько хорошо всё работает. Результат был не хуже каскада Хаара, а во многих ситуациях даже лучше. Можно отметить некоторые минусы:     не детектирует наклонные номера (их не было в обучающей выборке)   не детектирует номера, отснятые в упор (их тоже не было в выборке)   иногда не детектирует белые номера на чистых белых машинах (скорее всего тоже из-за неполноты обучающей выборки, но, что интересно — этот же глюк был и у каскада Хаара)    В целом проявление этих недостатков закономерно, сеть плохо находит то, чего не было в обучающей выборке. Если тщательно подойти к процессу подготовки базы, то и результат будет высокого качества.  Полученное решение можно применить для большого класса задач поиска объектов, не только автомобильных номеров. Хорошо, номер найден, теперь надо найти там цифры и распознать их. Это тоже не простая задача, как кажется на первый взгляд, нужно проверить множество гипотез их расположения, а что если номер не стандартный, не подходит под маску, то дело труба. Если автомобильные номера изготавливаются по госту и имеют определенный формат, то есть номера, которые могут быть написаны как угодно, от руки, с разным интервалом. Например, номера вагонов, пишутся с пробелами, единички занимают гораздо меньше места, чем другие цифры.  К нам на помощь снова спешат сверточные сети. А что если использовать туже самую сеть для поиска и распознавания. Будем искать и распознавать номера вагонов. На вход сети подается изображение, на котором есть номер и маска, где квадратики с цифрами заполняются значениями от 1 до 10, а фон заполняется нулем.         После не очень долгого обучения на Tesla K40 получен результат. Чтобы результат был более читабельным, разные цифры раскрашены в разные цвета. Определить номер по цветам большого труда уже не составит.          На самом деле получился очень хороший результат, даже самые плохие номера, которые до этого плохо распознавались, удалось найти, разделить на цифры и распознать весь номер. Получился универсальный метод, который позволяет не только распознать цифры, но и в целом найти объект на изображении, выделить и классифицировать его, если таких объектов может быть несколько.           А что если попробовать что-то более необычное, интересное и сложное, например выделение и сегментацию на медицинских изображениях. Для теста снимки флюорографии были взяты с открытой базы КТ и X-RAY снимков, по ним было проведено обучение сегментации легких и в результате удалось достаточно точно выделить интересующую область. На вход сети также подавалось исходное изображение и маска с нулем и единицей. Справа результат, который выдает сверточная сеть, а слева выделена та же область на изображении.        Например, в статье  используется модель легких для сегментации. Полученный с помощью сверточных сетей результат нисколько не уступает, а в некоторых случаях даже лучше. При этом, обучить сеть занимает куда быстрее, чем создавать и отлаживать алгоритм.     В целом данный подход показал высокую работоспособность и гибкость в большом спектре задач, с помощью него можно решить всевозможные задачи поиска объектов, сегментации и распознавания, а не только классификации.     выделение автомобильных номеров;   распознавание автомобильных номеров;   распознавание номеров на вагонах, платформах, контейнерах и т.д;   сегментация и выделение объектов: легкие, котики, пешеходы и т.д.    Работает данный метод достаточно быстро, на видеокарте Tesla обработка одного снимка происходит за 10-15 мс, а на Jetson TK1 за 1.4 секунды. Про то, как запустить Caffe на Jetson TK1 и какой скорости обработки можно на нем достичь в этих задачах, наверно лучше посвятить отдельную статью.    P.S.  Обучение занимало не более 12 часов.  Размер базы по номерам 1200 изображений.  Размер базы по вагонам 6000 изображений.  Размер базы по легким 480 изображений.    1. SegNet  2. A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation (pdf)  3. Haar   4. Hog  5. Сегментация легких на изображениях (pdf)","Caffe, GPU, сверточные сети, сегментация изображений, распознавание номеров, распознавание автомобильных номеров, распознавание номеров вагонов, сегментация легких на флюрографии","Использование сверточных сетей для поиска, выделения и классификации"
"Всем привет. Эта статья продолжение 10к на ядро с конкретными примерами оптимизаций, которые были проделаны для повышения производительности сервера. С написания первой части прошло уже 5 мес и за это время нагрузка на наш продакшн сервер выросла с 500 рек-сек до 2000 с пиками до 5000 рек-сек. Благодаря netty, мы даже не заметили это повышение (разве что место на диске уходит быстрее).      (Не обращайте внимание на пики, это баги при деплое)    Эта статья будет полезна всем тем кто работает с netty или только начинает. Итак, поехали.    Нативный Epoll транспорт для Linux  Одна из ключевых оптимизаций, которую стоит использовать всем — это подключение нативного Epoll транспорта вместо реализации на java. Тем более, что с netty это означает добавить лишь 1 зависимость:    <dependency>    <groupId>io.netty</groupId>    <artifactId>netty-transport-native-epoll</artifactId>    <version>${netty.version}</version>    <classifier>linux-x86_64</classifier> </dependency>   и автозаменой по коду осуществить замену следующих классов:      NioEventLoopGroup → EpollEventLoopGroup  NioEventLoop → EpollEventLoop  NioServerSocketChannel → EpollServerSocketChannel  NioSocketChannel → EpollSocketChannel    Дело в том, что java реализация для работы с не блокирующими сокетами реализуется через класс Selector, который позволяет вам эффективно работать с множеством соединений, но его реализация на java не самая оптимальная. Сразу по трем причинам:      Метод selectedKeys() на каждый вызов создает новый HashSet  Итерация по этому множеству создает iterator  И ко всему прочему внутри метода selectedKeys() огромное количество блоков синхронизации    В моем конкретном случае я получил прирост производительности около 30%. Конечно же, эта оптимизация возможна только для Linux серверов.    Нативный OpenSSL  Не знаю как на просторах СНГ, но ТАМ — безопасность ключевой фактор для любого проекта. “What about security?” — неминуемый вопрос, который Вам обязательно зададут, если заинтересуются Вашим проектом, системой, сервисом или продуктом.    В аутсорс мире, из которого я пришел, в команде всегда обычно был 1-2 DevOps на которых я всегда мог переложить данный вопрос. Например, вместо добавлять поддержку https, SSL/TLS на уровне приложения, всегда можно было попросить администраторов настроить nginx и с него уже прокидывать обычный http на свой сервер. И быстро и эффективно. Сегодня, когда я и швец и жнец и на дуде игрец — мне все приходится делать самому — заниматься разработкой, деплоить, мониторить. Поэтому подключить https на уровне приложения гораздо быстрее и проще чем разворачивать nginx.    Заставить openSSL работать с netty немного сложнее чем подключить нативный epoll транспорт. Вам понадобится подключить в проект новую зависимость:    <dependency>    <groupId>io.netty</groupId>    <artifactId>netty-tcnative</artifactId>    <version>${netty.tcnative.version}</version>    <classifier>linux-x86_64</classifier> </dependency>   Указать в качестве провайдера SSL — openSSL:            return SslContextBuilder.forServer(serverCert, serverKey, serverPass)                 .sslProvider(SslProvider.OPENSSL)                 .build();   Добавить еще один обработчик в pipeline:    new SslHandler(engine)   И наконец, собрать нативный код для работы с openSSL на сервере. Инструкция тут. По сути, весь процесс сводится к:    Выкачать исходники  mvn clean install    Для меня прирост производительности составил ~15%.  Полный пример можно глянуть тут и тут.    Экономим на системных вызовах  Очень часто приходится отправлять несколько сообщений в один и тот же сокет. Это может выглядеть так:    for (Message msg : messages) {     ctx.writeAndFlush(msg); }   Этот код можно оптимизировать     for (Message msg : messages) {     ctx.write(msg); } ctx.flush();   Во втором случае при write нетти не будет сразу отсылать сообщение по сети, а обработав положит его в буфер (в случае если сообщение меньше буфера). Таким образом уменьшая количество системных вызовов для отправки данных по сети.    Лучшая синхронизация — отсутствие синхронизации.  Как я уже писал в предыдущей статье — netty асинхронный фреймворк с малым количеством потоков обработчиков логики (обычно n core * 2). Поэтому каждый такой поток-обработчик должен выполнятся как можно быстрее. Любого рода синхронизация может этому помешать, особенно при нагрузках в десятки тысяч запросов в секунду.     С этой целью netty каждое новое соединение привязывает к одному и тому же обработчику (потоку) чтобы снизить необходимость кода для синхронизации. Например, если пользователь присоединился к серверу и выполняет некие действия — допустим, изменяет состояние модели, которая связана только с ним, то никакой синхронизации и volatile не нужно. Все сообщения этого пользователя будут обрабатываться одним и тем же потоком. Это отлично и работает для части проектов.     Но что, если состояние может изменятся из нескольких соединений, которые вероятней всего будут привязаны к разным потокам? Например, для случая, когда мы делаем игровую комнату и команда от пользователя должна менять окружающий мир?    Для этого в netty существует метод register, который позволяет перепривязать соединение из одного обработчика к другому.    ChannelFuture cf = ctx.deregister(); cf.addListener(new ChannelFutureListener() {    @Override    public void operationComplete(ChannelFuture channelFuture) throws Exception {        targetEventLoop.register(channelFuture.channel()).addListener(completeHandler);    } });   Этот подход позволяет обрабатывать события для одной игровой комнаты в одном потоке и полностью избавится от сихронизаций и volatile для изменения состояния этой комнаты.  Пример перепривязки на логин в моем коде тут и тут.    Переиспользуем EventLoop  Netty довольно часто выбирают для серверного решения, так как сервера должны поддерживать работу разных протоколов. Например, мое скромное IoT облако поддерживает HTTP/S, WebSockets, SSL/TCP сокеты для разного hardware и собственного бинарного протокола. Это значит, что для каждого из этих протоколов должен быть IO поток (boss group) и потоки обработчики логики (work group). Обычно создание нескольких таких обработчиков выглядит так:    //http server new ServerBootstrap().group(new EpollEventLoopGroup(1), new EpollEventLoopGroup(workerThreads))        .channel(channelClass)        .childHandler(getHTTPChannelInitializer(())        .bind(80);  //https server new ServerBootstrap().group(new EpollEventLoopGroup(1), new EpollEventLoopGroup(workerThreads))        .channel(channelClass)        .childHandler(getHTTPSChannelInitializer(())        .bind(443);   Но в случае netty чем меньше лишних потоков вы создаете, тем больше вероятность создать более производительное приложение. К счастью, в netty EventLoop можно переиспользовать:    EventLoopGroup boss = new EpollEventLoopGroup(1); EventLoopGroup workers = new EpollEventLoopGroup(workerThreads);  //http server new ServerBootstrap().group(boss, workers)        .channel(channelClass)        .childHandler(getHTTPChannelInitializer(())        .bind(80);  //https server new ServerBootstrap().group(boss, workers)        .channel(channelClass)        .childHandler(getHTTPSChannelInitializer(())        .bind(443);     Off-heap сообщения  Ни для кого уже не секрет, что для высоконагруженных приложений одним из узких мест является сборщик мусора. Netty быстра, в том числе, как раз за счет повсеместного использования памяти вне java heap. У netty есть даже своя экосистема вокруг off-heap буферов и система обнаружения утечек памяти. Так можете поступить и Вы. Например:    ctx.writeAndFlush(new ResponseMessage(messageId, OK, 0));   изменить на     ByteBuf buf = ctx.alloc().directBuffer(5); buf.writeByte(messageId); buf.writeShort(OK); buf.writeShort(0); ctx.writeAndFlush(buf); //buf.release();   В этом случае, правда, Вы должны быть уверены, что один их обработчиков в pipeline освободит этот буфер. Это не значит, что вы должны сразу же бежать и изменять свой код, но про такую возможность оптимизиции Вы должны знать. Несмотря на более сложный код и возможность получить утечку памяти. Для горячих методов это может идеальным решением.    Надеюсь эти простые советы позволят Вам ускорить ваше приложение.  Напомню, что мой проект open-source. Поэтому если Вам интересно как эти оптимизации выглядят в существующем коде — смотрите тут.","java, netty, blynk, iot, optimization, epoll, openSSL",Топ 6 оптимизаций для netty
"Доброго времени суток всем.    Недавно в одном из проектов мы столкнулись со следующей проблемой — функция openssl_random_pseudo_bytes() выдавала дублирующиеся псевдослучайные последовательности!    Этого не может быть, потому что этого не может быть никогда! — Скажет любой, кто читал документацию этой функции. И, да, $crypto_strong исправно выдавал TRUE.    И тем не менее — ошибки уникальности при вставке в базу сыпались пачками и лог подтверждал — 32-байтные последовательности генерировались повторно через разные интервалы, от суток до недели. Расследование заняло целый месяц. Сейчас я на 99% уверен, что причина найдена — но буду благодарен, если Хабражители подтвердят или опровергнут мои выводы.        А дело было в сочетании особенностей сразу трех продуктов:    Apache работающего с prefork MPM  PHP имеющего ограниченную поддержку функций OpenSSL  И самой библиотеки OpenSSL имеющий проблему Random fork-safety    Упрощенно происходящее выглядит так — Апач при старте создает первую копию ПХП которая стартует рандом-генератор OpenSSL. А дальше — Апач создает и использует форки, копируя в том числе и исходное состояние рандом-генератора.  Так как рандом генератор завязан еще и на PID процесса — то проблема проявляется не сразу. Поскольку на Linux типовое максимальное значение для PID 65536, то вот примерно через такое количество запросов к веб-серверу выдаваемые псевдослучайные последовательности и начнут повторяться. Больше точных технических подробностей лучше получить в уже приведенной выше статье базы знаний OpenSLL    Проблема усугубляется тем, что самые лучшие рекомендованные методы борьбы ( Call RAND_seed after a fork и Call RAND_poll after a fork) на ПХП неприменимы, так как эти функции OpenSSL попросту недоступны из ПХП.    К сожалению, мне не удалось найти в сети адекватных материалов по этой проблеме, за исключением уже приведенной статьи OpenSLL, но она не описывает конкретную связку Apache + PHP + OpenSSL. Зато статей настоятельно рекомендующих использовать openssl_random_pseudo_bytes() как криптостойкий ГСЧ — предостаточно.     А ведь король-то голый!    В итоге — пришлось попросту отказаться от использования openssl_random_pseudo_bytes() и перейти на прямое чтение из /dev/urandom. Не самое блестящее решение — но достаточное в нашем случае.    Поскольку автор не является экспертом в области криптографии и мои выводы могут быть неверны / неполны, а проблема является более чем серьезной, учитывая распространенность рекомендаций по использованию openssl_random_pseudo_bytes(), то я обязательно изучу все комментарии специалистов и возможно исправлю / дополню (или удалю, если в корне не прав) статью. Также, если выводы подтвердятся, необходимо будет внести дополнения в документацию ПХП и предложения по добавлению RAND_seed/RAND_poll и / или их вызовы при старте скрипта в ПХП.    Важно! Apache должен работать в prefork режиме (MPM prefork). Версия ПХП с которой проблема проверялась — 5.5.x, но, предположительно, будет воспроизводиться в любой версии имеющей openssl_random_pseudo_bytes()    P.S. Я отписался в security@php.net — почти месяц назад. Ни ответа, ни привета. Или не получили. Или проигнорировали. Не знаю.  Так что вывожу статью обратно в онлайн.","PHP, apache, fork, openssl",Неожиданное поведение openssl_random_pseudo_bytes() приводящее к фатальной потере криптостойкости
"Три переезда равно одному пожару. Выгребая старый ящик, пропахший ацетоном, с многослойной пылью на донышке (как хорошо, что жена не видела) я наткнулся на до боли знакомые мне компакт диски. Вот один из любимых фильмов детства… а вот моя когда-то любимая аркадная игрушка…    Странная вещь — любопытство. Вот на столе экземпляр CD — безнадёжно устаревшего в наш просвященный XXI век формата хранения данных; а все таки интересно, как же там хранятся данные?.. Каков сам стек хранения данных?.. Как исправляются ошибки?.. Какова избыточность кода?..    В детстве мне было достаточно знаний о лазерном луче, о какой-то головке, об «этой крутящийся штуки» и о таинственных питах.     Сказано — сделано. Проглядев стандарт ECMA-130 (есть, кстати и отечественный стандарт: ГОСТ 27667-88) обнаружил массу любопытных деталей. Например, я догадывался об избыточности, но я и подумать не мог, что для записи 700 Мб данных «в реальности» записывается 1943 Мб (То есть в 2.776 раз больше)…     Схематично весь стек можно представить картинкой:          Стек рассмотрим «сверху вниз».  То есть от момента передачи данных приводу до записи самих питов.  Следует сказать, что не вся область диска используется для записи/хранения/чтения информации.  Компакт диск разделен на зоны (areas):     Centre Hole (Центральное отверстие ) — это «та самая дырочка» диаметром 15 мм (±0.1 мм), за которую прикрепляется сам диск.     First transition area (Первая переходная зона) — «кольцо», между 15 и 20 мм от центра диска.     Сlamping area (Зона зажима) — как понятно из названия это область необходима, чтобы диск «не скакал» при чтении / записи. (26-33 мм)     Second transition area (Вторая переходная зона) — «второе кольцо», между 33 и 44 мм от центра диска.     Information area (Зона информации) — это «информационно полезная» часть компакт диска. Находится на расстоянии от 44 мм до 118 мм от центра.      Rim area (обод) — последняя область. Представляет собой кольцо от 118 мм до 120 мм от центра.        Именно зона информации нам и интересна, поэтому о ней скажем более подробно. Information area разделяется на следующие «подзоны»:     inner buffer zone (зона ввода)   user data zone (зона программы)   outer buffer zone (зона вывода)      Все три перевода сделаны ГОСТом… Так что от себя переводить не буду. При чем здесь «программа», когда речь идет о данных — ума приложить не могу. Если кто-то на Хабре сможет мне ответить, почему user data zone перевели как «зона программы» я буду крайне признателен!    Уф… С физическим ликбезом, надеюсь, покончили. Разумеется, я упустил многие моменты, связанные с ширинами и длинами; с методом покрытия; длиной волны луча; и прочими физическими свойствами. Однако, во-первых это не является целью данной статьи; во-вторых я и сам больше половины информации пока не разобрал. Да и нет желания, если честно… Мое любопытство чисто «программистской» направленности ;)    Information Tracks («Информационные дорожки»)     Первая фаза — это разбиение на «информационные дорожки». Уже тут есть два варианта записи, в цифровом виде (Digital Data Tracks, DDT) и аудиоданные (Audio Tracks).  В дальнейшем будем рассматривать только цифровые данные. Вся нижеследующая информация корректна только для DDT.    Sector (Сектор)    Данные разбиваются по 8 бит (по одному байту) и группируются в секторы.  Забавно, что количество секторов в диске не определено стандартом. Оно зависит от того, сколько данных «получиться» записать на диск…    Это кажется немного нелепым. Тем не менее эту нелепость в свою эпоху использовали различные компании по обнаружению контрафактных дисков. (Так как это «уход в сторону», то написал подробнее под спойлером чуть ниже, в конце главы.)     Разумеется, длина дорожки более-менее фиксирована и в целом QoS гарантировать можно.    Существует три способа (mode) записи данных в секторах:     Sector Mode (00) — это пустой сектор, заполняемый данными, состоящие из 0x00 байтов.   Sector Mode (01) — использование EDC, P-Q и CIRC кодирование. (об этом чуть ниже)   Sector Mode (02)  — отсутствие P и Q кодирования, только CIRC кодирование      Приведем картинки:      Метод определения контрафактного дискаМетод, как и все гениальное, прост.    Запись лицензионного ключа.    Следует сначала записать всю необходимую информацию на диск.   Затем мы выбраем 2 сектора. Например 103123 и 120234 сектора. Обозначим эти сектора как A и Б.   Выбираем два байта: по одному байту на каждом секторе. Например 4й байт первого сектора и 8й байт второго  Затем следует подсчитать угол между этими байтами на секторе. Как это сделать? Предположим, у вас есть доступ к низкоуровневому драйверу чтения и вы знаете время одного оборота.Тогда следует вычислить время, потраченное на считывание между А и Б. Поделив это время на время одного оборота можно с определенной погрешностью вычислить угол.  Зачение угла, округленное до определенного знака, подается на вход хеш-функции. У хеш-значения берутся несколько символов, например последние 3 символа.  Эти три символа записываются в лицензионный ключ.      Процедура проверки лицензионного ключа.     У пользователя просят ввести лицензионный ключ.   Вычисляем угол между выбранными байтами A и Б секторами   Вычисляем проверяемый список углов. Например мы вычислили угол 33,343°. Предположим, что округление происходит с точностью до градуса. Округляем и получаем 33°. Предположим, что погрешность ±2°. Список углов: [31°, 32°, 33°, 34°, 35°].    Для каждого угла из списка вычисляем хеш. Берем несколько символов из хеша. Для примера — последние 3 символа   Если хотя бы один хеш из списка совпал с хешем из лицензионного ключа, то делаем вывод, что диск лицензионный. Иначе диск контрафактный            Коррекция ошибок (только Sector Mode 01)    Полезная информация (User Mode) состоит из 2048 байт в режиме 01; или 2336 байт в режиме 02.  Какой режим выбрать? Все зависит от того, какое требование надежности вам требуется.  Sector Mode 01 надежнее, так как в нем используются дополнительно EDC проверка и P-Q кодирование.     EDC     Error Detection Code (EDC), как понятно по названию, предназначен только для обнаружения, но не для исправления ошибок.    Вот его полином: P(x)=(x16+x15+x2+1)(x16+x2+x+1)     Intermediate     Восемь байт поля Intermediate заполняются нулевыми байтами (0x00).  Вот уж не знаю зачем их оставили… Может «про запас» (в IT стандартах это любят), а может это коварный план стеганографической передачи данных.     P и Q кодирование (RSPC)     Reed-Solomon Product-like Code (RSPC), оно же P+Q кодирование используется с 12 по 2075 байт данных в режиме 01. Подробности опущу, вы можете прочесть их в Annex A стандарта ECMA-130.    Байты с 12 по 2075 и проверочные с 2 076 по 2 351 составляют 2340 байт данных. Эти данные разбиваются на два блока по 1170 байт каждый. Разбиение происходит как в школьных уроках физкультуры. ""На перррвый- вторрррой рассчитась!"". То есть на нечетные и четные байты.    Дальше идет кодирование внешним и внутренними кодами. Внешний называется P-кодированием, внутренний Q-кодированием.    Картинка с P и Q кодированием    Для большего понимания: картинка только с Q кодированием      Самая сложная в понимании стека ECMA-130 пройдена. Теперь будет значительно проще.     Скремблирование    Переходим к скремблированию. Вот так выглядит один сектор скремблирования:      Каждый такой сектор называют Scrambled Sector.    А что такое ''скремблирование'' и зачем это нужно?О смысле скремблирования кратко и емко в комментарии к одному из моих постов написал  snapdragon   Скремблер нужен для того, чтобы сделать спектр сигнала равномерным. Иначе, при однородных данных (например, много повторяющихся единиц или нулей) энергия сигнала будет сосредоточена в узком диапазоне.         F1, F2 и F3 frame'ы    Каждый Scrambled Sector разбивается на frame'ы, длиной по 24 байта каждый.  Данные frame'ы имеют название: F1 frame.  Каждый Scrambled Sector у нас состоит из 2352 байт.  Соответственно каждый сектор разбивается на 98 frame'ов.    CIRC кодирование (F2 frame)  Cross Interleaved Reed-Solomon (CIRC) кодирование осуществляется для каждого F1 frame'а.  Это код, корректирующий ошибки с длиной входного слова в 24 байта, а длиной выходного слова в 32 байта.  Причем в отличие от EDC и RSPC кодирования, CIRC кодирование применяется для всех Sector Mode.  Полученную последовательность из 32 байт называют F2 frame'ами.    Контрольный байт  В начало каждого F2 frame'а добавляют один проверочный байт и получается F3 frame с длиной в 33 байта (32+1=33).    8-to-14 кодирование  На этой стадии данные каждый байт (8 бит) преобразуется в 14 бит данных. Преобразование осуществляется по таблице.    Всю таблицу приводить не буду, вы можете найти её в Annex D стандарта ECMA-130.    ...      ... 00010000 10000000100000 00010001 10000010000000 00010010 10010010000000 00010011 00100000100000 00010100 01000010000000 00010101 00000010000000 00010110 00010010000000 ...      ...     Зачем необходимо 8-to-14 кодирование в стандарте не указано. (Стандарт и не обязан отвечать на вопросы ПОЧЕМУ, в стандарте должны быть ответы на вопросы КАКИМ ОБРАЗОМ)…    У меня есть одна гипотеза. Дело в том, что реальный мир не настолько «идеален», каким его видят программисты. Например, нарисованная точка — это маленькая «клякса», а нарисованная линия всегда имеет площадь; в противном случае наши бы глаза не видели бы точку и линию… По этой причине рискну высказать ряд предположений. Подчеркну, что я никогда не работал профессионально с изготовлением CD дисков. Это всего лишь предположения. (Дискуссия в коментариях категорически приветствуется!).    Гипотезы.    Пит не «идеально» выжигается на поверхности диска, поэтому необходимо некое пространство рядом с питом, т.к. за это пространство выжженный пит может «заскочить».   Скорее всего, есть определенные проблемы с синхронизацией самой головки.Слишком большое количество подряд идущих нулей — это плохо.   Возможно большое количество единиц это дополнительная «нагрузка» на считывающую головку. Поэтому их уменьшение позволит существенно увеличить срок эксплуатации CD привода. В среднем на 8 бит данных мы имеети 4 единицы. В 8-to-14 кодировании в кодовом слове у нас 1 или 2 единицы. То есть в два раза меньше.      Подсчитываем избыточность    Посмотрим, насколько протокол CD избыточен:    В зависимости от Sector Mode:  Sector Mode 01 (P-Q кодирование) — На входе блок из 2048 байт, на выходе 2352. Следовательно избыточность равна: 2352/2048=1.148   Sector Mode 02 (без P-Q кодирования) — 2352/2336=1.007    Скремблирование — мелочь, но для порядка учтем: (12+2340)/2340=1.005  F1-F2-F3 фреймы — 33/24=1.375  8-to-14 кодирование — 14/8=1.750      Перемножая все, получаем: 1.148 ⋅ 1.005 ⋅ 1.375 ⋅ 1.750 = 2.776. Таким образом на сам диск в итоге записывается в 2.776 раз больше информации, чем «полезная информация».  Например при объеме «полезной информации» в 700Мб, реально на диск записывается 1943 Мб данных.    Для Sector Mode 02 не используется P-Q кодирование. Для этого режима избыточность равна: 1.007 ⋅ 1.005 ⋅ 1.375 ⋅ 1.750 = 2.435.    Бонус: SCSI Multimedia Commands    Есть стандарт SCSI Multimedia Commands. В нем дано описание команд «сырого» чтения данных. Команды READ CD и WRITE CD позволяют считывать 2352 байт данных со всего сектора. Однако команд для считывания F-fraim'ов я не нашел… В принципе если записывать избыточную информацию, для которой не страшны частичные потери (например видео, телематика)  можно обойтись без F1-F2-F3 фреймов увеличив «полезную нагрузку» в 1.375 раз.     Так же есть ряд неиспользуемых областей в компакт диске (например тот же Intermidiate), которыми так же можно воспользоваться. Например ради задач стеганографии.    К сожалению я не нашел OpenSource кода, реализующий данные возможности…  Если на хабре есть спецы по данному вопросу, буду рад получить ссылочку (с меня плюс в карму).","компакт диск, коды Рида-Соломона",ECMA-130 (Compact Disc) на пальцах
"В пятницу, 19 февраля, мы выпустили бета-версию IntelliJ IDEA 16. Учитывая то, что с момента выхода IntelliJ IDEA 15 прошло немногим более трех месяцев, вы будете приятно удивлены тем, что мы успели сделать за это время. Мы приглашаем вас скачать свежую версию прямо сейчас и самостоятельно попробовать все улучшения, о самых главных из которых я с удовольствием расскажу далее.         Отладчик    Для Java проектов в Evaluate Expression и Watches теперь можно писать выражения и на Groovy, обладающим более компактным синтаксисом, что делает более удобной, например, работу с коллекциями.         Если текущий поток заблокирован другим остановленным потоком, то IntelliJ IDEA предложит его разблокировать.     Ранее, по нажатию Resume, возобновлялись все запущенные потоки. Теперь можно изменить это поведение, включив опцию Resume only the current thread.    Теперь IntelliJ IDEA предупреждает всякий раз, когда замечает, что исходники не соответствуют выполняемому коду. Это помогает избежать ошибок и сэкономить время.        Интеграция с Git и другими VCS    IntelliJ IDEA теперь поддерживает git worktrees – на случай если вы хотите работать с несколькими ревизиями репозитория одновременно и сэкономить место на диске.    Для Git в Branches popup появились две новые команды: Checkout with Rebase и Rename. Checkout with Rebase позволяет сэкономить время, если вы хотите выполнить эти две операции одну за другой.    Инструменты Diff viewer и Merge dialog стали удобнее за счет подсветки изменений на уровне конкретных частей строки.        Редактор    В редакторе появился новый инструмент: Move Element Right/Left (Alt + Ctrl + Shift + Arrows или Alt + Cmd + Shift + Arrows для OS X). С его помощью можно менять местами аргументы методов, элементы массивов и атрибуты тегов.        Мы добавили авто-импорт для статических методов и констант (ранее он работал только для классов). Опция Add unambiguous imports on the fly теперь также работает и для статических методов и констант.        Редактор поддерживает арабский, иврит и другие языки, где символы следуют справа налево.        В настройках стилей появилась опция для автоматического добавления пробела в начало комментариев.    Java 8    При вызове Inline method или Change signature, ссылки на метод (method references) преобразуются в соответствующие лямбда выражения.     IntelliJ IDEA предупредит, если вы вызываете метод get() на java.util.Optional не проверив перед этим с помощью isPresent() содержится ли там значение.         Добавлено много инспекций на использование функциональных интерфейсов, лямбда выражений и Optional.         Если вы используете Guava, IDE предложат Вам заменить FluentIterable, Function, Optional и Predicate их аналогами из Java 8.        Быстрый поиск    Быстрый поиск (speed-search) теперь доступен в окне Terminal, также был улучшен поиск во всплывающем окне Show usages и в окне Find usages.         Интерфейс Log viewer для Git и Mercurial теперь выглядит чуть приятнее.    Gradle    Теперь модель проекта IntelliJ IDEA полностью совпадает с моделью Gradle: каждый source set представлен в IntelliJ IDEA как отдельный модуль, и поэтому может иметь собственные зависимости (classpath). Это позволило решить огромное количество проблем.     IntelliJ IDEA научилась автоматически импортировать EAR-артефакты из билд-скриптов.    Spring    Для Spring Boot проектов добавленны подсказки внутри файлов YML и banner.txt. Аннотация \@SpringApplicationConfiguration учитывается при создании конфигураций запуска для тестов. Для пользовательских свойств конфигурации теперь работает Find usages.     Поддержка Spring MVC сильно улучшилась за счет поддежки аннотации \@EnableWebMvc.         Исправленнно много старых проблем.    Thymeleaf     Thymeleaf 3 Beta поддеживается “из коробки”.         Добавлена поддержка пользовательских диалектов: для них теперь доступны те же посказки, что и для стандартных.        Исправленно много проблем с поддержкой Thymeleaf 2.     Android    Профайлер утечек и новые инспекции, появившиеся в Android Studio 1.5, теперь доступны и в IntelliJ IDEA.         Kotlin    Да-да, если вы еще не в курсе (или не верите своим глазам), неделю назад состоялся релиз Kotlin 1.0.    Kotlin совместим с Java 6, 7, 8, Android, и любыми Java фреймворками (такими как Java EE, Spring) и билд-системами (Gradle, Maven). Kotlin можно использовать в существующих Java проектах. С версии 1.0 Kotlin обратно-совместим со своими предыдущими версиямми.     IntelliJ IDEA 16 включает плагиин для Kotlin 1.0.    Ранее на Хабре уже была проведена первая сессия вопросов и ответов.     Scala    Работая над Kotlin, мы не забываем и о Scala: комплишен претерпел ряд улучшений. Во-первых, порядок предлагаемых вариантов теперь учитывает такие факторы, как тип символа, историю его использования и контекст. Предпочтение отдается локальным переменным, затем аргументам, затем полям класса, затем методам и т.д. Если в данном контексте ожидается тип, IntelliJ IDEA учитывает это в порядке предложенных вариантов.         Во-вторых, появился комплишен для выражений pattern matching c подсказкой имен свойств case-классов.        ES6 и TypeScript    Поддержкка ES6 и TypeScript становится лучше с каждым днем. Добавлены рефакторинги Create method, Extract method, Inline method и Introduce field и интеншены Make class abstract, Make public/private и Remove modifier. Неиспользуемые import-выражения теперь подсвечиваются и убираются с помощью Optimize imports. Во время набора кода import-выражения добавляются автоматически.        Добавленна поддерджка TypeScript 1.8.    AngularJS    Для AngularJS 2 добавлен комплишн директив, переменных шаблонов, пользовательских обработчиков событий, путей в templateUrl и styleUrls полях, а также в event, data, и property bindings.        Также IntelliJ IDEA понимает компоненты, определенные с помощью module.component(), добавленного в AngularJS 1.5.    JavaScript отладчик    С новым отладчиком для Chrome гораздо лечге отлаживать асинхронный код и код с лямбда выражениями (arrow functions). Для асинхронного кода, стек фреймов включает место вызова асинхронного кода. При добавлении точек остановки, теперь можно выбрать конкреное лямбда выражение.    Также можно отлаживать main и render процессы Electron приложений.    JSON    Кроме этого теперь IntelliJ IDEA поддерживает JSON schema и предлагает на его основе комплишн внутри JSON файлов (tsconfig.json, .eslintrc, .babelrc и многих других).        Работа с базами данных    После релиза DataGrip мы продолжаем улучшать поддержку баз данных и инструменты для работы с ними. Добавлены поддержка некоторых операторов PostgreSQL 9.5 (hstore ?) а также User Defined Table и Column Types для SQL Server. С помощью Create/Modify Table теперь можно редактировать комментарии для колонок (если это поддерживается базой данных).         Если вы хотите удалить данные из таблиц, вам поможет новый инструмент: Truncate, который позволяет очень быстро и вне транзакции удалить данные из одной или более таблиц.        Docker    И наконец, Docker стал отдельным окном (tool window) и его интерфейс понемногу становится лучше и лучше: теперь правая панель отображает логи и позволяет редактировать настройки контейнеров. Образы и контейнеры теперь показываются раздельно и более понятно. Также мы поддержали Docker Machine.        Если вы хотите чтобы я написал о каких-то из новых фич отдельно (или о каких-то старых фичах), – просите в комментариях, и я с удовольствием напишу. Также буду рад ответить на любые вопросы.","Java, IntelliJ, IntelliJ IDEA, IDE",Обзор IntelliJ IDEA 16 Public Preview
"Продолжаем обзор новых возможностей 3.0, мы уже успели обзорно рассказать о возможностях новой версии, отдельно остановились на возможности прогнозирования проблем на основе собранной статистики, а сегодня расскажем о шифровании.  Шифрование было одним из самых давних и ожидаемых нововведений в Zabbix, и за это время успели обсудить несколько вариантов его реализации: от аутентификации по предварительному ключу PSK до полной поддержки TLS и Kerberos. Год назад было решено остановиться на TLS.        И вот теперь в Zabbix 3.0 все компоненты, такие как сервер, агент и прокси, могут быть настроены на использование шифрования, или могут продолжить общаться в открытую, как и прежде. Все это дает возможность использовать Zabbix в тех системах, где шифрование между узлами является обязательным условием. Кроме того, поддержка безопасного соединения добавлена и для утилит командной строки, таких как zabbix_sender и zabbix_get.    Технические детали  Для шифрования Zabbix может использовать одну из следующих библиотек: OpenSSL, GnuTLS или mbedTLS (PolarSSL). Такое решение в первую очередь принято, чтобы всегда иметь возможность сменить библиотеку, если в используемой обнаружится критическая уязвимость, изменится тип лицензии или разработка и поддержка закончится. Другим преимуществом является нейтралитет Zabbix к любому из перечисленных наборов.    TLS поддерживается версии 1.2 — предыдущие версии SSL/TLS протокола содержали уязвимости, поэтому использовать их мы посчитали плохой идеей.    Кроме скрытия передающихся сообщений от посторонних, реализована и аутентификация. Теперь возможно получить ответы на вопросы «Можно ли доверять источнику данных» или «Отправляем ли мы файл со списком паролей на наш сервер». Для аутентификации на выбор доступно: использование сертификата ( для тех, у кого в наличии инфраструктура открытых ключей или высокие требования к безопасности) или ключ PSK (для небольших инсталляций).    Не потребуется и открывать никаких новых портов — по-прежнему используются только стандартные для Zabbix TCP/10050-10051.  Если шифрование пользователю не интересно, то можно продолжить использовать Zabbix как и прежде. При этом обновленные компоненты Zabbix будут поддерживать новую возможность, поэтому начать настраивать защищенные соединения можно в любой удобный для себя момент.    Также важно отметить, что мы добавили несколько новых полей в таблицах БД Zabbix, для того чтобы хранить дополнительные настройки, сделали возможным их изменения через веб (логично же) или API, а также добавили новые параметры в конфигурационных файлах.  Пример настройки шифрования демонстрируем следующим скриншотом.        Два параметра определяют, как сервер или прокси подсоединяются к агенту (для пассивных проверок, «Connections to host») и отдельный параметр, определяющий, какие типы соединений разрешены от агента (активные проверки и zabbix_sender, «Connections from host»). «Connections from host» также используется для того, чтобы ограничить подключения к серверу от тех, кто обзавелся украденным сертификатом или разузнал PSK.    Со стороны агентов и утилит zabbix_get, zabbix_sender настройка шифрования производится через новые параметры конфигурационных файлов или аргументов командной строки.    В момент тестирования возможно разрешить сразу несколько видов соединений (т.е. открытые и защищенные), дальше настроить шифрование и аутентификацию на основе сертификата или PSK, убедиться, что все работает, и, наконец, запретить нешифрованные соединения.    Последующие улучшения  При этом, конечно еще есть над чем поработать, например:    В настоящий момент переиспользование TLS сессий через кеширование не поддерживается, поэтому сейчас каждое подключение инициализирует соединение с нуля, что естественно медленней.  Добавить поддержку шифрования для Zabbix Java gateway.  Сейчас реализована поддержка только сертификатов X.509 с ключами RSA. Поддержка сертификатов ECDSA должна помочь улучшить производительность.  Отозванные сертификаты можно проверить только через CRL-файлы, онлайн проверка издателя в текущий момент не реализована.      Поддержка шифрования и взаимной аутентификации в Zabbix дает возможность пользователям постепенно и выборочно улучшать безопасность компонентов системы мониторинга. Предлагаем вам самим опробовать это и другие нововведения. Полную спецификацию на шифрованию можно найти в документации , ну а также читайте про остальные нововведения в Zabbix на Хабре, если вдруг пропустили.    Перевод статьи из нашего блога.","zabbix, мониторинг, шифрование",Zabbix 3.0: Шифрование
"Если вы заходили с мобильного хрома в фейсбук, то наверняка видели, что интерфейс браузера красится в фирменный синий цвет соцсети. Но зачем и как?       Мобильные сайты все больше походят на приложения, нежели на просто сайт с информацией, и Google поддерживает эту тенденцию. Посмотрите хотя бы на то, что в последних версиях мобильного хрома по умолчанию вкладки браузера смешиваются со списком недавно запущенных приложений. Но что не хватает сайту, чтобы стать еще чуточку больше приложением? Конечно же кастомизации оболочки (интрефейса). Начиная с 39 версии хрома для Android Lollipop была внедрена возможность менять цвет интерфейса браузера веб-разработчиками.     Сделать это просто, нужно в <head> добавить новый мета-тег:    <meta name=""theme-color"" content=""#9CC2CE"">     Как видно из кода, в content мы записываем цвет, в который окрасится браузер.     Google также рекомендует использовать иконку высокого качества, чтобы это выглядело еще лучше:    <link rel=""icon"" sizes=""192x192"" href=""nice-highres.png"">     Через инспектор устройств хрома, я сделал пример, как это могло бы выглядеть на мобильной версии хабра:         Замечание: Если у пользователя не включено смешивание вкладок с приложениями, то он увидит выбранный вами цвет вкладки на вашем сайте, но в списке вкладок, она по прежнему будет серой.         P.S. Мне хотелось бы, чтобы этот цвет можно было задавать через css, так как это более логичный вариант, да и можно было бы воспользоваться переменной…","custom chrome tab, покрасить вкладки хрома, google chrome",Как покрасить вкладку Chrome
"Здравствуйте, меня зовут Дмитрий Карловский, и я… архитектор множества широко известных в узких кругах фреймворков. Меня никогда не устраивала необходимость из раза в раз решать одни и те же проблемы, поэтому я всегда стараюсь решать их в корне. Но прежде, чем их решить, нужно их обнаружить и осознать, что довольно сложно находясь в плену привычек, паттернов, стереотипов и ""готовых"" решений. Каждый раз сталкиваясь с проблемами в реализации задачи, я задумываюсь ""что, блин, не так с этим инструментом?"" и, конечно же, иду пилить свой инструмент: функцию, модуль, библиотеку, фреймворк, язык программирования, архитектуру ЭВМ… стоп, до последнего я ещё не докатился.    Речь сегодня пойдёт о JS-фреймворках. Нет, я не буду рассказывать про очередное готовое решение, не в том цель поста. Я лишь хочу посеять в ваших головах несколько простых идей, которые вы не встретите в документации ни к одному популярному фреймворку. А в конце мы постараемся сформировать видение идеальной архитектуры построения пользовательского интерфейса.        Длинный цикл отладки  Типичный цикл отладки выглядит так:      Редактирование кода.  Запуск приложения.  Проверка и обнаружение проблем.  Исследование их причин.    И цикл этот повторяется для любой опечатки. Чем быстрее разработчик поймёт где и почему ошибся, тем быстрее он всё реализует. Поэтому обратную связь программист должен получать как можно быстрее, прямо в процессе написания кода. Тут помогают средства среды разработки, которые в реальном времени анализируют набранный код и проверяют, будет ли это всё работать. Как следствие, очень важно, чтобы среда разработки могла получить из кода как можно больше как можно более конкретной информации. Чтобы этого добиться, используемый язык должен быть статически типизирован настолько на сколько это возможно. JavaScript же типизирован лишь динамически, от чего IDE пытается угадать типы по косвенным признакам (типичные паттерны, JSDoc-и), но, как показывает практика, даже у наиболее продвинутых сред разработки это получается плохо.        Однако, существует минималистичное расширение JavaScript, добавляющее в него опциональную статическую типизацию — TypeScript. Его отлично понимает даже, какой-нибудь простой текстовый редактор типа GitHub Atom.        На текущий момент TypeScript является наиболее оптимальным языком для разработки веб приложений. Разработчики AngularJS это уже поняли. Не опоздайте на поезд!    Вторым эшелоном в деле ускорения отладочного цикла идут автоматизированные тесты, позволяющие быстро проверить работоспособность и обнаружить место неисправности. К сожалению, многие фреймворки для тестирования сконцентрированы лишь на первой фазе (проверка), но о второй (локализация неисправности) частенько даже и не задумываются, хотя она не менее важна. Например, популярный тестовый фреймворк QUnit оборачивает все тесты в try-catch, чтобы не останавливаться на первом же упавшем тесте, а в конце нарисовать красивый отчёт, который довольно бесполезен в деле поиска причин неисправности. Всё, что он может выдать — название теста и некликабельный стектрейс. Но есть костыль — вы можете добавить в ссылку параметр ?notrycatch, который не всегда работает, и тогда тесты по идее должны свалиться на первой же ошибке, после чего в зависимости от режима отладки вы получите либо остановку отладчика в месте возникновения исключения, либо кликабельный стектрейс одного упавшего теста в консоли. Но идеальным было бы решение без костылей: в режиме останова на исключениях — останавливаться на каждом (а не только на первом), а в режиме логирования — логировать все падения в консоль с кликабельным стектрейсом. Это не так сложно, как может показаться — достаточно запускать тесты в отдельных обработчиках событий, и ни в коем случае не заворачивать их в try-catch.    В свете вышесказанного стоит подчеркнуть, что try-catch лучше не использовать не только в тестовом фреймворке, но и в любом другом, ведь перехватывая исключение вы теряете возможность остановиться отладчиком в месте его возникновения. Единственное разумное применение try-catch в JavaScript — это игнорирование ожидаемых исключений, как например, делает jQuery при старте, проверяя поддержку браузером некоторых фич. И именно поэтому такой костыль, как опция отладчика ""останавливаться не только на не перехваченных исключениях"" плохо помогает, так как даёт слишком много ложных срабатываний, которые приходится проматывать.    Последним гвоздём в гроб try-catch можно забить тот факт, что как минимум V8 не оптимизирует функции, содержащие эту конструкцию.    Код нужно писать так, чтобы он в любой момент мог спокойно упасть, не разломав всё приложение.    Именно из соображений удобства отладки в последней реализации атомов нет ни одного try-catch, зато есть обработка события error, по которому происходит возобновление синхронизации атомов с учётом упавших.    Где что лежит?      Как быстро перейти к объявлению сущности? Как быстро найти все места использования сущности? Как найти объявление сущности по имени? Эти детективные расследования снижают продуктивность разработчика и отвлекают от решаемой задачи. Статическая типизация, как было замечено выше, позволяет среде разработки понимать семантику кода: где какой тип объявлен, какой тип возвращает функция, какие типы я могу использовать в текущем контексте. Но не менее важно, располагать и именовать файлы по простым и универсальным правилам, потому как работа с файлами происходит не только и не столько средствами среды разработки. Например, группировать файлы имеет смысл не по типу, а по функциональности. Модули с родственными функциями — в более крупные модули. А коллекции разнообразных модулей одного автора — в пакеты. Именно эта логика заложена в архитектуре PMS (Package/Module*/Source). В ней, иерархия директорий в точности повторяет иерархию пространств имён в коде. Благодаря этому по имени сущности всегда можно понять где она должна лежать. Это свойство используется pms-сборщиком для построения дерева зависимостей модулей: он анализирует исходники на предмет использования сторонних модулей, потом сериализует полученный граф так, чтобы к моменту исполнения зависимого модуля, все его зависимости тоже были исполнены. И для этого не требудется каких-то специальных объявлений в коде. Никаких AMD, LMD, CommonJS. Никаких import, require, include. Вы просто пишете код, как если бы необходимые модули уже были объявлены в том же файле, что и ваш, а обо всём остальном позаботится сборщик. Это прямой аналог автозагрузки классов из PHP, но работает с любыми модулями, содержащими исходники на самых разнообразных языках.    Благодаря тому, что зависимости между модулями отслеживаются автоматически, становится очень просто добавлять новые модули и переносить существующие. Для создания модуля достаточно создать директорию. Для добавления в него кода на любом языке достаточно просто создать файл. Но самое главное — в релиз уходят только те модули, которые реально используются, а не все подряд. Это позволяет строить код фреймворка и библиотек не из нескольких крупных модулей, 90% функций которых не используется, а из множества микроскопических модулей, не приводя ко километровым портянкам инклудов. Для сравнения: JavaScript код ToDoMVC на AngularJS в общей сложности весит 1.1МБ (60КБ в ужатопережатом виде), а на $mol — 100КБ (8КБ в ужатопережатом виде). Сравните масштабы. Надо ли рассказывать какое приложение быстрее откроется на мобилке через EDGE? И не говорите, что сейчас 4G везде — в Москве даже EDGE во многих местах ловится с перебоями. А что нам предлагает современная индустрия? Подключать библиотеки целиком, а потом хитрой магией вырезать всё лишнее? Пожалейте мой CPU!    Микромодульная архитектура позволяет создавать компактные и быстрые приложения.    У многих сейчас наверняка уже поднялись руки выразить килобайты праведного гнева и возмущения моим непрофессионализмом в комментариях. Но позвольте отмотать время на несколько лет назад и напомнить об одной похожей ситуации. Ещё не так давно в тренде были XML технологии, все активно писали на XHTML, трепетали перед перед XSLT, а данными обменивались исключительно через XML. Если ты не ставил ""/"" в конце бестелесных тэгов или, упаси боже, не проходил html-валидацию, то на тебя смотрели как на профнепригодного. Но как-то больно много сложностей было с этим стеком технологий. Приходилось читать километровые спецификации, мириться с жёсткими ограничениями, вставлять кучу костылей, а светлое будущее всё не наступало — браузеры так и не довели поддержку XHTML до ума. Недовольство разработчиков росло, энтузиазм угасал, пока внезапно не пришло понимание, что с убогим JSON (по сравнению с мощным XML) работать проще и, главное, быстрее; что строгость и избыточность XML не даёт особого профита; что то, во что верили тысячи разработчиков, оказалось не самой лучшей идеей. Похожая ситуация была и с вендорными префиксами в CSS: сначала их копипастили руками, потом появились специальные утилиты, которые копипастят их автоматически, а теперь от них методично избавляются, так как они не решают никаких проблем, внося лишь излишнюю сложность. Но вернёмся к ""модулям"". Давайте напишем пару простых модулей на, например, RequireJS:    // my/worker.js define( function( require ) {     var $ = require( 'jQuery' )     return function () {         $('#log').text( 'Hello from worker' )         return 1     } } )  // my/app.js define( function( require ) {     var jQuery = require( 'jQuery' )     var worker = require( 'my/worker' )     var count = 0     return function () {         $('#log').text( 'Hello from app' )         count += worker()         count += worker()     } } )  Что не так с этим кодом:      Один и тот же модуль (который jQuery) в разных файлах имеет разные локальные имена. То есть программисту нужно постоянно держать в голове как jQuery называется в каждом модуле. Зачем нам возможность по разному именовать одну и ту же сущность? Чтобы всех запутать?  У нас нет простого доступа к переменной count. Мы не можете открыть консоль и просто набрать app.count, чтобы узнать какое там сейчас значение. Для этого необходимо изрядно пожонглировать отладчиком.  Каждый раз используя какую-либо сущность, нужно проконтролировать, чтобы она была ""импортирована"", а переставая её использовать надо удалить и эти ""импорты"". Существование специального инструмента для автоматической синхронизации списка импортов с кодом, подчёркивает их бессмысленность — это типовой, легко автоматизируемый инфраструктурный код, до которого программисту, вообще говоря, нет никакого дела.  Много лишнего кода, который зачастую просто генерируется по шаблону, так как писать руками одно и то же, никто не любит.    Как решить эти проблемы, не создавая новых? А очень просто, воспользуемся форматом jam.js:    // jq/jq.js  // my/worker.jam.js var $my_worker = function () {     $jq('#log').text( 'Hello from worker' )     return 1 }  // my/app.jam.js var $my_app = function () {     $jq('#log').text( 'Hello from app' )     $my_app.count += $my_worker()     $my_app.count += $my_worker() } $my_app.count = 0  Кода получилось существенно меньше и весь он по делу, при этом для работы с ним нам уже не обязательно нужна мощная среда разработки. Нам не приходится ломать голову в каком модуле как названа jQuery, ведь называется она везде одинаково — в соответствии с путём до неё в файловой системе. При этом мы всегда можем скопировать $my_app.count в консоль, чтобы посмотреть текущее состояние. Вот к чему стоит стремиться, а не к изоляции всего и вся.    Перила помогают не упасть в пропасть, но не стоит ими себя окружать.    А вот что действительно необходимо — это пространства имён, и структура директорий как нельзя лучше позволяет гарантировать отсутствие конфликтов в простанствах имён.    Раздувание кода      Если раньше были споры стоит ли грузить jQuery на сотню килобайт, то сейчас уже никого не смущает подключение мегабайтного фреймворка. Некоторые даже гордятся тем, что написали миллионы строчек кода для реализации не слишком сложного приложения. И если бы проблема была только в скорости загрузки этих слонов, да скорости их инициализации. Но есть и куда более существенные вытекающие из этого проблемы. Чем больше кода, тем больше в нём ошибок. Более того, чем больше кода, тем больше процент этих ошибок. Так что если вы видите огромный зрелый фреймворк, то можете быть уверенными, что на отладку таких объёмов кода была потрачена уйма человекочасов. И ещё уйму предстоит потратить, как на существующие ещё не замеченные баги, так и на баги, привносимые новыми фичами и рефакторингами. И пусть вас не вводит в заблуждение ""100% покрытия тестами"" или ""команда высококлассных специалистов с десятилетним опытом"" — багов точно нет лишь в пустом файле. Сложность поддержки кода растёт нелинейно по мере его разрастания. Развитие фреймворка/библиотеки/приложения замедляется, пока совсем не вырождается в бесконечное латание дыр, без существенных улучшений, но требующее постоянное увеличение штата. Так что, если кто-то предложит вам написать дополнительный код по превращению ВерблюжьихИдентификаторовДиректив в идентификаторы-директив-с-дефисами, только потому, что в JS традиционно используется одна нотация, а в CSS — другая, с ней не совместимая, то плюньте ему в лицо и воспользуйтесь универсальной_нотацией_идентификаторов, не требующей ни дополнительного кода, ни среды разработки со сложными эвристическими алгоритмами поиска соответствий.    Большой объём кода далеко не всегда означает большое число возможностей. Зачастую это следствие использования слишком многословных инструментов, переусложнённой логики и банальной копипасты (в том числе и кодогенерации). Не стоит впадать в крайности, давая всем переменным однобуквенные имена, но стоит насторожиться, видя десятки тысяч строк рисующие простую формочку на экране. Вы можете не обращать на это внимание, полагая, что вам не потребуется в нём разбираться, ограничившись лишь чтением документации; полагая, что публичного API вам хватит для любых хотелок; полагая, что багов либо нет, либо они будут быстро исправляться мейнтейнерами. Но практика показывает, что рано или поздно вам придётся лезть в эту груду кода, если не вносить изменения, то как минимум исследовать его работу. А чем больше кода, тем сложнее в нём разобраться. Но ещё сложнее разобраться в коде, изобилующем множеством абстракций. Небольшое число, грамотных абстракций позволяет значительно упростить код, но мы получаем резко противоположный эффект, когда лепим их без разбора. Фабрики, прокси, адаптеры, реестры, сервисы, провайдеры, директивы, декораторы, примеси, типажи, компоненты, контейнеры, модули, приложения, стратегии, команды, роутеры, генераторы, итераторы, монады, контроллеры, модели, отображения, модели отображения, презентаторы, шаблоны, билдеры, виртуальный дом, грязные проверки, биндинги, события, стримы… Во всём этом многообразии абстракций теряются даже опытные разработчики. А попробуйте объяснить новичку, как на таком фреймворке сделать простой компонент так, чтобы не плакать потом кровавыми слезами, глядя на то, что у него получилось.    Чем проще — тем лучше.    Сложность разработки      По мере роста приложения всё сложнее вносить в него изменения не разломав. Требуется учитывать всё большее число состояний одновременно, разбросанных по разным частям приложения. Множество неявных внутренних соглашений неизбежно начинают противоречить друг другу. Поэтому важно, чтобы за одно состояние отвечало лишь одно место в коде. Если вы меняете значение переменной из нескольких разных обработчиков событий, то рано или поздно вы получите неверное его значение, что будет выглядеть как глюк, на поиск причины которого придётся потратить не мало времени. Напротив, если вы будете выражать любое состояние как функцию от других состояний, то, даже если ваша переменная и примет неверное значение, вы всегда знаете где искать ошибку — в функции вычисления или получаемых ею данных.    Продолжая развивать тему, заметим, что такая функция не просто позволяет вычислить одно состояние на основе других, эта функция — задекларированная в коде зависимость между состояниями, инвариант, который должен быть поддержан в актуальном состоянии всё время жизни приложения. То есть, если изменяется состояние зависимости, то и все зависимые состояния должны быть обновлены автоматически, не полагаясь на внимательность и трудолюбивость разработчика.    Реактивная архитектура значительно упрощает поддержку.    Зачастую при разработке фреймворков основное внимание уделяется таким вещам как ""простота"", ""гибкость"" и ""скорость"", но практически игнорируется такое немаловажное качество как ""исследуемость"". Предполагается, что взявшийся работать с ним разработчик уже прочитал всю документацию, понял её правильно и вообще очень одарённый человек. Но правда жизни заключается в том, что документация зачастую не полна, плохо структурирована, на не родном языке и требует очень много времени для полного изучения, которого всегда не хватает, да и разработчик зачастую не имеет за плечами десяти лет опыта и досконального знания паттернов. Поэтому лучшая документация — это примеры. Лучшие примеры — это тесты. А лучший способ понять как оно работает — разобрать. Так что важно не изолировать внутреннее состояние, а предоставлять простой и удобный доступ к нему. Через консоль, через отладчик, через логи. Необходимо именовать одни и те же сущности одинаково в разных местах: имена переменных в разных модулях, имена модулей в разных контекстах, имена классов в скриптах, стилях и вёрстке и тд. А также необходимо добавлять информацию о не очевидных связях между сущностями. Например, посмотрите на этот код:    <div class=""my-panel"">     <button class=""my-button_danger""></button> </div>  Как тут можно догадаться, что оба этих элемента были добавлены в дерево компонентом my-page? Добавим недостающую информацию:    <div class=""my-page_content my-panel"">     <button class=""my-page_remove my-button_danger""></button> </div>  Теперь стало понятно куда копать, и кто виноват в том, что кнопка удаления страницы находится не на той панели. Другой яркий пример связан с парсингом. Когда вы применяете JSON.parse, то теряете информацию о расположении данных в исходном файле. Поэтому, когда при последующей валидации вы обнаруживаете ошибку, то не можете сообщить пользователю ""На такой-то строке обнаружен не валидный e-mail"", а вынуждены изобретать костыли вида ""Невалидный e-mail по пути departaments[2].users[14].mails[0]"". Напротив, при использовании формата tree вы всегда можете получить из узла информацию о месте его объявления:    core.exception.RangeError@./jin/tree.d(271): Range violation ./examples/test.jack.tree#87:20 cut-tail ./examples/test.jack.tree#87:11 cut-head ./examples/test.jack.tree#88:7 body ./examples/test.jack.tree#85:6 jack ./examples/test.jack.tree#83:0 test ./examples/test.jack.tree#1:1  Хлебные крошки не помешают как в пользовательском интерфейсе, так и в программном коде.    Компонентная декомпозиция      Самым важным аспектом любого фреймворка является реализация единого протокола взаимодействия. Собственно, в случае микромодульного фреймворка, единственное, чем занимается ядро — это организация взаимодействия модулей, как стандартных, так и пользовательских. В случае ui фреймворка, основной его задачей является организация взаимодействия компонент — этаких мини приложений, которые и сами по себе могут работать, но поддерживают и некоторый API для взаимодействия с другими компонентами, позволяя собирать из них как из кубиков лего более сложные компоненты, вплоть до полноценных приложений. Давайте рассмотрим, что нам предлагают современные фреймворки...    AngularJS      Объявление компоненты:    angular.module( 'my' ).component( 'panel' , {     transclude : {         myPanelHead : '?head',         myPanelBody : 'body'     },     template: `         <div class=""my-panel"">             <div class=""my-panel-header"" ng-transclude=""head""></div>             <div class=""my-panel-bodier"" ng-transclude=""body"">No data</div>         </div>     ` } )  Для объявления компоненты нам потребовалось написать немного скриптов, немного шаблонов и приложить к ним некоторый конфиг, описывающий API. Получилось весьма многословно и запутанно. Необходимо держать в голове, что ng-transclude содержит не просто текст, а имя параметра, значение которого будет вставлено внутрь элемента. Нужно внимательно поддерживать маппинг внешних имён параметров на внутренние. Нужно всем элементам расставить классы, чтобы их можно было стилизовать. А теперь попробуем воспользоваться этой компонентой:    <body ng-app=""my"">     <my-panel>         <my-panel-head>My tasks</my-panel-header>         <my-panel-body>             <my-task-list                 assignee=""me""                 status=""todo""             />         </my-panel-body>     </my-panel> </body>  Как видим, для каждого параметра нам пришлось добавить дополнительные тэги. Поскольку все эти тэги находятся в одном пространстве имён, то нам пришлось к каждому параметру добавить префикс с именем компонента (эти тэги как есть будут зачем-то вставлены в результирующее дерево). Да, в простейших случаях вы можете воспользоваться атрибутами, но если есть вероятность, что потребуется вставлять вложенную компоненту, а не просто строку текста, то приходится использовать такие вот громоздкие конструкции.    А теперь типичная ситуация: есть готовая компонента и нам надо её слегка кастомизировать. Например, добавить подвал. Беда многих подобных фреймворков в том, что реализованные на них компоненты слишком жёсткие и изолированные. Вы не можете просто отнаследоваться от компонента и слегка поменять его поведение. Вместо этого, вам необходимо либо вносить изменения в сам исходный компонент, усложняя его и заставляя уметь слишком многое:    angular.module( 'app' ).component( 'myPanelExt' , {     scope : {         myPanelShowFooter : '='     },     transclude : {         myPanelHead : '?head',         myPanelBody : 'body',         myPanelFoot : '?foot'     },     template: `         <div class=""my-panel"" my-panel-show-footer=""true"">             <div class=""my-panel-header"" ng-transclude=""head""></div>             <div class=""my-panel-bodier"" ng-transclude=""body"">No data</div>             <div class=""my-panel-header"" ng-transclude=""foot"" ng-if=""myPanelShowFooter""></div>         </div>     ` } )  Либо копипастить, создавая новый компонент очень похожий на старый, но слегка другой:    angular.module( 'app' ).component( 'myPanelExt' , {     transclude : {         myPanelHead : '?head',         myPanelBody : 'body',         myPanelFoot : '?foot'     },     template: `         <div class=""my-panel"">             <div class=""my-panel-header"" ng-transclude=""head""></div>             <div class=""my-panel-bodier"" ng-transclude=""body"">No data</div>             <div class=""my-panel-header"" ng-transclude=""foot""></div>         </div>     ` } )  ReactJS      Объявление компоненты:    class MyPanel extends React.Component {     render() { return (         <div class=""my-panel"">             <div class=""my-panel-header"">{this.props.head}</div>             <div class=""my-panel-bodier"">{this.props.body}</div>         </div>     ) } }  Уже гораздо лучше, хотя и осталась по прежнему неотделимая завязка на JS. Почему это плохо? Потому, что не везде JS является оптимальным языком программирования. В вебе у вас нет выбора. Но под iOS лучше бы подошёл Swift или ObjectiveC, под Android — Java, а под десктопы выбор языков вообще огромен, но на JS свет клином не сошёлся. По прежнему мы имеем проблему жёсткости и изолированности компоненты, так что с кастомизацией всё почти так же плохо, как и в AngularJS. ""Почти"", потому, что мы можем расчленить наш шаблон на мелкие кусочки, что позволит в дальнейшем их переопределять:    class MyPanel extends React.Component {     header() { return <div class=""my-panel-header"">{this.props.head}</div> }     bodier() { return <div class=""my-panel-bodier"">{this.props.body}</div> }     childs() { return [ this.header() , this.bodier() ] }     render() { return <div class=""my-panel"">{this.childs()}</div> }  class MyPanelExt extends MyPanel {     footer() { return <div class=""my-panel-footer"">{this.props.foot}</div> }     childs() { return [ this.header() , this.bodier() , this.footer() ] } }  Мы получили неплохую гибкость, но потеряли наглядность иерархии элементов. А использование XML синтаксиса в этом случае становится излишним. Особенно экстравагантно выглядело бы использование компоненты без расчленения на функции:    class MyApp extends MyPanel {     render() { return (         <MyPanel             head=""My Tasks""             body={                 <MyTaskList                     assignee=""me""                     status=""todo""                 />             }         />     ) } }  Polymer      Объявление компоненты:    <dom-module id=""my-panel"">     <template>         <div class=""header"">             <content select=""[my-panel-head]"" />         </div>         <div class=""bodier"">             <content />         </div>     </template>     <script>         Polymer({             is: 'my-panel'         })     </script> </dom-module>  Бросается в глаза получение параметров через селекторы, что делает использование компоненты весьма не наглядным:    <link rel=""import"" href=""../../my/panel/my-panel.html""> <dom-module id=""my-app"">     <template>         <my-panel>             <div my-panel-head>My tasks</div>             <my-task-list                 assignee=""me""                 status=""todo""             />         </my-panel>     </template>     <script>         Polymer({             is: 'my-app'         })     </script> </dom-module>  В целом, описание компоненты похоже на хак, который берёт чужое дерево и видоизменяет его до неузнаваемости. В хитросплетениях реального и теневого дерева довольно сложно разбираться.    Наследование пока не поддерживается, так что используем силу копипасты:    <dom-module id=""my-panel-ext"">     <template>         <div class=""header"">             <content select=""[my-panel-head]"" />         </div>         <div class=""bodier"">             <content />         </div>         <div class=""footer"">             <content select=""[my-panel-foot]"" />         </div>     </template>     <script>         Polymer({             is: 'my-panel-ext'         })     </script> </dom-module>  Отдельно стоит отметить проблему стилизации. Каждая компонента тянет с собой свои стили, которые по умолчанию изолированы от всех остальных компонент. С одной стороны, это позволяет использовать короткие имена классов в рамках одной компоненты, с другой же, добавляет проблем при необходимости кастомизировать визуализацию компоненты в каком-то конкретном контексте использования.    Сферический идеальный фреймворк      Давайте попробуем сформурировать, что необходимо для описания компонента. Как бы ни хотелось при построении интерфейса реиспользовать готовые компоненты как есть, их всегда требуется дотачивать под себя. То есть, описание компонента должно поддерживать наследование. Чтобы из компонент можно было собирать произвольные интерфейсы, требуется, чтобы их можно было вкладывать друг в друга произвольным образом. То есть, нам нужен полиморфизм. Чтобы одни и те же компоненты можно было использовать из разных языков, их описание должно быть декларативным, при этом синтаксис должен быть максимально простым и наглядным. Ну и статическую типизацию никто не отменял.    Как показано выше, XML совсем не удовлетворяет этим требованиям. Вообще, все попытки сделать из XML шаблонизатор выглядят как помесь ужа с ежом. То XML встраивается в JS, то JS встраивается в XML, а то и изобретается свой примитивный язык программирования с XML-синтаксисом.    Давайте воспользуемся tree-синтаксисом для объявления простой компоненты:    $my_header : $mol_view  Тут мы просто говорим, что компонента $my_header — наследник от $mol_view. Нас не волнует как и на каком языке реализована $mol_view, но мы утверждаем, что $mol_header должна быть реализована точно так же. Например, приведённое выше описание, может развернуться в следующее DOM-дерево:    <div id=""$my_header.app()"" my_header mol_view></div>  Как можно заметить, для элемента были автоматически сгенерированы некоторые атрибуты. Прежде всего это id — глобальный идентификатор компоненты. Он не зря имеет такой странный вид, ведь его можно скопировать и вставить в консоль, получив тем самым прямой доступ к инстансу компоненты, за этот элемент ответственной. Это очень сильно упрощает отладку и исследование чужого кода. Далее идут атрибуты, предназначенные прежде всего для стилизации. Вы можете определить стилизацию для базового компонента, а потом перегрузить её для наследника:    [mol_view] {     animation : mol_view_showing 1s ease; } [my_header] {     animation : none; }  Как видите, нам не потребовался даже css препроцессор, чтобы реализовать наследование в CSS. То есть, для декларации наследования у нас есть только одно место в коде — это описание компоненты. По этому описанию также может быть сгенерирован и TypeScript-класс для этой компоненты:    module $mol {     export class $my_header extends $mol_view {     } }  Компонент $mol_view может предоставлять стандартное свойство childs для определения списка дочерних узлов. Поэтому давайте добавим возможность объявления и перегрузки свойств:    $my_panel : $mol_view     head : null     body =No data     header : $mol_view         childs < head     bodier : $mol_view         childs < body     childs         < header         < bodier  Тут мы объявили свойства head и body, представляющие собой содержимое шапки и тела панели. После имени мы указали значение по умолчанию. Чтобы компонента была самодостаточной, значения по умолчанию должны быть всегда. Далее, мы определили свойства header и bodier, значения которых по умолчанию являются инстансами $mol_view с перегруженным свойством child — оно будет замещено соответствующим свойством из определяемой компоненты. Соответственно, при использовании панели мы можем переопределить любое из этих свойств, достигнув тем самым очень высокой гибкости:    $my_app : $my_panel     head : =My tasks     body : $my_task_list         assignee : =me         status : =todo  Однако, как и в случае с ReactJS, мы потеряли структуру. Но заметим, что после имени свойства при биндинге мы можем тут же указать и значение по умолчанию для него:    $my_panel : $mol_view childs     < header : $mol_view childs < head : null     < bodier : $mol_view childs < body =No data  В результате у нас получилось довольно компактное описание компоненты. При этом каждый вложенный элемент получил уникальное имя, которое можно использовать для генерации bem-like атрибутов:    <div id=""$my_panel.app()"" my_panel mol_view>     <div id=""$my_panel.app().header()"" mol_view my_panel_header></div>     <div id=""$my_panel.app().bodier()"" mol_view my_panel_bodier></div> </div>  В итоге мы сохранили иерархию, не потеряв в гибкости. Создадим вариант панельки с подвалом:    $my_panelExt : $my_panel childs     < header     < bodier     < footer : $mol_block childs < foot : null  Но что если нам нужна динамика? Например, показывать подвал в зависимости от какого-либо условия. Нет ничего лучше для описания логики, чем язык программирования. Поэтому воспользуемся TypeScript, чтобы добавить логику к декларативному описанию компоненты:    @ $mol_replace // перегружаем исходный класс class $my_panel extends $mol.$my_panel {      footerShowing() { return this.persist<boolean>('footerShowing') }      childs(){ return this.prop( () => [         this.header().get() ,         this.bodier().get() ,         this.footerShowing.get() ? this.footer().get() : null ,     ] ) }  }  Важно отметить, что компонента самодостаточна и без логики. Это позволяет верстальщику вывести одну и ту же компоненту в разных состояниях, не залезая в программный код, что очень сильно повышает его продуктивность:    $my_panel_demo : $mol_view childs     < simple : $mol_panel         footerShowing : false         body =I am simple panel     < footered : $mol_panel         footerShowing : true         body =I am panel with footer  А так как это описание в конечном счёте будет транслировано в typescript класс, то мы автоматом получим и статическую типизацию. И хотя IDE без специальных плагинов всё же не сможет вывести контекстные подсказки во view.tree коде, компилятор, тем не менее, проверит, что мы не используем необъявленные свойства, передаём им правильные типы значений и тп.    Производительность  В статьях про каждый второй фреймворк пишут, что он разрабатывается с прицелом на производительность, и что тот чуть ли не самый быстрый из всех. Иногда даже приводят в доказательство какие-нибудь синтетические тесты. Как правило код в таких тестах весьма далёк от того, что можно было бы встретить в реальном приложении. В погоне за попугаями теряется простота, гибкость, компонуемость, настраиваемость. Не редко можно встретить чуть ли не полный отказ от возможностей фреймворка ради скорости. Результат такого надругательства над кодом — производительность, которую можно достигнуть приложив знания, умения и время. Но куда важнее не максимальная скорость, которую можно достичь, а минимальная, которую вы получите написав идиоматичный для данного фреймворка код. Чем выше минимальная скорость, тем реже вам придётся браться за напильник, тем менее квалифицированные (и как следствие — менее дорогие) требуются разработчики, тем быстрее идёт разработка. Поэтому фреймворк, на котором сложно сделать медленно, предпочтительнее фреймворка, на котором можно сделать быстро.    Типичный jQuery рендеринг выглядит так: берём шаблон, генерируем новый HTML подставляя в него данные, вставляем в дерево, навешиваем события. Проблема этого подхода в том, что даже малейшее изменение данных требует повторного рендеринга всего шаблона. Это не заметно на малых объёмах данных, но рано или поздно данных становится много и всё начинает тормозить. А что более существенно, при ререндеринге сбрасываются скроллинги, введённые в поля значения и тому подобные вещи. Представьте, что вы пришли побриться, а цирюльник убивает вас, берёт ДНК и выращивает вашего клона без бороды. Решается это обычно вручную — находим нужные элементы и патчим их, что приводит к большому объёму очень хрупкого кода.    Всё, что делает ReactJS — это ускоряет рендеринг всего шаблона за счёт того, что рендерит его не напрямую в DOM дерево, а в промежуточное дерево JS объектов, которые уже сравниваются с реальным деревом и при наличии расхождения точечно меняет DOM. Уже лучше, конечно, но всё равно куча лишней работы: цирюльник берёт вашу ДНК, выращивает клона без бороды, раздевает вас обоих до гола и, педантично сравнивая волосяной покров, с хирургической точностью делает вас похожим на вашего клона, после чего клона отправляют в биореактор. Похожим образом поступает AngularJS с той лишь разницей, что сравнивает он не ваш клон и вас самих, а фотографии какого-то мужика вчера и сегодня, и если обнаруживает, что у того пропала борода, то сбривает её и вам. А что творится в Polymer после 100500 транспиляций и представить сложно.    Но нет ничего эффективней, чем просто показать цирюльнику фотографию чистого подбородка, удобно расположиться в кресле и быть уверенным, что всё, что тот сделает — это просто сбреет вам бороду.    Но вернёмся к бенчмаркам. Производительность всё же надо контролировать. Причём важна производительность не отдельных подсистем в рафинированных условиях, а общая производительность итогового приложения. Поэтому мы воспользуемся для тестирования широко известной коллекцией одного и того же приложения на разных фреймворках — ToDoMVC. Так как основная задача этого проекта — демонстрация идиоматичного кода на разных фреймворках, то он отлично подходит для интегральной оценки их производительности. По сети разбросано множество версий бенчмарка с ToDoMVC приложениями, поэтому пришлось попотеть, собирая из разных форков один ToDoMVC Benchmark.        Каждое приложение прогоняется через 3 теста:      Создание 100 задач через поле создания задач. Каждая следующая задача создаётся через setTimeout, что не позволяет фреймворку срезать путь и схлопнуть 100 операций рендеринга в одну. Визуально это выглядит как постепенное разрастание списка задач от 0 до 100. Этот шаг показывает насколько дорого фреймворку обходится формирование интерфейса.  Завершение всех задач, нажатием соответствующей кнопки. Этот шаг, наоборот, показывает насколько фреймворк умеет схлопывать множественные операции над моделью в одну операцию рендеринга, что важно, при массовом обновлении данных. Визуально это выглядит как зачёркивание всех задач.  Удаление задач по одной, нажатиями на кнопки удаления задачи. Тут опять используется setTimeout для предотвращения схлопывания 100 операций рендеринга в одну. Этот шаг показывает эффективность фреймворка по уничтожению интерфейса. Визуально это выглядит как постепенное исчезновение задач сверху списка. Такое удаление задач гарантирует, что все строки рано или поздно будут отрендерены, даже если фреймворк рендерит лишь компоненты, попадающие в видимую область.    Также стоит отметить, что каждое приложение прогоняется ровно один раз в отдельном фрейме, что не позволяет ему кешировать какие-либо объекты.    Особенности этой версии бенчмарка:      Защита от схлопывания рендеринга на первом и третьем шаге, но провоцирование на втором.  Вывод гистограммы в конце замеров. Подробности по времени выполнении каждого шага можно найти в консоли.  Автоматический подхват всех добавленных в ToDoMVC приложений. Те, которые с бенчмарком работают не корректно (около половины из всех), занесены в чёрный список, который можно найти в коде страницы. Патчи к этим решениям или к самому бенчмарку для совместимости с ними — приветствуются.  Чекбоксами можно включать и выключать отдельные приложения. Состояние чекбоксов запоминается.    А теперь уберём из тестирования совсем аутсайдеров и увеличим число задач вдвое:        Ожидаемым является увеличение времени работы не более чем в 2 раза. Однако лишь немногим решениям удалось добиться такой пологой динамики. Как правило замедление составило порядка 2.5, а в некоторых случаях даже превысило 3.    Неправильный выбор фреймворка может замедлить ваше приложение более чем в 10 раз.    Призыв к действию  Хватит это терпеть! Если вы, как и я, устали от этих всё более усложняющихся инструментов, которые не упрощают разработку, а меняют одни сложности на другие, то присоединяйтесь к разработке простого и эффективного сферического фреймворка в безвоздушном пространстве.    Что уже есть      Эффективная реализация реактивных переменных.  Модуль для работы с форматом tree.  Сборщик, отслеживающий зависимости и собирающий js+css пакеты из js/ts/css/view.tree исходников.  Реактивный рендерер.  Набор базовых компонент и несколько демонстрационных приложений.    Что ещё предстоит сделать      Переписать сборщик. Сейчас он написан на коленке и переусложнён.  Покрыть код тестами. Куда уж без них.  Заредизайнить существующие компоненты и продумать какие ещё необходимы.  Написать документацию.  Создать симпатичную домашнюю страничку.  Реализовать адаптеры для полупрозрачной интеграции в другие фреймворки.    Основные характеристики      Минимум абстракций и соглашений.  Статическая типизация.  Реактивная архитектура.  Высокая эффективность.  Компактный размер.  Простота компоновки.  Высокая кастомизируемость.  Высокая исследуемость.          	 		  	  		Ну что, за каким кактусом будущее?  		 		  		 			 				 					 				   				 					AngularJS, все на нём разрабатывают! 				 			 			 				 					 				   				 					ReactJS, состояния — зло! 				 			 			 				 					 				   				 					PolymerJS — последний писк веб стандартов! 				 			 			 				 					 				   				 					jQuery — наше всё! 				 			 			 				 					 				   				 					Сферический, я готов внести свою лепту в историю! 				 			 			 				 					 				   				 					Это всё фигня, в комментариях расскажу о единственно верном пути! 				 			 		 		 		Проголосовало 613 человек. Воздержалось 397 человек. 	      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.","javascript, jquery, jquery ui, angularjs, todomvc, reactjs, polymer.js, $mol",Идеальный UI фреймворк
"У нас в команде кто-то ходил в аспирантуру, кто-то на военную кафедру, кому-то служить не позволили здоровье или другие обстоятельства. Короче, служили трое.     Что интересно, все трое вместо обычных армейских баек и страшилок рассказывают, как служба помогла им прокачать айтишные и околоайтишные навыки. Верстальщик, например, научился чинить технику. Сисадмин стал стрессо- и отказоустойчив. А джуниор понял, что по жизни хочет заниматься веб-разработкой.     “У вас неправильные полигоны на танках”, или судьба верстальщика      Наглядное пособие, как спрятать рояль в кустах.     Жора явился в военкомат сам, когда в ИТ-колледже его не допустили до сдачи диплома. Верстку он уже выучил, служить считал своим долгом, а терять год после учебы не хотел.    За права на вождение грузовика и отменное здоровье парня определили служить водителем-механиком танка. Занятия на полигоне шли по несколько дней в неделю, а в перерывах Жора был компьютерщиком в штабе.     “Когда я только попал на службу, то считал, что хорошо владею офисным и графическим ПО. Как же я ошибался! В армии вся работа за компьютером происходила по принципу “есть задача => делай, что хочешь, но сделай” — то есть, с любым ремонтом и любым ПО надо было разобраться с нуля и влет”.      Жорин кабинет был на заглавной картинке, а это — танкодром, где он проводил остальное время.    Первые полгода прошли в учебной части: там был комп с одноядерным процессором (1.3 ГГц), 512 мегабайтами оперативки и Windows XP.     “Казалось бы, для делопроизводства нормально. Но комп лагал невыносимо. Проблема выяснилась быстро: каждые полгода за него садился новый человек и оставлял свой программный мусор. Программную чистку, чистку реестра на нем не проводили с момента покупки. Я сел, разобрался и все сделал. В механическом плане тоже пришлось повозиться: кулеры забились клочьями пыли, термопаста давно высохла”.     А вот в боевой части все было по-правильному с самого начала, потому что Жора работал на ноутбуке своего капитана.     “Командир роты купил технику на свои, и за ноутбуком я следил пристально: как механически, так и программно. Даже приобрел модем и выходил в интернет, чтобы обновлять операционку и все ПО”.      Жорин танк стреляет ночью.    В общем, за год службы Жора просто колоссально поднял свой уровень работы с программным обеспечением и техникой. И про танки тоже многое узнал: например, чтобы холодная машина поехала при -30, нужно прогревать ее полчаса.    “Родина вас не забудет, но чтобы поспать осталось два часа”, или становление сисадмина      Володя попал в армию, потому что днем вместо учебы тянул интернет по квартирам, а вечером подрабатывал оператором в колл-центре. В итоге, в институте пришлось взять академ. Следом за академом пришел аэродром:     “Мы попали на место службы зимой, и как-то снега навалило столько, что нас подняли ночью — откапывать фонари на взлетно-посадочной полосе. А ВПП — это пять километров и штук 500 фонарей”.       Сам фонарик небольшой, но также надо расчищать снег в радиусе нескольких метров от него.    “В пять утра наш прапорщик сказал: “Товарищи солдаты, Родина вас не забудет. До завтрака осталось 2 часа — вот, можете поспать”.    На пути к казарме Володя решил: дальше ты либо с лопатой, либо с бумажками. Уже через месяц он занимался в батальоне делопроизводством. Так у него появилась своя комнатка с тремя компьютерами. Локальную сеть между ними он протянул сам.       Сами машины мало отличались от тех, что стояли в часть у Жоры.    Не спать по ночам по-прежнему приходилось: например, во время учений, когда за несколько часов нужно было с нуля освоить редактор Visio для Windows и сделать в нем схему приема сигналов от других войсковых частей.     Заправить картриджи, установить программы без флешки и CD-rom’ов (по требованиям безопасности) — все это тоже было, в любое время. Это вырабатывало стрессо- и отказоустойчивость, а заодно приносило бонусные увольнительные и железо для “своих” машин.       Иногда вместо ужина Володя оставался, чтобы апгрейдить свой компьютер.    Опыт настройки компьютеров, рабочих мест и оргтехники пригодился на гражданке. Вернувшись и восстановившись на вечернем, Володя стал искать работу помощником сисадмина. Но мы взяли его сисадмином сразу. Потому что толковый.     Ракетная шахта в Ростове или выбор джуниора    Вадим закончил ИТ-колледж и успел поступить в вуз, но повестка уже ждала и пришлось брать “отпуск в армию”.     Служил он практически в центре родного города: на территории артиллерийского училища была ракетная шахта в натуральную величину, точная копия центра управления, муляжи ракет.       Вот это все Вадим протирал тряпочкой, тестировал и настраивал, прежде чем в учебный модуль запускали курсантов-ракетчиков.     “Меня брали в основном на физические работы. Но как-то подошел офицер: “Компьютерщик же? У нас Call of Duty лагает”.     С тех пор, если что настроить, починить или на компьютере сделать — звали. Но это было редко. О том, что парень айтишник, всерьез вспомнили только перед дембелем. Предложили остаться по контракту в отделе внутренней безопасности и защиты государственной тайны. “Я подумал — а почему нет. Стабильность, льготы, обучение”.     Оказалось, что кандидатуру будут рассматривать почти год. Вадим вернулся домой, стал ждать и вспоминать выученный в колледже C# и самостоятельно освоенный JavaScript. В итоге, победила страсть к веб-разработке. Вадим сходил на собеседование к нам и забыл о карьере военного.    P.S. В общем, звание «компьютерщика» и тяга к практическим знаниям, они везде выручают. А вы как провели время до 27 лет?","армейское ит, истории, армия, армейский способ",Как наши технари в армии служили
"К концу 2015 года в распоряжении JavaScript разработчиков образовалось огромное количество инструментов. В этой экосистеме легко потеряться, поэтому успешные команды следуют выработанным правилам, которые позволяют не терять время и сохранять здоровье проектов. Под катом перевод статьи 2016 года от команды Heroku, в которой они рассказывают о десяти привычках веб разработчиков, у которых все работает и ничего не болит. Скорее всего 80% написанного вы уже знаете – тем интереснее вам будет прочитать об оставшихся двух приемах!      1. Начинайте новый проект командой npm init  Эта команда сразу создаст правильный package.json, основываясь на имени директории проекта и ваших ответах на вопросы визарда:    $ mkdir my-awesome-app $ cd my-awesome-app $ npm init --yes   Использование флага --yes позволяет не отвечать на вопросы визарда, а в поле engines получившегося package.json можно сразу установить нужную версию ноды (node -v):    ""engines"": {   ""node"": ""4.2.1"" }   2. Используйте «умный» .npmrc  По умолчанию npm не записывает установленные зависимости в package.json (а за зависимостями надо следить всегда!).    Если вы воспользуетесь флагом --save для автоматического обновления package.json, то npm сохранит имя зависимости с префиксом ^. Семантическое версионирование ставит ваши зависимости под угрозу, так как при чистой инсталляции в проект могут установиться другие версии зависимостей, и кто знает, какие в них могут быть баги и проблемы. Такой способ хорош во время разработки, но не в продакшне. Одно из решений – сохранять точную версию зависимости:    $ npm install foobar --save --save-exact   А еще лучше – поменять глобальные настройки npm в ~/.npmrc, чтобы все происходило автоматически:    $ npm config set save=true $ npm config set save-exact=true $ cat ~/.npmrc   При такой конфигурации установка зависимостей сохранит точную версию и ваш проект никогда не настигнет жуткий “version drift”!    Если вы предпочитаете последние версии зависимостей во время разработки, плюс заморозку для прода, то вы можете воспользоваться функцией shrinkwrap. Такая конфигурация потребует несколько больше усилий, но позволит более гибко подойти к версионированию зависимостей.    3. Не упустите уходящий поезд ES6  Начиная с 4-й версии Node.js вы можете использовать многие функции ES6. Начните использовать уже сейчас простые и удобные улучшения синтаксиса, которые сделают ваш код проще и понятнее:    let user = users.find(u => u.id === ID); console.log(`Hello, ${ user.name }!`);   4. Придерживайтесь нижнего регистра  Некоторые языки рекомендуют именовать файлы так же, как классы. Например, файл, имплементирующий MyClass, предлагается называть MyClass.js. Не делайте так в node. Используйте нижний регистр:    let MyClass = require('my-class');   Node.js это редкий пример линуксовой программы с великолепной поддержкой кроссплатформенности. Несмотря на то, что OSX и Windows будут считать файлы myclass.js и MyClass.js одним и тем же файлом, Linux так считать не будет. Если вы хотите писать кросс-платформенный код, вам необходимо именовать файлы в том же регистре, который вы используете для require.    Самый простой пусть сделать все правильно – это использовать нижний регистр, то есть my-class.js (примечание переводчика: или my_class.js, что все-таки популярнее среди программистов, нежели css нотация).    5. Кластеризируйте ваше приложение  Так как процесс node ограничен одним ядром и полутора гигабайтами оперативной памяти, деплой некластеризованного приложения на мощный сервер является тратой полезных ресурсов впустую. Чтобы использовать несколько ядер и много памяти, добавьте в свое приложение поддержку кластеризации. Даже если сейчас вы запускаете приложение на одноядерной vps с гигом памяти, добавление кластеризации будет хорошим заделом на будущее.    Лучший способ найти оптимальное количество процессов это, конечно, тестирование. Но для начала подойдет значение по умолчанию, предлагаемое платформой. Ну и адекватный fallback, конечно же:     const CONCURRENCY = process.env.WEB_CONCURRENCY || 1;   Использование кластеризации позволяет не изобретать велосипед для управления потоками. Если вам нравится разделение по файлам на “master” и “worker”, то попробуйте forky. Если же вы предпочитаете единую точку входа, то throng.    6. Следите за переменными окружения  Используйте переменные окружения вместо конфиг файлов. Для этого установите node-foreman:    $ npm install --save --save-exact foreman   Затем создайте Procfile, чтобы указать тип приложения:    web: bin/web worker: bin/worker   И установите альяс для запуска:    ""scripts"": {   ""start"": ""nf start"" }   Теперь, чтобы изменить настойки окружения, достаточно создать (и добавить в .gitignore) файл с именем .env, который будет автоматически загружен node-foreman:    DATABASE_URL='postgres://localhost/foobar' HTTP_TIMEOUT=10000   С такими настройками достаточно одной команды npm start, чтобы запустить web и worker процессы, а затем прменить к ним настройки в соответствии с переменными окружения. Это намного проще, чем 'config/abby-dev.js', 'config/brian-dev.js', 'config/qa1.js', 'config/qa2.js', 'config/prod.js', итд.    7. Избегайте мусора  Node.js (в лице движка V8) использует «ленивый» и «жадный» сборщик мусора. С лимитом по умолчанию в полтора гигабайта он часто ждет до последнего, прежде чем освободить не используемую память. Если у вас постепенно растет использованная память, это может быть не ее утечка, а вполне штатное поведение сборщика мусора node.    Для лучшего контроля за сборщиком мусора, в Procfile можно поместить команды для движка V8:    web: node --optimize_for_size --max_old_space_size=920 --gc_interval=100 server.js   Это особенно важно, если ваше приложение запущено на машине с менее чем полутора гигами доступной памяти. Например, для контейнера с 512 мегабайтами памяти строка аргументов может выглядеть вот так:    web: node --optimize_for_size --max_old_space_size=460 --gc_interval=100 server.js   8. Используйте хуки  “Lifecycle scripts” в node.js предоставляют широкие возможности для автоматизации. К примеру, если вам нужно что-нибудь запустить перед сборкой приложения, вы можете воспользоваться скриптом preinstall. Нужно собрать ассеты с помощью grunt, gulp, browserify или webpack? Используйте скрипт postinstall. Все эти скрипты можно задать прямо в package.json:    ""scripts"": {   ""postinstall"": ""bower install && grunt build"",   ""start"": ""nf start"" }   И конечно вы можете использовать переменные окружения для управления скриптами:    ""postinstall"": ""if $BUILD_ASSETS; then npm run build-assets; fi"", ""build-assets"": ""bower install && grunt build""   Если скрипты стали слишком сложные – перенесите их в файлы:    ""postinstall"": ""scripts/postinstall.sh""   Важно: скрипты в package.json выполняются с ./node_modules/.bin автоматически добавленным в PATH, так что вы можете напрямую вызывать локально установленные npm пакеты, такие как webpack. Это позволяет вообще не устанавливать пакеты глобально.    9. Only git the important bits  Мне настолько понравился этот заголовок, что я оставил его без перевода. Красивая игра слов – «гитуйте только важные биты» = «добавляйте в git только то, что важно». Большинство приложений состоят из файлов двух типов: которые можно или нельзя сгенерировать автоматически. Когда вы используете систему контроля версий, избегайте добавлять в нее файлы, которые можно сгенерировать автоматически.    Например, у вашего приложения есть директория node_modules с установленными зависимостями. Да, есть люди, которые добавляют ее в систему контроля версий! Не надо так делать. Указание нужных версий зависимостей в package.json намного надежнее, так как npm install не очищает node_modules, и если там уже будут файлы из системы контроля версий, может случиться беда.    Также добавление генерируемых файлов производит много лишнего шума в логах и нотификациях вашего репозитория. Более того, некоторые зависимости требуют компиляции при установки, так что добавление установленной версии в репозиторий делает ваше приложение непортабельным и может привести к странным ошибкам.  По тем же причинам не стоит добавлять bower_components или скомпилированные grunt ассеты. Если в проекте директория node_modules уже добавлена в систему контроля версий, то ее всегда можно убрать, например, вот так:    $ echo 'node_modules' >> .gitignore $ git rm -r --cached node_modules $ git commit -am 'ignore node_modules'   Также я обычно добавляю в игнор логи npm, чтобы не засорять код:    $ echo 'npm-debug.log' >> .gitignore $ git commit -am 'ignore npm-debug'   Игнорируя эти менее важные файлы, вы уменьшаете размер репозитория, упрощаете коммиты и не получаете конфликтов слияния.    10. Упрощайте  Предсказания в айти – дело неблагодарное. Но я все-таки сделаю одно. Предсказываю, что для JavaScript 2016 год станет годом упрощения.  Все больше программистов упрощают архитектуру своих решений. На смену монолитных MVC решений приходит статический фронтенд, который так удобно раздавать через CDN, и Node.js бэкенд для динамических данных.    Мы также начали замечать, насколько сложные системы сборки усложняют наши проекты. На острие прогресса разработчики упрощают эти решения, в частности, используя npm скрипты вместо bower, gulp и grunt.    И, наконец, в 2016 году мы будем упрощать написанный нами код. В ряде случаев это будет отказ от лишних фичей, как показал Douglas Crockford в своих The Better Parts. Или, наоборот, упрощение придет от добавления новой функциональности. Такой, как мои любимые async и await. В ноде их пока нет, но всегда можно воспользоваться транспайлером, например BabelJS.    Попробуйте не соревноваться друг с другом в том, сколько новых технологий можно запихнуть в один проект, а сфокусироваться на упрощении вашей работы.","es6, es7, javascript, node, npm",10 привычек довольного node.js разработчика
"Использование готового и бесплатного скрипта, о котором пойдет речь, значительно упрощает и ускоряет процесс переноса исходников из Photoshop в Sketch. Но я думаю, скрипт может пригодится и для тех, кто с указанной программой не работает. Он экспортирует слои в формат .svg, избавляя от необходимости по одному генерировать SVG файлы.         Требования:     — Windows или Mac OSX;   — Photoshop CS5, CS6;   — Adobe Illustrator.    Загрузить скрипт PSD в SVG    Как использовать скрипт  1. Скачать скрипт по инструкции, описанной ниже.  2. Перетянуть в свою папку скриптов Photoshop («Adobe Photoshop/presets/scripts»).  3. (ОПЦИОНАЛЬНО) Перейти в Photoshop и сделать для скрипта горячую клавишу «Edit» menu –> Keyboard Shortcuts и потом в File –> Scripts –> PS to SVG. Как вариант, можно выбрать сочетание из CMD+ALT+E.   4. В файле PSD в конце векторных слоев, которые будут экспортироваться, нужно дописать префикс "".svg"". Например, слой под названием “Sky” преобразуется в “Sky.svg”.  5. После переименования слоев достаточно активировать скрипт горячей клавишей или через меню File –> Scripts –> PS to SVG. И файлы будут в той же папке, где хранятся PSD.    Примечание: чтобы скрипт работал быстрее, перед его активацией нужно открыть Illustrator.    Проблемы, которые встречаются:   — скрипт не совместим с новыми версиями Photoshop CC;   — дополнительно к SVG, скрипт создает и файлы AI (но их можно просто удалить);   — скрипт работает только с векторными слоями, не с группами слоев.    Если скрипт не работает:   — убедитесь, что Вы помещаете "".svg"" (DOT SVG) в конце названия слоя, который нужно экспортировать. А также, что каждый из слоев — в единственном экземпляре;   — проверьте, запущен и работает ли Illustrator;   — перед началом процесса убедитесь, что все экспортируемые слои являются замкнутыми векторами.    В некоторых случаях при экспорте исходников может наблюдаться искажение цвета. Тогда в скрипт после строки:     ""bt.body += ""app.open("" + s.svgFile.toSource() + "", DocumentColorSpace.RGB, dtOpts);""; ""    следует вставить строку:    ""bt.body += ""app.executeMenuCommand ('doc-color-rgb');"";""","скрипт, рhotoshop, illustrator, svg, psd, экспорт, sketch",Бесплатный скрипт для Photoshop: экспорт векторных слоев из PSD в SVG
"Не смотря на наличие нескольких статей на Хабре, про LVM2 и производительность Thin Provisioning, решил провести своё исследование, так как имеющиеся показались мне поверхностными.    Кому интересно, добро пожаловать под кат.      Немного о LVM2  Собственно, LVM это система управления дисковым пространством. Позволяет логически объединить несколько дисковых пространств (физические тома) в одно, и уже из этого пространства (Дисковой Группы или группы томов), можно выделять разделы (Логические Тома), доступные для работы. ОС получает к ним доступ через DevMapper.  Логические тома могут свободно размещаться в дисковой группе, занимая непрерывные или разрывные диапазоны адресов, или находится частично на разных дисках группы. Логические тома можно с легкостью изменять в размерах (а вот ФС не все так могут) или перемещать между дисками группы, а также создавать мгновенные снимки состояния логического тома (снапшот). Именно снапшоты и представляют главный интерес в статье.    Как вообще работает LVM  Идея тут проста, все вертится вокруг таблиц соответствия логических и физических адресов.  Общая схема такая: Физические тома отражаются на Дисковую группу. Дисковая группа отражается на Логические тома.  Снапшоты чуть иначе: собственная таблица ассоциирует блоки оригинального тома и блоки снапшота (копия блоков оригинала, до их модификации). Работает по принципу копирования при записи (CoW, делается копия блока оригинального тома, потом происходит запись этого блока в оригинальном томе). Создать снапшот от снапшота невозможно.    Тонкие тома работают чуть иначе. В группе томов создается специальный том (пул тонких томов, на самом деле состоит из двух, для данных и мета данных), из этого пула происходит выделение виртуального пространства для тонких томов. Пространство виртуально, потому как до момента реальной записи, блоки не выделяются из пула. И вообще, можно задать виртуальный размер тома, больше чем пул. Когда том распухнет до предела, система заморозит запись в него, до увеличения размеров пула. Снапшоты тут похожи на своих «толстых» братьев, но оперируют уже не блоками, а ссылками на блоки. То есть, после создания снапшота, запись в оригинал происходит так-же как и до создания снапшота (перезаписываемые блоки выделяются из пула). Есть возможность создавать снапшот от снапшота, а также можно указывать внешний том оригинала (размещенный вне тонкого пула в той же группе томов, защищенный от записи).    Производительность  Производительность толстых томов без снапшотов равна обычным дисковым разделам (разница очень мала), а как обстоят дела со снапшотами?  Тесты показали ад и ужас. Из-за CoW, операции записи в оригинальным том замедляются катастрофически! И чем больше снапшотов, тем хуже.  Несколько исправить положение дел может задание большего размера фрагмента (по умолчанию, это 4 килобайта, что дает большой объем маленьких iops). Ситуация несравнимо лучше, если писать не в оригинальный том, а в снапшот.     Тонкие тома показывают более сбалансированную картину. Скорость работы слабо зависит от числа снапшотов, и скорость значительно ниже, чем для простого толстого тома без снапшотов и близка к скорости записи в толстый снапшот. Основной вклад в тормоза дает процесс выделения места (фрагмент размером в 4 килобайта).    Итоги:    Простой раздел впереди по скорости, но не эффективен по занимаемому пространству  Толстый том быстрый, пока нет снапшотов, имеет среднюю эффективность по занимаемому пространству  Тонкий том самый медленный, но его скорость слабо зависит от снапшотов, самая большая эффективность по пространству    Самая быстрая ФС (в режиме записи): ext4, затем xfs, в конце ext3    Графики и расчетТаблица с данными для расчета                      Методика тестирования  Создавался раздел или логический том (толстый и тонкий) размером в 50Gb. Полученное пространство форматировалось в ext3, ext4 и xfs. После очищались все буферы и запускалась распаковка tgz архива (размещен в tmpfs в памяти) с образом Centos 7.3 Core (развернут Asterisk и FreePBX) 2Gb объемом в распакованном виде. В конце снова чистил буферы. Замер времени идет от начала распаковки, до финальной очистки буфера.  Для LVM создавался снапшот, а затем в оригинальном томе создавался каталог, куда распаковывался архив повторно. Процедуру повторял пять раз (не удаляя ни снапшотов ни копий файлов из оригинального тома).  Затем, однократно распаковывал архив в каждый снапшот.  В тестирование участвовал:    Gigabyte Z77X-D3H  WDC WD7500AAKS-00RBA0  Intel® Core(TM) i5-3570  ОЗУ 16Gb      Надежность  Оценивать надежность я буду просто, по уровню сложности механизма доступа к информации. Чем он сложнее, тем его надежность ниже (к реальной надежности это имеет довольно далекое отношение).  Самым надежным, выглядит конечно обычный раздел. Пространство линейно, никаких промежуточных абстракций. Ломаться особо не чему. Из клёвых плюшек нет ничего.  Второе место займет Толстый Том. Появившиеся абстракции конечно не делают его надежнее, но добавляют массу гибкости. Восстанавливать данные из рассыпавшегося LVM не так уж и легко, но скорее всего, большинство томов будут размещены линейно, с небольшой фрагментацией (и эти фрагменты возможно будут крупными, вряд ли вы будете расширят разделы маленькими блоками).  Самыми ненадежными выглядят Тонкие Тома. Сложный механизм выделения места, изначально фрагментированное пространство (но не сами данные, они то как раз размещены компактно и даже линейно). Повреждение таблицы трансляции адресов, сделает данные крайне сложно читаемыми. К слову, надежность btrfs, zfs или ntfs на более худшем уровне. У тонких томов только распределение пространства в опасности, а у ФС еще и своих абстракций полно.    Применение в продакшене  Поддержка Thin Provisioning появилась в RedHat Enterprise Linux 6.x, а в 7-ой ветке стала режимом по умолчанию для LVM, еще на этапе установки, что может косвенно свидетельствовать о надежности решения, а простой LVM2 трудится уже очень давно, и серьезных проблем не вызывает.  Имея на руках оценки производительности, можно принять решение о применении той или иной технологии для различных сценариев.  К примеру, толстые тома будут хорошо работать для виртуализации массы однотипных машин, без необходимости бекапа на лету (один мастер образ, и куча снапшотов для целевых виртуалок, снять снапшот со снапшота уже нельзя). Расход места будет приемлемым, а производительность и гибкость будут на высоте. В таком сценарии редко надо писать в образ оригинала.  А тонкие тома будут достойно показывать себя как в сценарии выше (просто будут значительно медленнее), так и в других, и дадут возможность создавать снапшоты от снапшотов, и т.д. Ко всему прочему, скорость тонких томов может быть близка к толстым, после выделения места под запись или увеличения размера фрагмента.        	 		  	  		Мотивирующий опрос  		 		  		 			 				 					 				   				 					Помогла ли вам статья в понимании LVM и его применимости? 				 			 			 				 					 				   				 					Сам тестировал, ничего нового 				 			 			 				 					 				   				 					Смотрел чужие тесты, ничего нового 				 			 		 		 		Проголосовало 73 человека. Воздержалось 83 человека. 	  		  	  		Какой вид разделов применяете чаще всего?  		 		  		 			 				 					 				   				 					Простой том 				 			 			 				 					 				   				 					Толстый том 				 			 			 				 					 				   				 					Тонкий том 				 			 		 		 		Проголосовал 81 человек. Воздержалось 76 человек. 	  		  	  		Буду мигрировать на?  		 		  		 			 				 					 				   				 					Простой том 				 			 			 				 					 				   				 					Толстый том 				 			 			 				 					 				   				 					Тонкий том 				 			 		 		 		Проголосовало 29 человек. Воздержалось 114 человек. 	  			 	 		Провести ли большое тестирование производительности? 		 		 				 					83%(24) 					Да 								 				 					17%(5) 					Нет 								 		 		 		Проголосовало 29 человек. Воздержалось 18 человек. 	      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.","lvm, lvm2, thin provisioning, linux","Что нам стоит LVM построить (принцип работы, производительность, Thin Provision)"
"    Это небольшой отчет о том, как я решил написать метафункцию is_function для С++98/03, таким образом, чтобы не нужно было создавать множество специализаций для разного количества аргументов.      Зачем, спросите вы, в 2016 году вообще подобным заниматься? Я отвечу. Это challenge. Кроме всего прочего, эта, сперва чисто теоретическая работа из разряда «возможно или нет», вскрыла некоторые проблемы в современных компиляторах. Всех, кому не чуждо подобное настроение, приглашаю взглянуть.    Постановка  Немного теории  Реализация №0  Реализация №1  Реализация №2  Вместо заключения    Постановка      Метафункция is_function<T> возвращает true если тип T является типом «функция», и false, если нет. Итак, нам требуется написать метафункцию без использования многочисленных специализаций для разного количества типов аргументов у исследуемого типа функции. На пример реализации со специализациями вы можете посмотреть в boost. На данный момент там 25 аргументов максимум и занимает все это достаточно много места.      Зачем делать специализации? В С++11 появилось замечательное средство — variadic templates. Появилось оно потому что назрело, можно сказать, наболело. Это средство позволяет обрабатывать вот такие последовательности аргументов шаблона   some_template<A1, A2, A3 /*, etc. */>как единый «пакет» параметров, parameter pack. Parameter pack помогает делать обобщенные специализации, перегрузки и подстановки в случаях, когда количество аргументов неизвестно. Именно с помощью этого средства в С++11 реализуется is_function. В С++98/03 такого средства не было. Это значит, что в общем случае, если нам необходимо было обеспечить разное количество аргументов в зависимости от ситуации, приходилось делать перегрузки и специализации «на все случаи жизни». Если вы посмотрите на реализации таких библиотек как variant или mpl в boost, то убедитесь в изобилии подобного кода (иногда его генерируют препроцессором). Если нам требовалось определить, является ли тип T функцией R(A1, A2), то самым простым и очевидным решением было создать соответствующую специализацию:  	template <typename F> 	struct is_function { /* .... */ };  	template <typename R, typename A1, typename A2> 	struct is_function<R(A1, A2)> { /* .... */ }; Зачастую, сделать по-другому было просто невозможно. Не хочу чтобы вы думали, будто бы я недоволен реализацией в boost – их решение наиболее переносимо, поэтому наиболее правильно, особенно в контексте такой библиотеки. Но мне было интересно в поставленной задаче обойтись без этого.      В общем, я один из тех, кому не повезло (хотя это как посмотреть) по долгу службы все еще работать с C++03. Поэтому моя озабоченность совместимостью со старым кодом не должна вас удивлять. Кто-то может сказать: «да далось тебе это старье, на дворе 2016 год!». С этим можно согласиться, но, кроме чисто субъективного ощущения соревнования, получилось извлечь из этого и определенную пользу. Да и дух С++03, в силу обозначенных выше причин, все еще не успел выветриться. Поэтому так, just for fun.      Прежде, чем мы перейдем к описанию, хочу предупредить, что от читателя потребуется понимание что такое SFINAE и базовых принципов написания compile-time проверок, а также я не стану точно переводить формулировки из стандарта. Предполагаю, что заинтересованный читатель, при желании, в состоянии справиться с этим самостоятельно.    Немного теории      Если классический подход со специализациями нас не устраивает, то как же тогда быть? Давайте попробуем подумать немного иначе. Какие есть свойства у типа функции, которых более нет у других? Обратимся к стандарту (С++03):  4.3/1  An lvalue of function type T can be converted to an rvalue of type “pointer to T.” The result is a pointer to the function.Итак, функция может быть неявно преобразована в указатель на функцию. Такое же свойство есть у массивов: массив неявно преобразуется в указатель. Посмотрим что есть еще:  8.3.5/3  After determining the type of each parameter, any parameter of type “array of T” or “function returning T” is adjusted to be “pointer to T” or “pointer to function returning T,” respectively.Это значит, что тип «функция», будучи указанным как параметр другой функции, неявно приобретает свойства указателя. На основе этого, в параграфе 13.1, описано, что вот такая декларация  	void foo(int ());соответствует такой (т.е. это одно и то же):  	void foo(int (*)());Вот это мы уже можем использовать. Мы можем написать проверку на основе этого и определить функция перед нами или нет. Впрочем все не так просто, как кажется, но об этом позже. А пока давайте посмотрим что еще можно использовать, основываясь на типе «функция»:  8.3.4/1  In a declaration T D where D has the form      D1 [ constant-expressionopt]  and the type of the identifier in the declaration T D1 is “derived-declarator-type-list T,” then the type of theidentifier of D is an array type. T is called the array element type; this type shall not be a reference type, the (possibly cv-qualified) type void, a function type or an abstract class type.Ага, это тоже интересно. Т.е. мы не можем получить массив из элементов типа «функция». Наравне с этим, мы не можем получить массив ссылок, массив void и массив с элементами типа абстрактного класса. Если написать проверку, которая отсечет остальные варианты, то мы сможем точно определить функция перед нами, или нет.      Резюмируем. У нас есть две отличительные особенности функций. Тип «функция»    «мутирует» в указатель, если используется в качестве типа аргумента другой функции  не позволяет создавать массивы с элементами своего типа      Эти свойства мы и будем использовать в реализации задуманного.     Реализация №0, предварительная  Я хочу начать с первой особенности типа «функция», которую мы обозначили в предыдущем разделе. Речь о 8.3.5/3.   Кратко сформулировать можно так: проверяемый тип F является функцией, если выполняется равенство      void( F ) == void( F * ).      Все это довольно просто звучит. И моя первая реализация была тоже достаточно простой. Простой, но неверной. По этой причине я не буду приводить ее полностью, но хочу отдельно рассказать об одном свойстве, которое я в ней использовал. Рассмотрим вот такой код.  	template <typename F> 	static void (* declfunc() )( F );  	template <typename F>  	static void (* gen( void (F *) ) )( F   ); 	template <typename F>  	static void (* gen( void (F  ) ) )( F * );     Забегая вперед, скажу, что этот код исходит из ошибочных предпосылок. Но компиляторы Clang (до версии 3.4 включительно), компиляторы GCC (до версии 4.9), компиляторы из состава VS (cl 19.x, может и более ранние) собирали его так, как я ожидал. Расскажу, каким образом это работает и как я планировал это использовать. Для начала напишем объявление функции, которое поможет нам в проверках:  	template <typename X>  	static char (& check_is_function( X ) ) 	[ 	    is_same<void(*)( F ), X>::value + 1 	]; Если тип, переданный в check_is_function, совпадает с void(*)( F ), то функция возвращает ссылку на массив из двух char, eсли не совпадает — из одного char (тип возвращаемого значения мы потом сможем проанализировать с помощью sizeof). Везде здесь принимаем, что F – это тип, который мы исследуем на принадлежность к типу «функция». Теперь, если оформить это в простой шаблон,  	template <typename F> 	struct is_function 	{ 	    template <typename X>  	    static char (& check_is_function( X ) ) 	    [ 	        is_same<void(*)( F ), X>::value + 1 	    ]; 	    enum  	    { 	        value = sizeof( check_is_function( gen( declfunc<F>() ) ) ) - 1 	    }; 	}; мы сможем удостовериться, что на упомянутых выше компиляторах для выражения вида  	is_function<int()>::value; 	is_function<int>::value;  	typedef void fcv() const; 	is_function<fcv>::value; мы получим результаты 1, 0 и 1 соответственно (полный код можно найти здесь, а запустить здесь). Да, это не полностью рабочее решение, здесь мы не отличаем указатели на функции от функций, есть проблема со ссылками, void и т.д. Но это все просто обходится и акцентировать внимание я хотел бы не на этом. Если мы запустим этот же пример на компиляторах новее указанных (GCC >= 4.9, Clang >= 3.5, cl 19.x), то убедимся, что вывод изменился. Теперь мы получим результаты 1, 0, 0 соответственно. Происходит это потому, что тип функции с cv-qualifier-seq (это тот самый const или volatile в конце), который подставлен в тип другой функции (приобретая при этом свойства указателя), перестал быть верной подстановкой аргумента в варианте:  	template <typename F>  	static void (* gen( void (F *) ) )( F   ); Почему? Потому что с внедрением нового стандарта, в котором ясно написано (последний черновик),  8.3.1/4  Forming a pointer to function type is ill-formed if the function type has cv-qualifiers or a ref-qualifier;изменился и подход компиляторов к подобному коду. Добавление звездочки к такому типу — это неверная подстановка, поэтому код перестал работать. В С++03 не было столь же четкого правила (почитать об изменениях можно здесь). Что, разумеется, не означает, что там это должно было быть разрешено. Однако туманность формулировок стандарта оставила возможность пропустить этот момент, о чем и написано по приведенной ссылке:  It is not sufficiently clear from the existing wording that pointers and references to function types containing cv-qualifiers or a ref-qualifier are not permitted and thus would result in a deduction failure if created during template argument substitution.Поэтому многие современные компиляторы до сих пор этого не учитывают (например cl 18.x или icc 13.x и 14.x). Внимательный читатель наверное уже задался вопросом, что если явное добавление звездочки к типу «функции с cv-qualifier-seq» не разрешено, то неявное, при указании такого типа в качестве параметра, тоже не должно быть доступно. Да, вероятно это так. Однако, на данный момент, нет ни одного компилятора, который бы это явно запрещал.   В стандарте С++03 есть такое:  8.3.5/4  A cv-qualifier-seq shall only be part of the function type for a nonstatic member function, the function type to which a pointer to member refers, or the top-level function type of a function typedef declaration. Это говорит нам о достаточно узком контексте применения типов функций с cv-qualifier-seq. И наш случай вроде бы туда не вписывается.       Поэтому, будем держать в голове, что код, построенный на основе «мутации» типа функции в указатель, не будет работать для всех случаев, но раз на данный момент он все-таки работает, я покажу полное решение. Думаю, это послужит пищей для размышлений о несовершенности мира.    Реализация №1, рабочая      Эта реализация была доработана с учетом ограничений, описанных в предыдущем разделе. Скорее всего (я не уверен на 100%, но на это многое указывает) эта реализация не полностью соответствует стандарту, и то, что она все-таки работает должно послужить поводом отправки багрепортов в поддержку как минимум трех современных компиляторов.      Основная проблема предыдущей реализации в том, что в случае функции с cv-qualifier-seq перестала работать подстановка с указателем. К счастью, в этой проблеме спрятан ключ к ее решению. Мы можем написать SFINAE-проверку, которая определит, возможно ли подставить к переданному типу указатель. Таким образом мы отсечем варианты, когда это невозможно. Проверка выглядит так:  	template <typename F> 	struct may_add_ptr 	{ 	    template <typename P> 	    static char (& may_add_ptr_check(P *) )[2]; 	    template <typename P> 	    static char (& may_add_ptr_check(...) )[1];  	    enum  	    {  	         value = sizeof( may_add_ptr_check<F>(0) ) - 1  	    }; 	}; Если подстановка P* неверна, то выбирается перегрузка с эллипсисом. В зависимости от выбранной перегрузки sizeof от возвращаемого значения вернет либо 1, либо 2. Вычитанием единицы мы добьемся значения value 0 или 1, где 1 получается в случае, если к типу возможно подставить указатель, и 0, если невозможно (далее я буду пользоваться этим приемом таким же образом). Теперь у нас есть возможность сформировать тип указателя на функцию, основываясь на этой проверке. Мы можем поступить разными способами — на основе перегрузки или специализации. Я покажу способ на основе перегрузки, т.к. он более переносим.  	template <typename F> 	static typename enable_if< 	    may_add_ptr<F>::value == 1, void (*)(typename remove_reference<F>::type *) 	>::type declfunc();  	template <typename F> 	static typename enable_if< 	    may_add_ptr<F>::value == 0, void (*)(typename remove_reference<F>::type  ) 	>::type declfunc(); Итак, мы сформировали тип type – указатель на функцию, параметром которой является другой тип. Это тот тип, который мы исследуем на принадлежность к типу «функция». Итак, если      declfunc<F>() == void(*)( F )  то наш тип F – функция. Удаление ссылки (remove_reference) нужно обязательно, в этом случае мы автоматически получим неравенство в случае, если       F = R(&)(Args), или F = T &  т.к. после всех подстановок сравниваться будут следующие типы:      void(*)( R(*)(Args) ) и void(*)( R(&)(Args) )  или      void(*)( T ) и void(*)( T & )  соответственно. Эти типы, очевидно, не совпадают, что нам и требуется.   В случае же, если тип F – функция вида R(Args), то сравниваться будут      void(*)( R(*)(Args) ) и void(*)( R(Args) ).  Эти типы равны, исходя из вышеприведенных положений стандарта (8.3.5/3).   В случае, если F – функция вида R(Args) const, то сравниваться будут      void(*)( R(Args) const ) и void(*)( R(Args) const ).  Эти типы тоже равны, что нам и требуется.  В случае, если F = T (не функция), то сравниваться будут      void(*)( T * ) и void(*)( T ).  Эти типы не равны, что нам и требуется.      Теперь нам нужно собственно сравнить типы. Есть одно но, мы не можем здесь использовать обычную проверку на основе is_same, т.к. аргументом нашей is_function может быть и абстрактный тип, использование которого в этом контексте приведет к ошибке компиляции. Поэтому мы заменим is_same на SFINAE-проверку следующего толка:  	template <typename F> 	static char (& is_function_check( void( F ) ) )[2];  	template <typename F> 	static char (& is_function_check(    ...    ) )[1]; Воспользуемся которой мы так:  	value = sizeof( is_function_check<Tp>( declfunc<Tp>() ) ) - 1;Полный код выглядит так.	template <typename Tp> 	struct is_function 	{ 	private: 	    template <typename F> 	    struct may_add_ptr 	    { 	        template <typename X> 	        static char (& may_add_ptr_check(X *) )[2]; 	        template <typename X> 	        static char (& may_add_ptr_check(...) )[1];  	        enum  	        {  	            value = sizeof( may_add_ptr_check<F>(0) ) - 1  	        }; 	    };  	    template <typename F> 	    static  	    typename enable_if< 	        may_add_ptr<F>::value == 1, void (*)(typename remove_reference<F>::type *) 	    >::type declfunc();  	    template <typename F> 	    static  	    typename enable_if< 	        may_add_ptr<F>::value == 0, void (*)(typename remove_reference<F>::type ) 	    >::type declfunc();  	    template <typename F> 	    static char (& is_function_check( void( F ) ) )[2];  	    template <typename F> 	    static char (& is_function_check(    ...    ) )[1];  	public: 	    enum  	    {  	        value = sizeof( is_function_check<Tp>( declfunc<Tp>() ) ) - 1  	    }; 	};   Для проверки того, что этот шаблон действительно работает, давайте напишем тестирующий макрос.  	#define TEST_IS_FUNCTION(Type, R) \ 	    std::cout << ((::is_function<Type>::value == R) ? ""[SUCCESS]"" : ""[FAILED]"") \ 	              << "" Test is_function<"" #Type ""> (should be ["" #R ""]):"" \ 	              << std::boolalpha \ 	              << (bool)::is_function<Type>::value << std::endl И запустим на следующемнаборе тестов.	struct S { virtual void f() = 0; };  	int main() 	{ 	    typedef void f1() const; 	    typedef void f2() volatile; 	    typedef void f3() const volatile;   	    TEST_IS_FUNCTION(void(int), true); 	    TEST_IS_FUNCTION(void(), true); 	    TEST_IS_FUNCTION(f1, true); 	    TEST_IS_FUNCTION(void(*)(int), false); 	    TEST_IS_FUNCTION(void(&)(int), false); 	    TEST_IS_FUNCTION(f2, true); 	    TEST_IS_FUNCTION(f3, true); 	    TEST_IS_FUNCTION(void(S::*)(), false); 	    TEST_IS_FUNCTION(void(S::*)() const, false); 	    TEST_IS_FUNCTION(S, false); 	    TEST_IS_FUNCTION(int, false); 	    TEST_IS_FUNCTION(int *, false); 	    TEST_IS_FUNCTION(int [], false); 	    TEST_IS_FUNCTION(int [2], false); 	    TEST_IS_FUNCTION(int **, false); 	    TEST_IS_FUNCTION(double, false); 	    TEST_IS_FUNCTION(int *[], false); 	    TEST_IS_FUNCTION(int &, false); 	    TEST_IS_FUNCTION(int const &, false); 	    TEST_IS_FUNCTION(void(...), true); 	    TEST_IS_FUNCTION(int S::*, false); 	    TEST_IS_FUNCTION(void, false); 	    TEST_IS_FUNCTION(void const, false); 	}       Здесь можно посмотреть полный код примера и теста, а здесь запустить. Кстати, этот код работает также и для С++11. Тестировалось на GCC 4.4.x – 6.0, Clang 3.0 – 3.9, VS 2013 и VS 2015. Есть компиляторы, которые считают добавление указателя к F c cv-qualifier-seq верной подстановкой (например icc 13.x). На этих компиляторах проверка работать не будет.    Реализация №2, соответствующая стандарту      Вспомним 8.3.4/1. Там говорилось, что функция — один из немногих типов, массив которых нельзя создать. Раз с предыдущим способом не все однозначно, может быть здесь нас ждет бóльшая удача? Давайте еще раз перечислим массивы каких типов мы не можем создать:    ссылки  void  абстрактные классы  функции      Итак, нашу задачу теперь можно разбить на два этапа. Отсеять остальные типы с подобным поведением и написать SFINAE-проверку, определяющую можно ли создать массив заданного типа. Для начала давайте отсеем абстрактные классы. Хотя проще всего отсеять все классы сразу. Для этого нам понадобится метафункция:  	template <typename Tp> 	struct is_class;     Теперь нам нужно убрать из рассмотрения ссылки и void. Используем для этого следующие два шаблона:  	template <typename Tp> 	struct is_lvalue_reference;  	template <typename Tp> 	struct is_void;     Вроде бы все, но чего-то не хватает. В самом деле, есть еще один тип, который не может быть элементом массива — это array of unknown bound (массивы неизвестного размера T[]). Его нам тоже нужно отсеять. В принципе, можно не мучиться и отсеивать сразу все массивы.  	template <typename Tp> 	struct is_array; Реализацию этих метафункций можно найти здесь, или взять, например, из boost.  Теперь пришло время сформировать основной шаблон:  	template <typename Tp> 	struct is_function 	{ 	private: 	    template <typename F>  	    static char (& check_is_function( ... ) )[2]; 	    template <typename F>  	    static char (& check_is_function( F (*)[1] ) )[1];  	public: 	    enum  	    {  	        value = !is_class<Tp>::value 	             && !is_void<Tp>::value 	             && !is_lvalue_reference<Tp>::value 	             && !is_array<Tp>::value 	             && (sizeof( check_is_function<Tp>(0) ) - 1) 	    };  	};     Проверку, может ли являться тип элементом массива, мы организуем через определение типа «указатель на массив», с элементами тестируемого типа в качестве параметра функции check_is_function. Если подстановка неуспешна, значит тип F является функцией.      В качестве теста возьмем предыдущий набор из реализации 1. Полный код можно посмотреть здесь, а запустить здесь. Эта реализация полностью соответствует стандарту и скорее всего будет работать на большинстве компиляторов. Этот код также работает и для С++11, нужно только дополнительно отсеивать rvalue-ссылки.    Вместо заключения  1) Я отправил три багрепорта насчет нелегального продвижения функции с cv-qualifier-seq до указателя:  В поддержку Clang.  В поддержку GCC.  В поддержку VS.  Как уже говорилось, я не уверен на 100%, но это единственный способ узнать мнение разработчиков по этому вопросу.    2) Полный код, который находится на моем гитхабе, немного отличается от приведенного в статье в лучшую сторону. Здесь были намеренно опущены некоторые детали и правила хорошего тона.    2.1) Меня попросили подробнее описать типы функций с cv-qialifier-seq.Совершенно ясно, что такой тип не может служить для объявления свободной функции, т.к. cv-qialifier-seq относится к this (см. 9.3.1/3).  Так в каких же ситуациях это работает? Стандарт разрешает это для следующих случаев:  8.3.5/7  A typedef of a function type whose declarator includes a cv-qualifier-seq shall be used  only to declare the function type for a nonstatic member function, to declare the function type to which a  pointer to member refers, or to declare the top-level function type of another function typedef declaration.  [Example:typedef int FIC(int) const; FIC f; // ill-formed: does not declare a member function struct S {     FIC f; // OK }; FIC S::*pm = &S::f; // OK —end example]Т.е. можно с помощью typedef типа функции с cv-qualifier-seq:  объявлять функции-члены,   сформировать указатель на функцию-член и   использовать его в качестве типа верхнего уровня в typedef декларации другого типа функции.  Последний случай довольно туманный (что имеется в виду под another function?). Для сравнения приведу цитату из последнего черновика:  8.3.5/6  A function type with a cv-qualifier-seq or a ref-qualifier (including a type named by typedef-name (7.1.3, 14.1)) shall appear only as:  (6.1) — the function type for a non-static member function,  (6.2) — the function type to which a pointer to member refers,  (6.3) — the top-level function type of a function typedef declaration or alias-declaration,  (6.4) — the type-id in the default argument of a type-parameter (14.1), or  (6.5) — the type-id of a template-argument for a type-parameter (14.3.1).  [ Example:typedef int FIC(int) const; FIC f; // ill-formed: does not declare a member function struct S {     FIC f; // OK }; FIC S::*pm = &S::f; // OK — end example ]Согласитесь, это намного более понятно (обратите внимание на 6.3, теперь он легализует использование cv-qualifier-seq в typedef декларации типа функции верхнего уровня и только, никаких another function). То, что такой тип сейчас можно использовать в качестве параметра функции, и есть предмет багрепортов, которые я сделал. Кроме этого, в этом случае эти типы будут продвинуты до указателя, а это тоже запрещено (8.3.1/4).   Сравнить изменения можно здесь. Также возможно кому-то будет интересен вот этот материал.    3) Спасибо за внимание :)","compile-time checks, c++, metafunction",Другая реализация метафункции is_function<T> для C++98/03
"В Scala есть интересная коллекция — Stream. Контейнер, который представляет собой список, элементы которого вычисляются (и сохраняются после этого) при первом обращении:    The class Stream implements lazy lists where elements are only evaluated when they are needed.  Мне захотелось реализовать нечто подобное на C++.     Цель  Хочется получить контейнер, который можно использовать со стандартными алгоритмами и чтобы его элементы могли генерироваться по мере обращения к ним.    Забегая вперед, приведу пример использования:typedef lazy::list<int> list_type;  list_type fibonacci(int n1, int n2) {      int next = n1 + n2;      std::cout << ""fibonacci("" << n1 << "", "" << n2 << "") -> "" << n1 << std::endl;      return list_type(n1, std::bind(&fibonacci, n2, next)); }  int main()  {      list_type list(fibonacci(0, 1));      auto res3 = std::find_if(list.begin(), list.end(), [](int v){return v > 3;});     std::cout << ""first number greater 3 is "" << *res3 << std::endl;     std::cout << std::endl;      auto res10 = std::find_if(list.begin(), list.end(), [](int v){return v > 10;});     std::cout << ""first number greater 10 is "" << *res10 << std::endl;     return 0;  }  На выводе будет:    fibonacci(0, 1) -> 0 fibonacci(1, 1) -> 1 fibonacci(1, 2) -> 1 fibonacci(2, 3) -> 2 fibonacci(3, 5) -> 3 fibonacci(5, 8) -> 5 first number greater 3 is 5  fibonacci(8, 13) -> 8 fibonacci(13, 21) -> 13 first number greater 10 is 13    Как видно из примера, функция генерации элемента вызывается только один раз для каждого элемента, полученное значение хранится в контейнере и повторно не вычисляется.    Ограничения  Предположим, мы хотим выполнить что-то типа:    auto iter = --list.end();  Чтобы получить элемент, предшествующий end, необходимо обойти все элементы, генерируемые функцией, а это бесконечность (для примера выше). Соответственно, итератор по ленивому списку будет однонаправленный — ForwardIterator. Аналогичная ситуация будет и при получении количества элементов в списке, и при удалении последнего элемента (pop_back). Таким образом, этих методов у контейнера не будет.    Для простоты я не стал реализовывать вставку в произвольное место и удаление произвольного элемента. Исключительно из соображения, что последовательность элементов может генерироваться какой-то функцией, и при вставке/удалении эта последовательность будет нарушена. Из этих же соображений элементы не модифицируемые. Но это уже условности.    Что же получилось?  Получился список, в который можно добавлять как элементы, так и функторы, генерирующие ленивый список, который в свою очередь может содержать элементы и функторы. Удалять можно либо первый элемент (pop_front), либо все элементы (clear). Вставка элементов осуществляется в начало или конец списка.    Итератор по списку однонаправленный, не позволяющий модифицировать элементы списка.    template< typename T, typename Allocator = std::allocator<T> > class list;  определение спискаtemplate< typename T, typename Allocator = std::allocator<T> > class list { public:     typedef list<T, Allocator> self_type;     typedef T value_type;     typedef std::function<self_type ()> func_type;     typedef __details_lazy_list::const_iterator<self_type> iterator;     typedef __details_lazy_list::const_iterator<self_type> const_iterator;      friend __details_lazy_list::const_iterator<self_type>;      list();     list(const self_type& that);     list(self_type&& that);      template<typename ... Args>     explicit list(Args&&... args)     {         push_others(std::forward<Args>(args)...);     }      void push_back(const value_type& value);     void push_back(value_type&& value);     void push_back(const func_type& func);     void push_back(func_type&& func);     void push_back(const self_type& that);     void push_back(self_type&& that);      void push_front(const value_type& value);     void push_front(value_type&& value);     void push_front(const func_type& func);     void push_front(func_type&& func);     void push_front(const self_type& that);     void push_front(self_type&& that);      void clear();      bool empty() const;      const_iterator begin() const;     const_iterator end() const;  private:     typedef std::list<value_type, Allocator> container_type;     typedef typename container_type::iterator inner_iterator;     typedef value_type const * funcs_map_key_type;     typedef std::pair<funcs_map_key_type, func_type> funcs_map_value_type;     typedef std::map<funcs_map_key_type, func_type> funcs_map_type;      void forward(const_iterator& iter) const;     void insert(inner_iterator pos, self_type&& that) const;      template<typename Arg, typename ...Args>     void push_others(Arg&& arg, Args&&... args)     {         push_back(std::forward<Arg>(arg));         push_others(std::forward<Args>(args)...);     }      template<typename Arg>     void push_others(Arg&& arg)     {         push_back(std::forward<Arg>(arg));     }      void push_others() {}      mutable container_type _cont;     mutable funcs_map_type _funcs; };    определение итератораtemplate< typename lazy_list_type > class const_iterator:     public std::iterator<std::input_iterator_tag, typename lazy_list_type::value_type> {     friend lazy_list_type; public:     typedef std::iterator<std::input_iterator_tag, typename lazy_list_type::value_type> base_type;     const_iterator();      typename base_type::reference const operator* () const;     typename base_type::pointer const operator-> () const;      const_iterator& operator++();     const_iterator operator++(int);      bool operator== (const const_iterator& that);     bool operator!= (const const_iterator& that); private:     typedef typename lazy_list_type::inner_iterator inner_iterator;      const_iterator(const lazy_list_type* owner, inner_iterator iter);      const lazy_list_type* _owner;     inner_iterator _iter; };    В конце статьи есть ссылка на репозиторий, где можно взять реализацию с юнит-тестами.    Шаблонные параметры.  T — тип хранимых элементов    Allocator — аллокатор, используемый для размещения элементов.    Внутренние типы        Тип  Описание          value_type  T      self_type  собственный тип      func_type  тип функтора, используемого для генерации элемента. Функтор возвращает объект self_type.      iterator  константный forward итератор      const_iterator  константный forward итератор        Методы        Метод  Описание          push_front  вставка в начало      push_back  вставка в конец      empty  проверка, является ли контейнер пустым      clear  удалить все элементы      pop_front  удалить первый элемент        Методы push_front и push_back принимают функтор, генерирующий элементы, значение хранимого элемента или другой объект типа self_type.    Конструкторы        Сигнатура  Описание          list();  Создает пустой контейнер      template<typename ... Args> explicit list(Args&&... args)  Cоздает контейнер и помещает в него переданные элементы.  Могут быть переданы значения следующих типов:  value_type  func_type  self_type        Как это работает  Внутри используются два стандартных контейнера — std::list для хранения значений и std::map для хранения функторов. Функтор должен возвращать ленивый список, т.е. self_type. Это позволяет, во-первых, рассчитать при необходимости сразу несколько элементов, а во-вторых, не заботиться о случае, когда нет следующего значения — закончилась последовательность, в этом случае можно просто вернуть пустой контейнер.    С добавлением нового элемента все просто — он сразу добавляется во внутренний список.    При добавлении функтора проверяется, есть ли функтор, ассоциированный с элементом, после которого он добавляется (push_back). Если функтора нет, то переданный функтор добавляется в map, а в качестве ключа берется указатель на предыдущий элемент. При добавлении в начало, в пустой контейнер или после элемента, с которым уже есть ассоциированный функтор, просто вызывается метод функтора operator(), и значения из полученного контейнера вставляются в нужное место (в начало или конец), в map добавляются новые функторы, если они есть в возвращенном контейнере.    Можно было бы хранить в списке пару ""значение — функтор"", но мне кажется, что в процессе работы количество функторов будет существенно меньше количества вычисленных элементов, и затраты памяти в случае хранения пар будут выше.  Опять же, так как предполагаю, что количество функторов не будет очень большим, то нет особой разницы, что использовать — map или unordered_map. Единственное, что при использовании map затраты памяти будут чуть меньше, мне так кажется.    Когда инкрементируется итератор, то проверяется наличие функтора для текущего элемента, если он есть, то возвращаемое им значение добавляется в контейнер, а функтор удаляется. После этого инкрементируется итератор на внутренний список. Если функтора нет или он вернул пустой контейнер, то просто производится переход к следующему элементу во внутреннем списке.    Зачем все это?  К реализации такого списка меня подтолкнула задача Water Pouring, представленная в лекции по языку Scala. Суть в следующем: есть несколько стаканов фиксированного объема, кран, из которого можно наполнить любой стакан (мы можем наполнить стакан только полностью), и раковина, куда можно вылить содержимое стаканов. Наполняя, опустошая и переливая воду из одного стакана в другой, нужно получить заданное количество воды в одном из стаканов. Решение — последовательность действий для получения такого состояния.    Например, есть два стакана объемом 3 и 5 литров, мы хотим получить 4 литра.      Будем рассматривать текущее количество воды в каждом из стаканов как состояние. Из каждого состояния можно получить новый набор возможных состояний, применив одну из возможных операций: наполнить, вылить, перелить. Из каждого набора состояний можно получить новый набор. Чтобы не зациклиться, будем отдельно хранить полученные состояния и отбрасывать их при получении набора новых состояний.      В каждом наборе состояний будем смотреть, есть ли искомое состояние — стакан с искомым уровнем воды.    Нам понадобятся возможные варианты воздействия на текущее состояние. Каждый вариант перемещения воды будет наследовать интерфейс imove:    class imove { public:     virtual state operator()(const state& cur) const = 0;     virtual std::unique_ptr<imove> clone() const = 0;     virtual std::string to_string() const = 0;     virtual ~imove() {} };  Метод to_string нужен только для вывода информации на экран.    Для этой задачи возможны следующие типы перемещения:    Наполнить стакан - fillclass fill: public imove { public:     fill(unsigned glass, unsigned capacity);     state operator()(const state& cur) const override;     std::unique_ptr<imove> clone() const override;     std::string to_string() const override; protected:     const unsigned _glass;     const unsigned _capacity; };  fill::fill(unsigned glass, unsigned capacity) :     _glass(glass),     _capacity(capacity) {}  state fill::operator()(const state& cur) const {     assert(cur.size() > _glass);     state next(cur);     next[_glass] = _capacity;     return next; } std::unique_ptr<imove> fill::clone() const {     return std::unique_ptr<imove>(new fill(_glass, _capacity)); } std::string fill::to_string() const {     return ""fill("" + std::to_string(_glass) + "")""; }    Вылить воду из стакана - emptyclass empty: public fill { public:     empty(unsigned glass);     std::unique_ptr<imove> clone() const override;     std::string to_string() const override; };  empty::empty(unsigned glass) :     fill(glass, 0) {}  std::unique_ptr<imove> empty::clone() const {     return std::unique_ptr<imove>(new empty(_glass)); }  std::string empty::to_string() const {     return ""empty("" + std::to_string(_glass) + "")""; }    Перелить из одного стакана в другой - pourclass pour: public imove { public:     pour(unsigned from, unsigned to, unsigned capacity_to);     state operator()(const state& cur) const override;     std::unique_ptr<imove> clone() const override;     std::string to_string() const override; protected:     const unsigned _from;     const unsigned _to;     const unsigned _capacity_to; };  pour::pour(unsigned from, unsigned to, unsigned capacity_to) :     _from(from),     _to(to),     _capacity_to(capacity_to) {}  state pour::operator()(const state& cur) const {     assert((cur.size() > _from) && (cur.size() > _to));     assert(_capacity_to >= cur[_to]);     unsigned amount = std::min(cur[_from], _capacity_to - cur[_to]);     state next(cur);     next[_from] -= amount;     next[_to] += amount;     return next; }  std::unique_ptr<imove> pour::clone() const {     return std::unique_ptr<imove>(new pour(_from, _to, _capacity_to)); }  std::string pour::to_string() const {     return ""pour("" + std::to_string(_from) + "", "" + std::to_string(_to) + "")""; }    Также нужно хранить информацию о новом состоянии, а именно состояние и набор перемещений, приведших к нему. За это будет отвечать класс path:    class path { public:     path(const state& initial_state);     path(const path& that);      void extend(imove_ptr move);     const state& end_state() const;     std::string to_string() const;     bool empty() const; private:     std::list<imove_ptr> _history;     state _end_state; };  И собственно класс, который использует ленивый список и вышеперечисленные вспомогательные классы для нахождения решения:    typedef std::list<path> paths_list;  class water_pouring { public:     water_pouring(std::initializer_list<unsigned> capacities);      path solve(unsigned target); private:     typedef lazy::list<paths_list> list_of_paths_type;     list_of_paths_type extend(const paths_list& paths);      const std::vector<unsigned> _capacities;     const std::vector<imove_ptr> _posible_moves;     const state _initial;     std::set<state> _explored_states;     list_of_paths_type _paths; };  Класс имеет два публичных метода — конструктор, который принимает емкости стаканов, и метод, возвращающий путь достижения требуемого состояния.    Приватный метод extend используется для генерации элементов ленивого списка.    Он хранит емкости стаканов, набор возможных перемещений, начальное состояние, уже ""найденные"" состояния и собственно ленивый список состояний с историей их получения.    Для получения возможных перемещений используется функция create_movesstd::vector<imove_ptr> create_moves(const std::vector<unsigned>& capacities) {     std::vector<imove_ptr> moves;     for (size_t i = 0; i < capacities.size(); ++i)     {         moves.emplace_back(new empty(i));         moves.emplace_back(new fill(i, capacities[i]));         for (size_t j = 0; j < capacities.size(); ++j)         {             if (i != j)                 moves.emplace_back(new pour(i, j, capacities[j]));         }     }     return moves; }    Метод water_pouring::extend:    water_pouring::list_of_paths_type water_pouring::extend(const paths_list& paths) {     paths_list next;     for (auto& cur_path: paths)     {         for (auto move: _posible_moves)         {             state next_state = (*move)(cur_path.end_state());              if (_explored_states.find(next_state) == _explored_states.end())             {                 path new_path(cur_path);                 new_path.extend(move);                 next.push_back(new_path);                 _explored_states.insert(next_state);             }         }     }      if (next.empty())         return list_of_paths_type();      return list_of_paths_type(next, std::bind(&water_pouring::extend, this, next)); }  Метод water_pouring::solve:    path water_pouring::solve(unsigned target) {     paths_list::const_iterator solution;     auto it = std::find_if(         _paths.begin(),         _paths.end(),         [target, &solution](const paths_list& paths) -> bool {             solution = std::find_if(                 paths.begin(),                 paths.end(),                 [target](const path& p) -> bool {                     auto it = std::find(                             p.end_state().begin(),                             p.end_state().end(),                             target);                     return it != p.end_state().end();                 });             return solution != paths.end();         });      if (it != _paths.end())         return *solution;      return path(state({0})); }  Собственно, для поиска решения используется функция std::find_if, а в качестве предиката — лямбда функция, просматривающая пути на наличие необходимого состояния. Лямбда захватывает по ссылке solution, чтобы лишний раз не проходиться по списку решений, на которые будет указывать итератор it в случае, если решение было найдено.    В итоге программа выведет следующее решение:    fill(1) pour(1, 0) empty(0) pour(1, 0) fill(1) pour(1, 0) --> (3, 4)  Придумать другую задачу, где мог бы пригодиться ""ленивый"" список, я не смог. Надеюсь, идея кому-нибудь приглянется.    Ссылки    Исходный код lazy_list  Исходный код решения Water Pouring на C++  Исходный код решения Water Pouring на Scala  Курс лекций Functional Programming Principles in Scala в рамках которого была расмотрена и задача Water Pouring  Для юнит тестов использовал фреймворк Catch","c++, ленивые вычисления",Ленивый список в C++
"Всем привет! В эти праздничные выходные мы решили поделиться с вами несколькими свежими материалами, полезными, как для работы, так и для организации вашего собственного кружка «очумелых ручек». :)    Standalone Android-эмулятор от Microsoft  Начнем с «простого» — года полтора назад мы включили в состав Visual Studio 2015 эмулятор для Android'а. Для тех, кто следит за развитием VS, это не новость, но, как показывает наш опыт, многие не в курсе. Вкратце, история такая: x86, Hyper-V, симуляция сенсоров, различные версии Android… и, — о чем многие не догадываются, — он может работать без Visual Studio (и вообще ставиться отдельно), интегрируясь с Android Studio, Eclipse и другими инструментами, поддерживающими ADB.        Выше подробный свежий рассказ про работу с текущей версией эмулятора Android.      Охлаждение пива с Raspberry Bi 2 и Windows 10 IoT  Вторая порция свежих материалов к нам пришла из Эстонии, где Гуннар Пейпман (Gunnar Peopman), MVP по ASP.NET/IIS и домашний пивовар, решил, что хорошо бы автоматизировать охлаждение приготовленного пива.          К презентации прилагаются подробные инструкции с кодом и формулами:     Measuring temperature with Windows 10 IoT Core and Raspberry Pi   Moving to ITemperatureClient interface  Measuring cooling rate    Making cooling rate calculation testable   Estimating beer cooling time   Reporting measurements to Azure IoT Hub   Using Stream Analytics to save data from IoT Hub to SQL database   Visualizing sensors data using Power BI   Building Universal Windows Application to monitor cooling process      The Maker Show      И третья порция контента на сегодня — наше новое шоу для разработчиков, не боящихся работать с железом, паять и собирать различные конструкции из электроники и кода. Выше вы найдете вводное видео серии, а ниже ссылки на первые три эпизода:      Intro to Electronics   Blinking LED...Now What?   Arduino and Servos       Успешных выходных!","Android, Visual Studio, пиво, IoT, makers, Arduino, Raspberry Pi 2, Windows 10, Windows 10 IoT","Эмулятор Android от Microsoft, охлаждение пива с IoT и Windows 10 и The Maker Show для тех, кто любит паять"
"С того момента, когда cjdns добавили в официальный репозиторий OpenWRT, процесс подготовки mesh-роутера стал просто тривиален. Саму установку OpenWRT на роутеры описывать подробно не вижу смысла, т.к. это популярная тема. На многих официально поддерживаемых роутерах задача установки свободной прошивки сводится к простому скачиванию Factory архива и скармливанию его стандартной форме обновления прошивки на заводской системе. Главное, чтобы не было блокировки на установку сторонней прошивки, но и в этом случае, как я полагаю, будет множество обходных путей.    До появления официальной поддержки пакетов cjdns, приходилось возиться немного больше. Можно было собирать пакеты самому или искать собранные кем-то пакеты и устанавливать их. Еще были прошивки от энтузиастов с уже установленным нужных ПО. В любом случае, приходилось повозиться чуть больше, чем сейчас.        Итак, мы имеем роутер на OpenWRT, выполняющий обычные функции домашнего маршрутизатора. Администрировать все это дело будем левой кнопкой мышки в LuCY.    Установка  Как написал выше, OpenWrt Chaos Calmer 15.05 имеет необходимые пакеты в официальном репозитории. Заходим в меню System -> Software. Обновляем список и устанавливаем пакеты luci-app-cjdns и cjdns. Последний является зависимостью первого и должен автоматически установиться.    Mesh режим  После установки, cjdns сразу начнет работать. Т.е. обычная установка проходит в один шаг. Но мы хотим, чтобы роутер не только соединялся с cjdns нодами в нашей локальной сети, но и стал активным участником меш-сети. В меню появляется раздел Services -> cjdns. Во вкладке Overview отображаются все текущие соединения. Если на ваших домашних компьютерах работает cjdns, с большой вероятностью роутер установит с ними соединение в меш-режиме. Нам нужно добавить 802.11s WiFi сеть. Делается это так же просто:  идем в раздел Network -> WiFi,  на нужном физическом интерфейсе нажимаем Add,  по текущей договоренности на форуме cjdroute.net выбираем 11 канал в 2.4 ГГц,  поле Mode будет 802.11s — WiFi mesh,  Network у нее будет отдельный, т.к. нам не нужно, чтобы эти участники попадали во внутреннюю Lan сеть (я создал новый с именем Mesh),  SSID: hyperboria_mesh,  защищать эту сеть не нужно.    В разделе Network -> Interfaces у нас появится новая сеть. Protocol я выбрал unmanaged — нам нет необходимости присваивать участникам обычные IP адреса.    Так же можно добавить простую точку доступа без защиты с именем, например, cjdroute.net, которая подключает устройства к этой сети. Это позволит подключать девайсы, не умеющие в 802.11s, но умеющие в cjdns, к hyperboria.    Теперь идем в Services -> cjdns -> Settings, спускаемся к пункту Ethernet Interfaces и добавляем там нашу новую сеть (если забыли имя, то можно подсмотреть в списке Network -> Interfaces). Beacon Mode я выбрал 2, чтобы он не только ожидал там специальные Ethernet фреймы, с помощью которых будет устанавливать mesh-соединения, но и сам рассылал такие. Так же стоит убедиться, что в домашней локальной сети тоже соединяемся. Если провайдер предоставляет локалку свою, можно попробовать и через WAN интерфейс соединяться с другими клиентами провайдера.    Оверлэйные UDP-соединения (он же UDP транспорт)    Естественно, глупо ожидать, что по всему городу рассыпаны уже эти коробочки, и функционирует полноценная сеть на всю страну. Тем более, что между городами крайне низкая плотность населения. Для дополнительной связи нам нужно будет добавить коннекты через UDP транспорт. Это обычный оверлэйный режим поверх других сетей типа интернета. Примерно так работает TOR. Только в данном случае, соединения будут устанавливаться напрямую без попыток анонимизации и потери скорости.    Стоит так же упомянуть о том, что здесь есть непонятый многими, но очень важный момент — необходимость добавлять такие соединения вручную и устанавливать связь с теми, с кем обменялся ключами и паролями. На первый взгляд, эта процедура идиотская и создает лишние сложности в установке: ведь в оверлэйных сетях типа Tor и I2P этого делать не надо — есть bootstrap ноды, которые передают вам список всех адресов других участников. Однако, это удобство является уязвимостью — в том же Китае к этим нодам подключаются атакующие, получают список всех участников и блокируют соединения с этими адресами и самой нодой. Т.е. удобство добавляет в систему единую точку отказа и ставит под угрозу стабильность работы вообще.    Обменяться ключами и адресами можно на том же форуме cjdroute.net в специальном разделе. Вероятно, позже, ваши друзья и коллеги тоже присоединяться к сети и обменяются с вами нужными данными.    Добавление соединений происходит так же просто в разделе Services -> cjdns -> Peers. Там можно добавить пароли для входящих подключений (помните, что для этого надо роутеру иметь белый IP адрес и открыть UPD порт) и информацию об исходящих подключениях. Каждое новое соединение увеличивает стабильность работы. А благодаря простоте интерфейса, добавлять новые можно будет между делом в течение пары минут.    На этом пункте настройку полноценного mesh-роутера можно считать завершенной. Теперь ваш роутер будет выполнять свои обычные функции домашнего маршрутизатора, но при появлении таких же сознательных соседей, будет создавать с ними новые коннекты, независимые от вашего провайдера.    Бонусный пункт: защищенный выход в интернет    Cjdns — это по сути решение, которое позволяет создавать распределенную виртуальную частную сеть. Благодаря своей распределенной архитектуре эта VPN вырастает в огромную Semi-meh (оверлэйная + меш) сеть и строит новый интернет будущего (Hyperboria).  Если на ваших компьютерах установлен и правильно настроен CJDNS, значит они будет «видеть» друг друга, даже если оба находятся в тысячах километрах друг от друга за провайдерскими NATами. CJDNS предоставляет IPv6, и все поддерживающие его программы будут отлично работать. Этакий хамачи здорового человека. Но кроме собственно доступа в hyperboria и соединения личных устройств в единую сеть хотелось бы в пару кликов сделать защищенный туннель в обычный интернет.     Настройки на сервере, который будет выпускать нас в интернет.    Допустим, у нас есть VPS в какой-нибудь нормальной стране, где интернет не подвергается агрессивным нападкам. Мы настроили на этом VPS такую же CJDNS ноду и хотим иметь возможность выхода через нее в интернет. Для этого в конфиге cjdns нашей VPS нужно добавить входящий туннель в разделе ipTunnel -> allowedConnections. Например:   {   ""publicKey"": ""kdddddgfgsftrtrtrnrmnmnmgnmdfndmfnmdfnmdnfmdfmdndfdf0.k"", // Публичный ключ нашего клиента. Для роутера достаем в настройках Services -> cjdns -> Settings   ""ip4Address"": ""192.168.45.10"", // IP адрес клиента, который будет автоматически присвоен    ""ip4Prefix"": 24 // Префикс. Соответствует маске 255.255.255.0   }      В консоли добавляем IPv4-адрес на интерфейс туннеля и разрешаем Forwarding, чтобы выпускать в интернет клиентов.  ip addr add dev tun0 192.168.45.1/24  echo 1 > /proc/sys/net/ipv4/conf/all/forwarding    Добавляем роут и правила для firewall    ip route add dev tun0 192.168.45.0/24  iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE  iptables -A FORWARD -i eth0 -o tun0 -m state --state RELATED,ESTABLISHED -j ACCEPT  iptables -A FORWARD -i eth0 -o tun0 -j ACCEPT    Теперь наш VPS сервер может заменять нам обычные платные VPN сервисы. Кстати, по моим ощущениям, настройка Получается гораздо быстрее и проще чем того же OpenVPN.    Я на этом пункте попался в интересную ловушку. Захотел подключать сразу и роутер, и ноутбук, выдавая им разные IPv4 адреса, но в одной подсети. Как выяснилось, оно для этого не предназначено, первый вписанный клиент нормально роутился, а второй просто получал адрес, но даже не мог пинговать сервер. Если хотите несколько устройств подрубать, то надо делать разные подсети. Это может пригодиться, например, для обеспечения защищенного выхода в интернет поверх не доверенного интернет-соединения.    Настройки на роутере    Тут все просто. Идем в Services -> cjdns -> IP Tunnel и в Outgoing connections добавляем публичный ключ cjdns из конфига нашей VPS ноды. Прописанный IP присваивается автоматически. Маршрутизация клиентов роутера в интернет через этот VPN настраивается добавлением соответствующих правил на фаерволе.        Вы можете прописать адрес нашего сервера 192.168.45.1 в качестве шлюза для всех соединений, а можете просто добавлять статические роуты и перенаправлять запросы только для конкретных сайтов: Network -> Static routes    В данном случае, я сделал защищенные и устойчивые соединения для всех устройств в моей домашней сети на рутрекер и btc-e.com. По какой-то странной причине, мне захотелось сделать подключения к этим сайтам более безопасными.    В этом бонусном пункте есть и обратный вариант: сделать роутер таким сервером. Например вы находись где-то в гостях или пользуетесь общественным WiFi и хотите сделать соединения более безопасными. В той же вкладке настроек роутера добавляем входящие соединения, а на ноутбуке исходящие. Немного другие правила для роутинга — и вот вам более защищенная связь из любого места.    Таким образом, мы левой и правой кнопками мыши добились поставленных задач. На написание, возможно, банальной для хабра статьи меня сподвигла эта карта cjdroute.net/map. Многие добавили свое местоположение с комментарием, что готовы присоединиться, если им помогут все настроить. Во времена первых статей о CJDNS, процедура настройки была местами запутанной, а большинство мануалов подразумевали ручную сборку OpenWRT и ковыряние конфигов. Конечно, есть ряд настроек, которые хорошо бы добавить позже в консоли, но для старта достаточно описанных пунктов. Надеюсь, это может многим превратить свое желание в намерение. Если будут какие-то вопросы, мы охотно отвечаем на форуме cjdroute.net","mesh, cjdns, openwrt",Mesh-роутер — это просто
"Предисловие: Я пишу на Python более 6 лет и могу назвать себя профессионалом в этом языке. Недавно я даже написал о нем книгу. Однако последние 8 месяцев я переключился на D и уже 4 месяца активно участвую в разработке этого языка по части расширения стандартной библиотеки Phobos. Так же я участвовал в код-ревью модуля std.ndslice о котором и пойдет речь.    std.ndslice так же как и Numpy предназначен для работы с многомерными массивами. Однако в отличие от Numpy ndslice имет крайне низкий оверхэд так как базируется на ranges (диапазонах), которые используются в штатной библиотеке повсеместно. Ranges позволяют избежать лишние процедуры копирования, а так же позволяют красиво организовать ленивые вычисления.     В этой статье мне хотелось бы рассказать о том какие преимущества std.ndslice дает по сравнению с Numpy.    Почему программисты Python должны посмотреть в сторону D?  Потому что D позволяет писать практически такой же простой и понятный код как Python, при этом этот код будет работать значительно быстрее чем код на Python.    Следующий пример создает диапазон чисел от 0 до 999 используя функцию iota (аналог в Python называется xrange) и возвращает массив размерностью 5x5x40.    import std.range : iota; import std.experimental.ndslice;  void main() {     auto slice = sliced(iota(1000), 5, 5, 40); }  Хотя D статически типизируемый язык, и явное указание типа повышает читаемость кода, чтобы программистам на Python было проще мы воспользуемся автоматической типизацией с использованием ключевого слова auto.    Функций sliced возвращает многомерный срез. На входе она может принимать как простые массивы так и ranges. В итоге у нас получился куб размером 5x5x40 состоящий из чисел от 0 до 999.    Пару слов о том что такое Ranges. На русский язык их правильнее переводить как диапазоны. Ranges позволяют описать правила перебора данных любого типа данных, будь то класс или структура. Для этого достаточно чтобы вы реализовали функцию: front, возвращающую первый элемент диапазона, popFront, переходящий к следующему элементу и empty возвращающую булевое значение показывающую, что перебираемая последовательность пуста. Ranges позволяют выполнять ленивый перебор обращаясь к данным по мере их необходимости. Более подробно концепция Ranges освещена тут.    Обратите внимание, что никаких пустых аллокаций памяти! Это происходит потому что iota так же позволяет генерировать ленивые диапазоны, а sliced в ленивом режиме так же принимает данные из iota и обрабатывает их по мере поступления.     Как вы видите std.ndslice работает несколько иначе чем Numpy. Numpy создает собственный тип для данных, тогда как std.ndslice представляет способ манипуляции с уже существующими данными. Это позволяет вам во всей вашей программе использовать одни и те же данные не тратя ресурсы на бесполезный мемори аллокейшен! Не сложно догадаться, что это крайне серьезно сказывается на производительности ваших решений.     Давайте посмотрим на следующий пример. В нем мы будем получать данные из stdin, фильтровать только уникальные строки, сортировать их, и затем возвращать в stdout.    import std.stdio; import std.array; import std.algorithm;  void main() {     stdin         // get stdin as a range         .byLine(KeepTerminator.yes)         .uniq         // stdin is immutable, so we need a copy         .map!(a => a.idup)         .array         .sort         // stdout.lockingTextWriter() is an output range, meaning values can be         // inserted into to it, which in this case will be sent to stdout         .copy(stdout.lockingTextWriter()); }  Желающим более подробно разобраться с ленивой генерацией диапазонов рекомендую почитать эту статью.    Так как slice имеет три измерения это диапазон который возвращает диапазон диапазонов (ranges of ranges). Это хорошо видно на следующем примере:    import std.range : iota; import std.stdio : writeln; import std.experimental.ndslice;  void main() {     auto slice = sliced(iota(1000), 5, 5, 40);      foreach (item; slice) {         writeln(item);     } }  Результат его работы будет следующим (троеточия для краткости):    [[0, 1, ... 38, 39], [40, 41, ... 78, 79], [80, 81, ... 118, 119], [120, 121, ... 158, 159], [160, 161, ... 198, 199]]  ...  [[800, 801, ... 838, 839], [840, 841, ... 878, 879], [880, 881, ... 918, 919], [920, 921, ... 958, 959], [960, 961, ... 998, 999]]  Цикл foreach работает почти так же как for в Python. Однако в D вы можете его использовать так в стиле Cи, так и на манер циклов в Python, но без мороки с enumerate или xrange.     Используя UFCS (Uniform Function Call Syntax) вы можете переписать код на следующий манер:    import std.range : iota; import std.experimental.ndslice;  void main() {     auto slice = 1000.iota.sliced(5, 5, 40); }  UFCS позволяет записывать вызов методов по цепочке и писать:    a.func(b)  вместо:     func(a, b)  Давайте теперь сгенирируем пустой проект при помощи менеджера пакетов dub. Командой dub init и в файле \source\app.d напишем:    import std.experimental.ndslice;  void main() { }  В настоящий момент std.experimental.ndslice; находится в секции std.experimental. Это не значит, что он сырой. Это значит, что ему нужно настояться.    Теперь соберем проект командой:    dub  Модуль D ndslice весьма похож на Numpy:    a = numpy.arange(1000).reshape((5, 5, 40)) b = a[2:-1, 1, 10:20]  Равнозначно:    auto a = 1000.iota.sliced(5, 5, 40); auto b = a[2 .. $, 1, 10 .. 20];  Теперь давайте создадим двухмерный массив и получим середину каждой его колонки.    Python:     import numpy  data = numpy.arange(100000).reshape((100, 1000)) means = numpy.mean(data, axis=0)  D:    import std.range; import std.algorithm.iteration; import std.experimental.ndslice; import std.array : array;  void main() {     auto means = 100_000.iota         .sliced(100, 1000)         .transposed         .map!(r => sum(r) / r.length)         .array; }  Для того, чтобы данный код не работал в ленивом режиме мне пришлось вызывать метод array. Однако в реальном приложении результат не будет рассчитан, пока он используется в другой части программы.     В настоящий момент в Phobos нет встроенных модулей работы со статистикой. Поэтому в примере используется простая лямбда для нахождения среднего значения. Функция map! имеет в конце восклицательный знак. Это значит, что это шаблонная функция. Она позволяет на этапе компиляции генерировать код основанный на выражении указанном в ее теле. Вот хорошая статья о том как работают сами шаблоны в D.    Хотя код на D и получился чуть более многословным, чем на Python, но благодаря map! код будет работать с любыми входными данными являющимися диапазоном (range). В то время как код на Python будет работать только со специальными массивами из Numpy.    Тут нужно сказать, что после проведения этого теста оказалось, что Python проиграл D в разы и после долгих дисскусий на Hacker News, я понял что допустил ошибку и сравнение оказалось не совсем корректным. iota динамически создает данные которые принимает функция sliced. И соответственно мы не трогаем память до момента последней ее релокации. Так же D возвращает массив с типом данных long в то время как Numpy из double. В итоге переписал код и довел значение массива до 1000 000 вместо 10 000. Вот что получилось:    import std.range : iota; import std.array : array; import std.algorithm; import std.datetime; import std.conv : to; import std.stdio; import std.experimental.ndslice;  enum test_count = 10_000;  double[] means; int[] data;  void f0() {     means = data         .sliced(100, 10000)         .transposed         .map!(r => sum(r, 0L) / cast(double) r.length)         .array; }  void main() {     data = 1_000_000.iota.array;      auto r = benchmark!(f0)(test_count);     auto f0Result = to!Duration(r[0] / test_count);     f0Result.writeln; }  Тесты проводил на 2015 MacBook Pro with a 2.9 GHz Intel Core Broadwell i5. В Python для замера скорости использовал функцию %timeit в D std.datetime.benchmark. Компилировал все при помощи LDC v0.17 со следующими ключами: ldmd2 -release -inline -boundscheck=off -O. Или если вы используете dub то аналогом этих ключей будут опции dub --build=release-nobounds --compiler=ldmd2.    Вот результаты первого теста:    Python: 145 µs LDC:      5 µs  D is 29x faster  Вот теста исправленной версии:    Python: 1.43 msec LDC:  628    μs  D is 2.27x faster  Согласитесь весьма не плохая разница учитывая то что Numpy написан на С, а в D все ругают сборщик мусора.    Как D позволяет избежать проблем Numpy?  Да Numpy быстр, но быстр он лишь в сравнении с простыми масисвами Python. Но проблема в том, что он с этими простыми массивами совместим лишь частично.    Библиотека Numpy находится где то сбоку от остального Python. Она живет своей жизнью. В ней используются свои функции, она работает со своими типами. К примеру если нам потребуется использовать массив созданный в Python в NumPy нам нужно будет использовать np.asarray который скопирует его в новую переменную. Беглый поиск по github показал что тысячи проектов используют этот костыль. В то время как данные могли бы быть просто переданы из одной функции в другую без этих пустых копирований.    import numpy as np  a = [[0.2,0.5,0.3], [0.2,0.5,0.3]] p = np.asarray(a) y = np.asarray([0,1])  Эту проблему пытаются решить переписав часть стандартной библиотеки Python на использование типов Numpy. Однако это все равно не полноценное решение, которое приводит к замечательным приколам, когда написав:    sum(a)  вместо:    a.sum()  мы получаем 10x кратное падение в скорости.    У D таких проблем просто нет by design. Это компилируемый, статически типизируемый язык. Во время генерации кода известны все типы переменных. В самом std.ndslice вы получаете полный доступ ко всем функциям библиотеки Phobos к примеру к таким замечательным вещам как std.algorithm и std.range. Ах да, при этом вы сможете использовать шаблоны D позволяющие генерировать код прямо на этапе компиляции.    Вот пример:    import std.range : iota; import std.algorithm.iteration : sum; import std.experimental.ndslice;  void main() {     auto slice = 1000.iota.sliced(5, 5, 40);     auto result = slice         // sum expects an input range of numerical values, so to get one         // we call std.experimental.ndslice.byElement to get the unwound         // range         .byElement         .sum; }  Вы берете и просто используете функцию sum и она просто берет и работает, ровно как и любая другая функция из базовой библиотеки.    В самом Python для того чтобы получить список опреленной длинны инициализированный определенным значением нам нужно написать:    a = [0] * 1000  В Numpy уже будет совсем по-другому:    a = numpy.zeros((1000))  И если вы не вспользуетесь Numpy то ваш код будет в 4 раза медленне не считая лишней операции копирования съедающей память. В D нам на помощь приходят range, который позволяют сделать эту же операцию быстро и без пустых операций копирования:    auto a = repeat(0, 1000).array;  И если нужно мы можем тут же вызвать ndslice:    auto a = repeat(0, 1000).array.sliced(5, 5, 40);  Главное преимущество Numpy в настоящее время это его распространенность. Сейчас он используется реально очень широко от банковских систем до машинного обучения. По нему очень много книг, примеров и статей. Однако математические возможности в D очевидно будут уже очень скоро расширены. Так автор ndslice заявил, что сейчас ведет работы над BLAS (Basic Linear Algebra Subprograms) для Phobos, который так же будет полностью интегрирован с ndslice и со стандартной библиотекой.     Мощная математическая подсистема позволит очень эффективно решать ряд задач где необходима работы с большими данными. К примеру системы компьютерного видения. Прототип одной из таких систем уже разрабатывается и называется DCV.    Обработка изображений на D  Следующий пример покажет как работает медианный фильтр позволяющий ликвидировать шумы на изображении. Функция movingWindowByChannel может быть использована так же в других фильтрах где требуется использование скользящее окно. movingWindowByChannel позволяет перемещаться по изображению используя скользящее окно. Каждое такое окно передается в фильтр, который рассчитывает значения пикселей на основании выбранной зоны.     Эта функция не обрабатывает зоны с частичным перекрытием. Однако с ее помощью можно вычеслить значения в них тоже. Для этого нужно создать увеличенное изображение с краями отражающими границы оригинального изображения и затем обработать его.    /** Params:     filter = unary function. Dimension window 2D is the argument.     image = image dimensions `(h, w, c)`,         where с is the number of channels in the image     nr = number of rows in the window     nс = number of columns in the window  Returns:     image dimensions `(h - nr + 1, w - nc + 1, c)`,         where с is the number of channels in the image.         Dense data layout is guaranteed. */ Slice!(3, C*) movingWindowByChannel(alias filter, C) (Slice!(3, C*) image, size_t nr, size_t nc) {     // local imports in D work much like Python's local imports,     // meaning if your code never runs this function, these will     // never be imported because this function wasn't compiled     import std.algorithm.iteration: map;     import std.array: array;      // 0. 3D     // The last dimension represents the color channel.     auto wnds = image         // 1. 2D composed of 1D         // Packs the last dimension.         .pack!1         // 2. 2D composed of 2D composed of 1D         // Splits image into overlapping windows.         .windows(nr, nc)         // 3. 5D         // Unpacks the windows.         .unpack         // 4. 5D         // Brings the color channel dimension to the third position.         .transposed!(0, 1, 4)         // 5. 3D Composed of 2D         // Packs the last two dimensions.         .pack!2;      return wnds         // 6. Range composed of 2D         // Gathers all windows in the range.         .byElement         // 7. Range composed of pixels         // 2D to pixel lazy conversion.         .map!filter         // 8. `C[]`         // The only memory allocation in this function.         .array         // 9. 3D         // Returns slice with corresponding shape.         .sliced(wnds.shape); }  Следующая функция пример того как можно рассчитать медиану у объекта. Функция сильно упрощена в целях повышения читаемости.    /** Params:     r = input range     buf = buffer with length no less than the number of elements in `r` Returns:     median value over the range `r` */ T median(Range, T)(Range r, T[] buf) {     import std.algorithm.sorting: sort;      size_t n;      foreach (e; r) {         buf[n++] = e;     }      buf[0 .. n].sort();     immutable m = n >> 1;     return n & 1 ? buf[m] : cast(T)((buf[m - 1] + buf[m]) / 2); }  Ну а теперь собственно сам Main:    void main(string[] args) {     import std.conv: to;     import std.getopt: getopt, defaultGetoptPrinter;     import std.path: stripExtension;      // In D, getopt is part of the standard library     uint nr, nc, def = 3;     auto helpInformation = args.getopt(         ""nr"", ""number of rows in window, default value is "" ~ def.to!string, &nr,         ""nc"", ""number of columns in window, default value is equal to nr"", &nc);      if (helpInformation.helpWanted)     {         defaultGetoptPrinter(             ""Usage: median-filter [<options...>] [<file_names...>]\noptions:"",             helpInformation.options);         return;     }      if (!nr) nr = def;     if (!nc) nc = nr;      auto buf = new ubyte[nr * nc];      foreach (name; args[1 .. $])     {         import imageformats; // can be found at code.dlang.org          IFImage image = read_image(name);          auto ret = image.pixels             .sliced(cast(size_t)image.h, cast(size_t)image.w, cast(size_t)image.c)             .movingWindowByChannel                 !(window => median(window.byElement, buf))                  (nr, nc);          write_image(             name.stripExtension ~ ""_filtered.png"",             ret.length!1,             ret.length!0,             (&ret[0, 0, 0])[0 .. ret.elementsCount]);     } }  Если не все примеры показались вам понятными рекомендую вам прочитать бесплатную версию замечательной книги Programming in D.    P.S. Если знаете как данную публикацию перевести в статус ""переводы"", то напишите в приват.","D, dlang, python, numpy, ndslice, матрицы, массивы",D std.ndslice как замена Python Numpy
"Цикл for в R может быть очень медленным, если он применяется в чистом виде, без оптимизации, особенно когда приходится иметь дело с большими наборами данных. Есть ряд способов сделать ваш код быстрее, и вы, вероятно, будете удивлены, узнав насколько.    Эта статья описывает несколько подходов, в том числе простые изменения в логике, параллельную обработку и Rcpp, увеличивая скорость на несколько порядков, так что можно будет нормально обрабатывать 100 миллионов строк данных или даже больше.     Давайте попробуем ускорить код с циклом for и условным оператором (if-else) для создания колонки, которая добавляется к набору данных (data frame, df). Код ниже создает этот начальный набор данных.   # Создание набора данных col1 <- runif (12^5, 0, 2) col2 <- rnorm (12^5, 0, 2) col3 <- rpois (12^5, 3) col4 <- rchisq (12^5, 2) df <- data.frame (col1, col2, col3, col4)   В этой части: векторизация, только истинные условия, ifelse.  В следующей части: which, apply, побайтовая компиляция, Rcpp, data.table.    Логика, которую мы собираемся оптимизировать  Для каждой строки в этом наборе данных (df) проверить, превышает ли сумма значений 4. Если да, новая пятая переменная получает значение «greater_than_4», в противном случае — «lesser_than_4».  # Исходный код на R: Перед векторизацией и предварительным выделением system.time({   for (i in 1:nrow(df)) { # for every row     if ((df[i, 'col1'] + df[i, 'col2'] + df[i, 'col3'] + df[i, 'col4']) > 4) { # check if > 4       df[i, 5] <- ""greater_than_4"" # присвоить значение в 5-й колонке     } else {       df[i, 5] <- ""lesser_than_4"" # присвоить значение в 5-й колонке     }   } })   Все последующие вычисления времени обработки были проведены на MAC OS X с процессором 2.6 ГГц и 8ГБ оперативной памяти.    Векторизируйте и выделяйте структуры данных заранее  Всегда инициализируйте ваши структуры данных и выходные переменные, задавая требуемые длину и тип данных до того, как запустить цикл вычислений. Постарайтесь не увеличивать объем ваших данных пошагово внутри цикла. Давайте сравним, как векторизация улучшает скорость на разных размерах данных от 1000 до 100 000 строк.   # после векторизации и предварительного выделения output <- character (nrow(df)) # инициализируем выходной вектор system.time({   for (i in 1:nrow(df)) {     if ((df[i, 'col1'] + df[i, 'col2'] + df[i, 'col3'] + df[i, 'col4']) > 4) {       output[i] <- ""greater_than_4""     } else {       output[i] <- ""lesser_than_4""     }   } df$output})     Исходный код и код с векторизацией    Уберите за границы цикла условные операторы  Вынесение за границы цикла условных проверок сравнимо по выигрышу с векторизацией самой по себе. Тесты проводились на диапазонах от 100 000 до 1 000 000 строк. Выигрыш в скорости снова колоссален.  # после векторизации и предварительного выделения, вынесения условного оператора за границы цикла output <- character (nrow(df)) condition <- (df$col1 + df$col2 + df$col3 + df$col4) > 4  # проверка условия вне цикла system.time({   for (i in 1:nrow(df)) {     if (condition[i]) {       output[i] <- ""greater_than_4""     } else {       output[i] <- ""lesser_than_4""     }   }   df$output <- output })     Проверка условия вне цикла    Выполнить цикл только для истинных условий  Еще одна оптимизация, которую тут можно использовать — запустить цикл только по истинным условиям, предварительно проинициализировав выходной вектор значениями False. Ускорение здесь сильно зависит от количества случаев с True в ваших данных.    В тестах сравнивается производительность этого и предыдущего улучшения на данных от 1 000 000 до 10 000 000 строк. Обратите внимание на увеличение количества нулей здесь. Как и ожидалось, есть вполне определенное заметное улучшение.  output <- character(nrow(df)) condition <- (df$col1 + df$col2 + df$col3 + df$col4) > 4 system.time({   for (i in (1:nrow(df))[condition]) {  # запуск цикла только по истинным условиям     if (condition[i]) {       output[i] <- ""greater_than_4""     } else {       output[i] <- ""lesser_than_4""     }   }   df$output })     Запуск цикла только по истинным условиям    Используйте ifelse(), где это возможно  Эту логику можно сделать гораздо быстрее и проще с помощью ifelse(). Синтаксис аналогичен функции if в MS Excel, но ускорение феноменальное, особенно с учетом того, что здесь нет предварительного выделения, и условие проверяется каждый раз. Похоже, что это очень выгодный способ ускорить выполнение простых циклов.  system.time({   output <- ifelse ((df$col1 + df$col2 + df$col3 + df$col4) > 4, ""greater_than_4"", ""lesser_than_4"")   df$output <- output })     Только истинные условия и ifelse",R,"Стратегии по ускорению кода на R, часть 1"
"Под конец прошлого года компания Amazon выпустила (наконец-то!) C++ SDK для своей платформы AWS. Непонятно, почему так долго тянули — есть масса приложений, написанных на С++, из которых давно хотелось удобно пользоваться AWS. Не джавой же единой, в самом деле.     Всегда интересно посмотреть на что-то, написанное на С++ в наши дни с нуля, без груза обратной совместимости и странных архитектурных решений, присущих некоторым разработкам прошлого. Я не буду тут пересказывать документацию, но остановлюсь на ключевых моментах, определяющих всё поведение SDK.    С++11  Как в своё время должен был быть разрушен Карфаген, так сегодня должны умереть все библиотеки и приложения, до сих пор не перешедшие на С++11. Им просто не место в современном мире. Со слезами на глазах я читаю посты вроде этого о библиотеках, которые «чтобы расширить аудиторию — стараются обходиться без C++11». Как по мне, то это решение где-то около «будем ездить на лошадях, потому что они привычнее автомобилей». Новое SDK от Amazon не гонится за ретроградами — из коробки нам предлагаются все фичи С++11: auto, умные указатели, лямбды и т.д. Дать возможность указать в виде колбэк-функции лямбду — жизненно необходимо, без этого нас даже Javascript засмеёт. Ну и никаких new\delete в явном виде, конечно. Красота!    Простота кода  Как результат предыдущего пункта — посмотрите только на код, который получается в результате. Положим в DynamoDB пару «ключ-значение»:    DynamoDBClient client; AttributeValue haskKeyAttribute; hashKeyAttribute.SetS(""SampleHashKeyValue"") AttributeValue valueAttribute; valueAttribute.SetS(""SampleValue"")  PutItemRequest putItemRequest; putItemRequest.WithTableName(""TestTableName"").AddItem(""HashKey"", hashKeyAttribute).AddItem(""Value"", valueAttribute);  client.PutItem(putItemRequest);     Ну вот и скажите мне, любители выражений вроде «С++ слишком сложен и избыточен» — чем это хуже Java или Python? А памяти съест меньше и выполнится быстрее.    Нет лишних зависимостей  Ребята из Амазона понимают, что каждая лишняя зависимость порождает дополнительную сложность при подключении и сборке проекта, а значит уменьшает количество пользователей. Поэтому AWS C++ SDK не зависит ни от boost, ни от других фундаментальных фреймворков. Даже маленьких библиотек, вроде логирования, в нём нет. Единственная зависимость — HTTP клиент. Всё-таки запихивать в SDK собственную реализацию этого дела было бы уже чересчур. Из коробки предлагается на выбор привязка к системным CURL, WinHTTP, WinInet или возможность внедрить свою реализацию.    CMake  Мир С++ по-маленьку движется к стандартизации описания проектов. CMake может быть ещё пока и не победил окончательно, но у тех, кому нужно дать возможность сборки под как можно более широкий набор платформ и компиляторов, этот вариант приходит на ум первым. Не отличился тут и Amazon — всё в CMake и компилируйте себе на чём угодно. Очень удобно работать в CLion.    Dependency Injection  Внедрение зависимостей, как основа конфигурации приложения. У нас есть возможность использовать дефолтные реализации от Amazon или вместо них свои буквально для всего: http-клиента, логирования, авторизации, аллокатора памяти, стратегий ограничения скорости и повтора попыток, обработчика асихронных событий и т.д. В итоге каждый из этих компонентов получается небольшим, поддающимся тестированию и не вызывающим проблем при интеграции.    Логирование  Библиотеки для логирование в зависимостях нет. Если вы хотите получать логи — реализуйте определённый интерфейс и передайте его в конфиг при инициализации клиента. Будете получать колбэк-вызовы с текстом логов — и пишите их куда хотите. Дефолтная реализация пишет в логи всё подряд, в текущую папку и удаляет старые логи каждый час.    Синхронные и асинхронные версии всех операций  Amazon понимает, что архитектура приложений бывает разной и поэтому все операции представлены как в синхронном, так и в асинхронном вариантах. В синхронном варианты вы ждёте окончания операции (возможно, долго ждёте, если, например, грузите большой файл), а после этого проверяете, что вернула функция:    CreateTableOutcome createTableOutcome = dynamoDbClient->CreateTable(createTableRequest);   if (createTableOutcome.IsSuccess())   { ...   } else { ...   }     В асинхронном варианте вы указываете колбэк-функцию, которая будет вызвана по окончанию операции. В колбэк-функцию будет переданы данные запроса, ответа плюс некоторый контекст, который вы можете определить перед вызовом, чтобы поном понять что это за вызов и как обработать его результат.     void putItemFinished(const DynamoDBClient *client, const PutItemRequest& request,  				const PutItemOutcome& outcome, 				const std::shared_ptr<AsyncContext>& context) {  if (outcome.IsSuccess())  {  	SEND_QUEUE.erase(context->GetUUID());  } }  auto context = Aws::MakeShared<AsyncContext>(""Hello!""); context.SetUUID(""UniqueRequestKey""); SEND_QUEUE[""UniqueRequestKey""] = putItemRequest; client.PutItemAsync(putItemRequest, &putItemFinished, context);     Снова слава С++11: в качестве колбэка можно передавать не только функцию, но и метод класса некоторого объекта (через std::bind) или лямбду.    Поддержка Asynchronous executor  Если вы любите асинхронные операции и у вас в программе уже есть какой-нибудь пул бэкграунд-потоков, класс выполнения асинхронных операций — вы можете задействовать его для запуска операций AWS C++ SDK. Таким образом все асинхронные операции в вашем приложении будут выполнятся по одной схеме и вы сэкономите ресурсы на фоновых потоках, которые запускала бы SDK в противном случае.    Ограничение скорости  Если вам нужно ограничивать скорость передачи данных для некоторых операций — всё в вашей власти. Реализуете интерфейс RateLimiter, передаёте свою реализацию в конфиг при инициализации — и получаете полный контроль полосы передачи данных.    Нет исключений  Вопрос использования или неиспользования исключений — давний холивар в мире С++. Но во всё большем количестве примеров в последнее время исключений стараются избегать. В AWS C++ SDK исключений тоже нет. Аргументируют большей гибкостью и производительностью. Нужен код ошибки — проверяйте outcome.IsSuccess(), если там false — в outcome будут также дополнительные данные об ошибке. Не знаю как вам, а мне такой подход нравится.    Включенная по умолчанию стратегия повтора попыток выполнения операции  Поскольку любая SDK от Amazon — это в первую очередь история о сетевых операциях, то нужно понимать, что сетевые операции частенько заканчиваются ошибками. Где-то Wi-Fi подвёл, где-то мобильная сеть, а где-то вроде бы стабильный наземный канал почему-то отказался загружать смешной трёхтеррабайтный файл. AWS C++ SDK по-умолчанию пытается повторить закончившуюся ошибкой операцию. Детали алгоритма (количество попыток повтора, время между попытками) можно контролировать собственной реализацией стратегии повторов.    Возможность определения аллокаторов памяти  Если для вас важно, сколько памяти использует ваше приложение, когда оно её выделяет и когда удаляет, как быстро это происходит (а для вас это всё должно быть интересно, зачем иначе вы вообще пишете на С++?) — вы можете определить собственный аллокатор памяти. Для этого нужно унаследоваться от MemorySystemInterface и определить несколько методов. Можно попробовать стянуть откуда-нибудь хороший аллокатор, заточить его под свои объёмы данных и алгоритмы и получить неплохой прирост производительности.    GitHub  Всё живёт на GitHub, принимаются Issues, Pull Requests. Контакт с сообществом поставлен хорошо.    Удачи в использовании AWS из приложений на С++!","aws, c++",C++ SDK для Amazon Web Services
"Предлагаем вашему вниманию подборку с ссылками на полезные ресурсы, интересные материалы и IT-новости            Веб-разработка      CSS      Javascript      Браузеры      Дизайн      Новости      Занимательное           Веб-разработка      Тестируем вёрстку правильно   6 впечатляющих веб-технологий 2015 года   Я web разработчик и я уже 10 дней не могу написать простейшее приложение   Как выбрать фреймворк для frontend-разработки. Перевод статьи How To Pick a Frontend Web Framework   Эффективное использование ARIA в HTML5  Google не советует закрывать ссылки от индексации через JavaScript и AJAX   Готовимся к HTTP/2: руководство для дизайнеров и разработчиков (Getting Ready For HTTP/2: A Guide For Web Designers And Developers)   Все, что вы хотели знать про AMP от Google (Turn Your AMP Up To 11: Everything You Need To Know About Google’s Accelerated Mobile Pages)   5 способов улучшения рейтинга вашего сайта в поисковой выдаче Google с помощью CDN (5 Ways to Improve Google Ranking in SERPs With a CDN)   The Anatomy of a Frame   Уборка дома после Internet Explorer (Cleaning House after Internet Explorer)   Руководство по Jade для новичков (A Jade Tutorial for Beginners)   Удаленная отладка для фронтенд-разработчиков (Remote Debugging for Front-End Developers)   Frontend дизайн (Frontend Design)   API: как работает Интернет за кулисами (APIs: How the Internet Works Behind the Scenes)   mjml — единственный фреймворк, позволяющий делать адаптивные email-шаблоны легко  Эффектный веб:     Еженедельная подборка красивых эффектов на CSS/SVG/JS #25   Создание концентрических кругов с симпатичными цветовыми палитрами (SVG)   Полароидный эффект заполнения сетки с продуктами   Шаблон выезжающих панелей (Sliding Panels Template)   Such Magic демо   Codeology — кибер-существа из гитхаба           CSS      Несколько неочевидных frontend-хитростей   Все способы вертикального выравнивания в CSS  Мысли вслух о подсетках в CSS Grid Layout. Перевод статьи Subgrids thinking out loud сообществом css-live.ru  Свойство background-clip и его применения. Перевод статьи Аны Тудор The `background-clip` Property and its Use Cases  Будущее загрузки CSS. Перевод статьи The future of loading CSS Джейка Арчибальда.  Управление загрузкой CSS с помощью пользовательских свойств. Перевод статьи Control CSS loading with custom properties от него же   Одно свойство CSS, которое я действительно хочу (The One CSS Feature I Really Want)   CSS Custom Properties: как использовать переменные в Chrome 49   Hand-Drawn Border Buttons   Реализация сердечек на CSS (I Heart CSS)   Быстрый обзор `object-fit` и `object-position`   Стайлгайд для бедных (Poor Man's Styleguide)   Sass: Плейсхолдеры и селекторы  @extend-Only (Placeholders and  @extend-Only Selectors)   Создание динамического портфолио с помощью CSS Scrolling Snap Points (Make a Dynamic Portfolio with CSS Scrolling Snap Points)   Отключите задержку в 300ms delay на мобильных с помощью CSS touch-action или асинхронно скачайте FastClick как полифилл   Убираем двойные проблемы у выделяемых цитат (Taking the Double Trouble Out of Pull Quotes)   Начало работы с системами сеток на Flexbox (Getting started with Flexbox grid systems)   Переключение тем с помощью CSS custom свойств (Theme switcher using CSS custom properties)   Пингвин (Без HTML и JS) (Penguin (No HTML, No JS))      JavaScript      Дополнительные свойства Javascript   Чем плох JavaScript в большом проекте? С какими проблемами мы столкнулись и как их решали   Правила хорошего тона при написании плагина на jQuery  Radio JS, Выпуск 34: Синтаксический диабет. Андрей рассказывает о своём опыте использования React Native. Обсуждаем новый V8, загрузку JS по HTTP/2.0 и оператор pipeline.  Локализация времени в JavaScript  Советы и техники для манипуляций с DOM элементами   Как выучить JavaScript (How to learn JavaScript)   MERN — простейший путь по созданию изоморфических JS приложений с MongoDB, Express, React & Redux, Node (What's MERN?)   Генерация PDF из веб-страниц на лету с jsPDF (Generating PDFs from Web Pages on the Fly with jsPDF)  Фреймворки:      React vs Angular 2 — сравнение несравнимого? (React vs Angular 2 — compare the incomparable?)   Dan Abramov о переводе redux и еще части проектов в организацию reactjs на GitHub   Анимация неанимированного. Плавные переходы при изменении позиций в react.js (Animating the Unanimatable. Smooth reordering transitions in React.js)   Как использовать Laravel API с AngularJS (How to Consume Laravel API with AngularJS)   Руководство по стилю для Angular приложений (Angular Style Guide)   Вызов Rest сервиса в AngularJS (Call Rest Service In AngularJS)   Необходимые инструменты для создания SPA с AngularJS (Essential Tools for Building SPAs with AngularJS)   Видео: урок по React Props, Defaults и Modules (Watch: A Lesson on React Props, Defaults and Modules)   Плавные игровые анимации в React (Smooth Game Animations in React)   Функциональное программирование в Ember: Daniel Chappell в Q2 eBanking (Functional Programming in Ember: Daniel Chappell at Q2 eBanking)       ES6:     ES6 const: иммутабельность ни при чём   ES2016: Should the Future of JavaScript Be Developer-Driven?   Шпаргалка по Javascript ES6 от duckduckgo (Javascript ES6 Cheatsheet — the best of JS ES6)   ES6 library starter — основа для библиотеки на ES6 (ES6 library starter)        Как работают промисы в JavaScript (JavaScript Promises – How They Work)   JavaScript: как создать и использовать обьект Promise (JavaScript: How to Define and Process a Promise Object)   Объяснение неизменяемости (Explaining immutability)   Отправка писем с помощью Gmail JavaScript API (Sending Emails with the Gmail JavaScript API)   Введение в достаточно чистое функциональное программирование (An Introduction to Reasonably Pure Functional Programming)   Интервью с создателем Durandal и Aurelia, Rob-ом Eisenberg (An Interview with Durandal’s Rob Eisenberg)   Видео по теме Full Stack JavaScript (Full Stack JavaScript)   Простой пример Ajax/jQuery.getJSON (Ajax/jQuery.getJSON Simple Example)   Descartes — экспериментальная библиотека для написания CSS в JS (Descartes — an experimental library for writing CSS in JavaScript)   Введение в Web MIDI (Introduction to Web MIDI)   search-index — механизм полнотекстового поиска для браузера и Node.js (search-index — a persistent full text search engine for the browser and Node.js)  Плагины:      Bricks.js -генератор структуры в стиле masonry для фиксированных элементов   okayNav — универсальный jQuery плагин для адаптивной навигации   jQuery easypin — добавление информационных булавок на изображения   Sticky Elements — элементы-прилипалы          Браузеры     Firefox с поддержкой мультипроцессовости будет изначально использовать на 10%-20% больше памяти  Как поменять количество используемых процессов в Firefox e10s  Google Chrome будет рекомендовать пользователям новостные статьи  Debian и Mozilla договорились об использовании бренда Firefox вместо Iceweasel  Firefox не примет участие в соревнованиях Pwn2Own 2016  Изучение изменения потребления памяти Firefox в многопроцессном режиме   Представление компилятора B3 JIT (Introducing the B3 JIT Compiler)      Дизайн      Мои правила хорошего интерфейса  Мнение: «Лучшая иконка — это текст»  Графические редакторы:     Sketch + Craft: союз, созданный на цифровых небесах  Как хакнуть Sketch Content Generator для использования вашего собственного контента  Интервью с основателем Sketch, Питером Омвли  Безболезненный переход с Photoshop на Sketch   Мои настройки Sketch (Getting Set Up With Sketch)   Почему Sketch — идельный инструмент для дизайна интерфейсов (Why Sketch is the Perfect Tool for Interface Design)   Illustrator за 60 секунд: как создать свою настройку рабочего пространства (Illustrator in 60 Seconds: How to Create a Custom Workspace)       UI/UX:      О китайских трендах в мобильном UI (More Chinese Mobile UI Trends)   5 принципов для эффективной анимации в UX (Five Principles for Effective Animation in UX)   UX дизайн в 14-ти простых шагах (UX Design in 14 Simple Steps)   collectui — коллекция решений для UI (Daily inspiration collected from daily ui archive and beyond. Hand picked, updating daily.)        Методы для использования микро-взаимодействий на вашем сайте (Methods for Using Micro-Interactions on Your Site )   9 способов использования контроля над разумом в веб-дизайне (9 ways to use mind control in web design)   Зарубежные художники: южная Африка (International Artist Feature: South Africa)   Fred Gelli про брендинг олимпийских игр в Рио и планировании открытия пара-олимпийских игр (Fred Gelli on branding the Rio Olympics and planning the Paralympics Opening Ceremony)   Дизайн с осмысленными данными (Designing with Meaningful Data)   Дизайн для Apple TV (Designing for the Apple TV)   Вспоминая дизайнерское наследие другой великой компании Стива Джобса (Remembering The Design Legacy Of Steve Jobs's Other Great Computer Company)   Делаем дизайн для видео Walkie Talkie (Designing a Video Walkie Talkie)   Креативность и продуктивность в FiftyThree (Creativity and Productivity at FiftyThree)   Как Quartz привлекает новых пользователей (How Quartz Onboards New Users)   Руководство по авиа-рекламе 12-летней давности (A 12 Year-Old’s Guide to Airline Advertisements)   Uber's Atomic Meltdown   Преимущества наема сторонней дизайнерской команды (The (not so) secret benefits of hiring an external design team)   Обновленный Marvel 2.0 (Welcome to Marvel 2.0)   Как лампочка стала ассоциацией с новой идеей? (How did the lightbulb become associated with a new idea?)   Почему символ сердца настолько не верен с точки зрения анатомии? (Why Is the Heart Symbol so Anatomically Incorrect?)   Детали дизайна: Quartz для iOS (Design Details: Quartz for iOS)   Редактирование последнего неуместного сообщения (Editing your last embarrassment)   Последние лучшие источники для вдохновения дизайном (The 14 Latest and Greatest Sources of Design Inspiration)   Сделайте прототипирование менее болезненным используя эти подсказки (Make Prototyping Less Painful With These Tips)   Создание вайрфреймов контента для адаптивного дизайна (Quick Tips Creating Content Wireframes For Responsive Design)   Как создавать руководства по стилю для веб проектов в InVision (How To Build A Style Guide For Your Web Project Using InVision)   Пять десятков взрывных визиток (50 Unique Business Cards That Will Make Your Mind Explode)   Логотипы, созданные женщинами (Logos Designed by Women)   Подбор шрифтовой пары для книги с рецептами (Choosing and pairing typefaces for cookbooks)   ippy — текстовые иконки   Новый логотип для Олимпийских игр в Париже в 2024-м (New Logo for Paris 2024 Candidate City by Dragon Rouge)      Подборка бесплатных дизайнерских печенек     Бесплатные текстуры кофейных разводов на бумаге  Векторный клипарт для мужского дизайна  Бесплатные текстуры пластика и пенопласта    Иконки на тему загрязнения и источников энергии (AI, EPS, SVG) (Freebie: Pollution & Energy Icons (AI, EPS, SVG))   Минималистичный набор UI мобильного чата (Free Download: A Minimal Mobile Chat UI Kit)   Chat–a минималистичный набор UI (Chat–a beautifully minimal UI kit)   RePrint — календари на 2016-й год   Foundry ∞ — коллекция векторных изображений   Наборы иконок посвященных спорту (Free Sport Icons and New Examples)    Набор иконок Nucleo Flat Business (AI & SVG) (Freebie: Nucleo Flat Business Icon Set (50 Icons, AI & SVG))   Olios: PSD шаблон для eCommerce сайтов (Olios: Free PSD Template for eCommerce Sites)   Наборы иконок для веб-дизайна (29 Of The Best Minimalist Icons For Web Design Projects)   Fenton — шрифт без засечек (Fenton Typeface Family)   Weem — шрифт с засечками (Weem — Free Font)      Новости и занимательное      GitHub добавил поддержку шаблонов для Issue и Pull-реквестов  Google открывает API сервиса Google Photos, умеющий распознавать объекты и эмоции  Twitter добавил встроенный поиск и публикацию GIF-изображений  Apple занялась решением «Проблемы 1970»  Apple серьёзно относится к технологиям виртуальной и дополненной реальности  Bluetooth SIG предложила платформу для вещей с подключением к Интернету  Первому электронному компьютеру исполнилось 70 лет  Google предложила забыть о Picasa  В Twitter'е появится поиск по анимированным GIF  Yahoo сократит 300 человек, а также закроет ряд онлайн-журналов  Уязвимость JSF**k, которую eBay отказался исправить, уже эксплуатируют хакеры  Полное руководство по настройкам конфиденциальности Windows 10  История MySpace: почему Крис Де Вольф и Том Андерсон проиграли конкуренцию с Facebook   Открытое письмо главы Apple (A Message to Our Customers, Tim Cook)     Просим прощения за возможные опечатки или неработающие/дублирующиеся ссылки. Если вы заметили проблему — напишите пожалуйста в личку, мы стараемся оперативно их исправлять.     Дайджест за прошлую неделю.  Материал подготовили dersmoll и alekskorovin.","дайджест, новости, css, js, es6, svg, анимация, react, angular, html5, браузеры, ссылки, веб-дизайн, веб-разработка",Дайджест интересных материалов из мира веб-разработки и IT за последнюю неделю №199 (15 — 21 февраля 2016)
"Новый способ охлаждения дата-центра от Microsoft      Погрузить компьютер под воду (не пытайтесь воспроизвести в реальной жизни — так бы гласила бегущая строка в рекламе). Но вот мировому гиганту Microsoft данное действие показалось отличнейшей идеей. Ведь известно, что на запитку систем охлаждения серверов затрачивается немалое количество электроэнергии, которая не так уж и дешево стоит. Почти половина от общего энергопотребления серверной фермы уходит на охладительные устройства (кондиционеры, чиллеры и т.д.). Поэтому многие компании уже давно в поисках новых эффективных методов охлаждения для своих «детищ». Не так давно, а если быть точными, то в августе 2015 года компания Microsoft решила сьекономить на оплате счетов за электроэнергию, погрузив экспериментальный прототип нового подводного дата-центра на морское дно. Так родился прототип ЦОД под кодовым названием Leona Philpot соответственно проекту Natick.               Прототип серверной фермы был погружен в километре от тихоокеанского побережья США и успешно эксплуатировался на протяжении четырех месяцев. Leona Philpot был снабжен большим количеством теплообменников, которые в свою очередь передавали излишнее тепло от серверов в холодную воду.         Инженеры Microsoft утверждают, что такой способ оптимизации температурного режима серверного оборудования является одним из наиболее эффективных и в то же время дешевых методов охлаждения серверных ферм. Кроме этого, он имеет ряд преимуществ. Так как около половины населения нашей планеты живет в 200 км от побережья (будь то море или океан), расположение дата-центра на дне «морском» будет способствовать еще и хорошей связности.            Компания Microsoft утверждает, что такие автономные блоки могут быть готовы к эксплуатации уже в течении 90 дней, то есть, чтобы развернуть «подводный» ЦОД уйдет всего три месяца, вместо обычных двух лет (как минимум одного года) стандартного строительства.        Своих инженеров и персонал компания «погружать под воду» вместе с ЦОД не будет. Планируется такое решение данной ситуации: автономные подводные блоки с серверами на борту будут иметь срок службы 5 лет (работать в режиме нон-стоп без физического обслуживания), после чего успешно подниматься на поверхность для устранения/замены возможно возникших неполадок, проверки и т.д и т.п.","дата-центр, Leona Philpot, ЦОД, система охлаждения, Microsoft, Natick",Leona Philpot — «подводный» способ охлаждения дата-центра от Microsoft
"Стараясь оставаться в тренде и следуя веяниям моды веб разработки, последнее веб приложение я решил реализовать как набор микросервисов на ruby плюс “толстый” клиент на ember. Одна из первых проблем, вставших перед мной была связана с аутентификацией запросов. Если в классическом, монолитном, приложении все просто, используем куки, сессии, подключаем какой-нибудь devise, то тут все как в первый раз.    Архитектура  За базу я выбрал JWT — Json Web Token. Это открытый стандарт RFC 7519 для представления заявок (claims) между двумя участниками. Он представляет из себя структуру вида: Header.Payload.Signature, где заголовок и payload это запакованые в base64 json хэши. Здесь стоит обратить внимание на payload. Он может содержать в себе все что угодно, в принципе это может быть и просто client_id и какая-то другая информация о пользователе, но это не очень хорошая идея, лучше передавать там только ключ идентификатор, а сами данные хранить где-то в другом месте. В качестве хранилища данных можно использовать что угодно, но мне показалось, что redis будет оптимальным, тем более что он пригодится и для других задач. Еще один важный момент — каким ключем мы будем подписывать наш токен. Самый простой вариант использовать один shared key, но это явно не самый безопасный вариант. Коль скоро мы храним данные сессии в redis, ничто не мешает нам генерировать уникальный ключ для каждого токена и хранить его там же.    Понятно, что генерировать токены будет сервис отвечающий за авторизацию, но кто и как будет их проверять? В принципе можно проверку затолкать в каждый микросервис, но это противоречит идеи их максимального разделения. Каждый сервис должен будет содержать логику обработки и проверки токенов да еще и иметь доступ к redis. Нет, наш цель получить архитектуру в которой все запросы приходящие в конечные сервисы уже авторизованы и несут в себе данные о пользователе (например в каком-нибудь специальном заголовке).    Проверка JWT токенов в NGinx  Тут мы и подходим к основной части этой статьи. Нам нужен какой то промежуточный элемент, через который бы проходили все запросы а он их аутентифицировал, заполнял клиентскими данными и посылал дальше. В идеале сервис должен быть легковесным и легко масштабироваться. Очевидным решением будет NGinx reverse proxy, благо мы можем добавить к нему логику аутентификации с помощью lua скриптов. Если быть точным, то мы будем использовать OpenResty — дистрибутив nginx с кучей “плюшек” из коробки. Для пущей красоты реализуем все это в виде Docker контейнера.    Начинать полностью с нуля пришлось. Есть прекрасный проект lua-resty-jwt уже реализующий проверку подписи JWT. Там даже есть пример работы с redis кешем для хранения подписи, осталось только его допилить чтобы:      вытягивать токен из Authorization заголовка   в случае успешной проверки доставать данные сессии и посылать их в X-Data заголовке  немного причесать ошибки, чтобы отдавался валидный JSON    Результат работы можно найти тут: resty-lua-jwt    В nginx.conf нужно прописать в http секцию ссылку на lua пакет:    http {    ...    lua_package_path ""/lua-resty-jwt/lib/?.lua;;"";    lua_shared_dict jwt_key_dict 10m;    ... }  Теперь для того чтобы аутентифицироваться запрос осталось в секцию location довавить:    location ~ ^/api/(.*)$ {     set $redhost ""redis"";     set $redport 6379;     access_by_lua_file /lua-resty-jwt/jwt.lua;     proxy_pass http://upstream/api/$1; }  Запускаем все это дело:    docker run --name redis redis  docker run --link redis -v nginx.conf:/usr/nginx/conf/nginx.conf svyatogor/resty-lua-jwt  И готово… ну почти. Надо еще положить в redis сессию и отдать клиенту его токен. jwt.lua плагин ожидает, что токен в своей Payload секции будет содержать хэш виа {kid: SESSION_ID}. В redis этому SESSION_ID должен соответствовать хэш как минимум с одним ключем secret, в котором находится общий ключ для проверки подписи. Еще там может быть ключ data, если он найдет то его содержимое уйдет в upstream сервис в заголовке X-Data. В этот ключ мы сложим сериализованый объект пользователя, ну или, как минимум, его ID, чтобы апстрим сервис понимал от кого же пришел запрос.    Логин и генерация токенов  Для генерации JWT есть великое множество библиотек, полное описание тут: jwt.io В моем случае я выбрал jwt гем. Вот как выглядит action SessionController#create    def new     user = User.find_by_email params[:email]     if user && user.authenticate(params[:password])         if user.kid and REDIS.exists(user.kid) > 0             REDIS.del user.kid         end          key = SecureRandom.base64(24)         secret = SecureRandom.base64(24)         REDIS.hset key, 'secret', secret         REDIS.hset key, 'data', {user_id: user.id}.to_json          payload = {""kid"" => key}         token = JWT.encode payload, secret, 'HS256'         render json: {token: token}     else         render json: {error: ""Invalid username or password""}, status: 401     end end  Теперь в нашем UI (ember, angular или же мобильное приложение) нужно получить у authorization сервиса токен и передавать его во всех запросах в заголовке Authorization. Как именно вы это будете делать зависит от вашего конкретного случая, так что я приведу лишь пример с cUrl.    $ curl -X POST http://default/auth/login -d 'email=user@mail.com' -d 'password=user' {""token"":""eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJraWQiOiI2cDFtdFBrVnhUdTlVNngxTk5yaTNPSDVLcnBGVzZRUCJ9.9Qawf8PE8YgxyFw0ccgrFza1Uxr8Q_U9z3dlWdzpSYo""}%  $ curl http://default/clients/v1/clients -H 'Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJraWQiOiI2cDFtdFBrVnhUdTlVNngxTk5yaTNPSDVLcnBGVzZRUCJ9.9Qawf8PE8Ygxy Fw0ccgrFza1Uxr8Q_U9z3dlWdzpSYo' {""clients"":[]}  Послесловие  Логично будет поинтересоваться, есть ли готовые решения? Я нашел только Kong от Mashape. Для когото это будет неплохим варинатом, т.к. кроме разных видов авторизации он умеет работать с ACL, управлять нагрузкой применять ACL и много чего еще. В моем случае это была бы стрельба из пушки по воробьям. Кроме того он зависит от БД Casandra, которая, мягко скажем, тажеловата да и довольно чужеродна этому проекту.    P.P.S. Незаметно ""добрые люди"" слили карму. Так что плюсик будет очень кстати и будет хорошей мотивацией к написанию новых статей на тему микросервисов в веб разработке.","nginx, microservices, lua-nginx-module",Аутентифицируем запросы в микросервисном приложении с помощью nginx и JWT
"Если вы еще никогда не поддерживали чужие приложения, или пусть даже свои, но таких размеров, что уже не помещаются в одной голове, то прошу вас расслабиться, откинуться на спинку кресла и воспринимать прочитанное как поучительную сказку с надуманными проблемами, забавным сюжетом и очевидным счастливым концом. В противном случае, если реальный боевой опыт у вас имеется, добро пожаловать в ад, но с IDDQD и IDKFA.    К вашему сведению! В этой статье мы рассматриваем само явление docker-контейнеров, а не составляем список микросервисов, которые гнездятся внутри. Этим мы займемся в следующей серии, во имя справедливости!    UPDATE: пришлось заменить «докер» на «docker», иначе статья не ищется. Заранее прошу прощения за все «docker'ы» в тексте. Селяви.    Что мы имеем сегодня    Зоопарк дубовых VPS-хостингов.  Дорогие IaaS и PaaS с гарантированным vendor lock in.  Уникальные сервера-снежинки.  Ворох устаревших зависимостей на неподдерживаемой операционке.  Скрытые связи частей приложения.  Незаменимый админ полубог на скейтборде.  Радуга окружений: development, testing, integration, staging, production.  Генерация конфигов для системы управления конфигами.  Feature flagging.      Удивительно то, что корень всех этих проблем только в одном: сервер дорого поднимать, а потом еще и дорого гасить. Дорогим может оказаться время потраченное на художественное плетение по конфигам, или энтерпрайз железяка просто долго стартует, или состояние серверов надо хитро бекапить и потом восстанавливать, или всё просто и быстро, но работает только на какой-то уникально дорогущей платформе, с которой не сбежишь.    Конечно, это капитанское вступление. И цель вступления как раз в капитанстве — показать, что эти проблемы уже настолько банальны, что даже простое их перечисление заставляет кое-где болеть.    Если представить, что на холодный запуск приложения на новой машине уйдет минута, и при этом все данные в сохранности, то дышать сразу становится легче, а на лице появляется предвкушение написания полезного кода заместо возни с сервером.    Если ваши уши владеют английским, то прошу сначала послушать восторжженное Введение в docker от основателя.    Назад к основам  Как часто бывает, в какой-то момент состояние некой системы становится неуправляемым и требует новых технологий. В программировании, правда, часто выходит наоборот — требуется вспомнить старые технологии. Вот и в случае с docker'ом не случилось ничего нового: взяли принципы функционального программирования (другие видят ООП) и микроядерности, применили к инфраструктурному слою приложения (серверам, сети и т.п.) и получили stateless-иммутабельные-изолированные-микросервисы. Все прелести docker'а вырастают именно из этого. Как известно, функциональщина не так проста, как хотел бы Рич Хикки. Чтобы можно было каждый день пользоваться мудростью предков, нужно иметь хорошие инструменты. Docker как раз и есть такой инструмент.    Вот базовые отличия docker-контейнера (не микросервиса, см. выше) от простого сервера.    Стейтлес    Конфигурация контейнера не может и не должна меняться после запуска. Все предварительные работы делаются на этапе создания образа или старта контейнера: конфиги, порты, общие папки, переменные окружения, всё это должно быть известно к моменту старта контейнера. Конечно, docker позволяет запущенному внутри контейнера процессу делать со своей памятью и файловой системой всё что тому заблагорассудится, но трогать что-то, что можно было потрогать до запуска считается плохим тоном.    Чистый (pure)    Контейнер ничего не знает о хост системе и не может мешать другим контейнерам: ни влезть в чужую файловую систему, ни послать сигнал чужому процессу, ни даже в случайный порт стукнуться. На то он и контейнер. Само собой, docker позволят контейнерам общаться, но только строго задекларированными способами. Еще можно запускать контейнеры наделенные какими-нибудь суперсилами, например, доступом к реальной сети или физическим девайсам.    Ленивый    При запуске контейнер не копирует файловую систему образа, из которого создан. Контейнер всего-лишь создает пустую файловую систему поверх образа. В docker'е это называется слоем. Так же устроены и образы. Образ состоит из списка (геологических) слоев, накладывающихся друг на друга. Отсюда такая высокая скорость запуска нового контейнера: меньше секунды.    Декларативный    Все свойства контейнера хранятся в декларативном виде. Этапы создания образа тоже описаны строго разделенными шагами. Параметры сети, содержимое файловой системы, объем памяти, публичные порты и так далее и тому подобное задается в Dockerfile или статичными ключами при запуске. Всё это легко читается на паре экранов текста даже для очень сложной системы.    Функциональный    Контейнер делает только одно дело, но делает его хорошо. Предполагается, что в контейнере будет жить всего один процесс (можно с семьёй), выполняющий всего одну функцию в приложении. За счет того, что в контейнере нет своего ядра, загрузочного раздела, init-процесса и, чаще всего, даже пользователь всего один (псевдо-root) — то есть в контейнере нет полноценной операционной системы — за счет отсутствия всего этого контейнер стартует так быстро, как стартовал бы ваш сервис будь операционная система уже полностью загружена. Это узкая специализация делает функцию реализуемую контейнером предсказуемой и масштабируемой. Так как процесс только один, ему неоткуда ждать (если только снаружи) переполнения логов, попадания в своп и тому подобного.    Строгий    По-умолчанию, docker запрещает контейнеру всё, кроме доступа в сеть (которую тоже можно запретить). Однако, при необходимости позволяется нарушать любые эти правила, когда нарушить правила бывает логичнее. И что интересно, разрешать доступ также просто, как и запрещать. Соединять или разъединять контейнеры в docker'е с каждой версией становится всё проще и проще, особенно радует работа с сетью, распределенной по датацентрам.    Чем docker не является  Docker часто путают с Vagrant’ом или OpenVZ, или даже VirtualBox’ом. Docker это всего-лишь user-space демон на языке Go, который умело жонглирует уже существующими технологиями ядра Linux. Думаю, стоит объяснить подробнее, так как сам начал знакомиться с темой с запроса «docker vs. vagrant».    Не вагрант    Вагрант занимается тем, что управляет виртуальными машинами (или, более приближенно к теме хостинга, виртуальными серверами). Так же как и у docker'а, у вагранта есть целая библиотека образов виртуальных машин, и он как и docker умеет их снепшотить, скачивать и заливать, настраивать и запускать. Но повторюсь, Vagrant управляет полноценными тяжелыми виртуальными машинами запущенными, например, в VirtualBox, VMWare, DigitalOcean, AWS, да где только он их не запускает. Шикарная вещь, этот вагрант, если вам нужна полноценная виртуальная машина с состоянием внутри.    Не виртуальная машина    И конечно, docker вовсе не является новой технологией (пара)виртуализации, как например KVM/Qemu, VirtualBox/VMWare/Parallels, XEN, etc. Docker-демон (не путать с консольной командой docker) работает только в Линуксе (и скоро в винде) поверх линуксовых же контейнеров. А контейнеры, в свою очередь, не являются полноценной виртуальной средой, так как они делят общее запущенное ядро одной физической машины. Если вы работали с OpenVZ или щупали Linux Containers то вы должны знать, как они устроены, чем хороши и плохи.    А что же тогда docker?  Docker состоит из трех частей:      docker daemon — сердце docker'а. Это демон работающий на хост-машине и умеющий скачивать и заливать образы, запускать из них контейнеры, следить за запущенными контейнерами, собирать логи и настраивать сеть между контейнерами (а с версии 0.8 и между машинами). А еще именно демон создает образы контейнеров, хоть и может показаться, что это делает docker-client.      docker это консольная утилита для управления docker-демоном по HTTP. Она устроена очень просто и работает предельно быстро. Вопреки заблуждению, управлять демоном docker'а можно откуда угодно, а не только с той же машины. В сборке нового образа консольная утилита docker принимает пассивное участие: архивирует локальную папку в tar.gz и передает по сети docker-daemon, который и делает всё работу. Именно из-за передачи контекста демону по сети лучше собирать тяжелые образы локально.      Docker Hub централизованно хранит образы контейнеров. Когда вы пишете docker run ruby docker скачивает самый свежий образ с руби именно из публичного репозитория. Изначально хаба не было, его добавили уже после очевидного успеха первых двух частей.    Что docker заменил  Для пользы дела стоит перечислить технологии, ставшие лишними при работе с инфраструктурой после появления docker'а. Хочу заметить, что для других применений эти технологии до сих пор незаменимы (вот как, например, заменить SSH или Git используемые по назначению?).    Chef / Puppet / Ansible    Очевидный кандидат на вылет — системы управления конфигурацией stateful сервера. Им на смену приходит лаконичный и быстрый Dockerfile. Так как docker умеет собирать образ контейнера за секунды, а запускать сам контейнер за миллисекунды, больше не надо громоздить скрипты, поддерживающие тяжелый и дорогой сервер в актуальном состоянии. Достаточно запустить новый контейнер и погасить старый. Если вы и до прихода docker'а использовали Chef для быстрого развертывания stateless серверов, то вы уже любите docker, а статью читаете ради удовольствия и сбора аргументов против админа-слоупока, сидящего на FreeBSD 4.11.    Upstart / SystemD / Supervisor / launchd / God.rb / SysVinit, тысячи их    Архаичные системы запуска сервисов, основанные на статичных конфигах идут в сад. На их место приходит docker daemon, который является init-подобным процессом умеющим следить за запущенными сервисами, перезапускать отвалившиеся, хранить коды выхода, логи, а главное, скачивать любые контейнеры с любыми сервисами на любой машине, не важно какая у машины «роль».    Ubuntu_14.04.iso / AMI-W7FIS1T / apt-get update    Так же как и вагрант, docker позволяет, наконец, забыть про скачивание давно устаревших установочных DVD-образов (совсем недавно пришлось качать для Dell блейда), помнить панели управления разных облачных платформ и сокрушаться, что самой свежей убунты у них никогда нет. А главное, запустить свой собственный кастомный образ со всем необходимым можно за пару минут, собрав его на локальной машине и загрузив в бесплатное облако Docker Hub. И никаких больше apt-get update после установки — ваши образы всегда будут готовы к работе сразу после сборки.    RUBY_ENV, database.dev.yml, testing vs. staging vs. backup    Так как docker позволяет запускать контейнеры со всеми зависимостями и окружением бит-в-бит совпадающим с тем, что вы сконфигурировали, можно за пару секунд поднять точную копию продакшна прямо на своём ноутбуке. Все операционные системы, все сервера баз данных и кеширования, само приложение и версии библиотек им используемые, всё это будет точно воспроизведено на девелоперской машине или на машине тестера. Всегда мечтали иметь резервный продакшн сервер готовый к бою в случае падения главного — docker-compose up и у вас два боевых сервера являющихся копией друг друга с точность до бита. Начиная с версии docker'а 1.10 идентификаторы контейнеров являются их же собственной SHA256 подписью гарантирующей идентичность.    Xcode, brew, port install, ./configure && make && sudo make install, mysql --version    Всё, больше никаких мук с установкой серверного софта локально, никаких несовпадений версий мускуля для разных проектов, и возни с удалением всего этого добра после завершения проекта. Это еще ничего, если вы маковод, а вот если у вас виндоус… Docker позволит запускать почти всё, что угодно, лишь бы это работало на линуксе. Можно и Хром в контейнере запускать.    SSH (sic!), VPN, Capistrano, Jenkins-slave    Для того чтобы запустить контейнер не нужно получать root на сервере, морочиться с ключами и настраивать систему деплоя. С docker'ом деплой выглядит как сборка и настройка физического сервера дома, дольшейшего его всестороннего тестирования, а затем магического перемещения в произвольное количество детацентров одной командой. Для этого достаточно запустить docker run на своём ноутбуке и docker сам секьюрно подключится к серверу по TLS и всё запустит. Внутрь контейнера тоже попасть проще простого: docker exec -it %containername% bash и у вас в руках консоль для отладки. Можно даже rsync через docker'овский туннель гонять вот так: rsync -e 'docker exec -i' --blocking-io -rv CONTAINER_NAME:/data ., главное не забыть добавить --blocking-io.    Git (!!!)    Используете Git как способ выкатки приложения на сервер? Отлично! Docker это буквально новый Git, только для контейнеров. Вместо git push (и настройки кучи дополнительного софта и сопряжения с Github’ом) вы получаете docker push. Вместо запуска скрипта деплоя, docker-compose up. И все контейнеры обновятся и перезапустятся. Конечно, если вы хотите zero-downtime деплой, надо будет немного покумекать. Но даже без этого, ваши пользователи увидят MAINTENANCE.HTML не более чем на несколько секунд. Вдобавок, теперь вы можете отправлять тестру не хеш коммита гита и ждать, пока он (или дженкинс) воссоздаст окружение, а просто шлёте ему хеш контейнера и он его сразу запускает.    Datacenter lock-in    Всё, больше его нет. Хотите переезжайте всем приложением за пару минут на другой континент, а хотите — размазывайте своё приложение по всему земному шару. Контейнерам всё равно где крутиться, они объединены виртуальной сетью, видят и слышат друг друга отовсюду, включая девелоперкую машину в случае живой отладки.    bundler, rvm, dotenv, /opt    Нужна старая версия руби для запуска старого скрипта раз в месяц — упакуйте его в docker со старым руби. Нужны разные версии библиотек для разных частей приложения — разбейте по контейнерам и не заморачивайтесь с bundle exec.    *.log, *.pid    И логами, и процессами заведует docker. Есть много драйверов сборки логов (kudos ➞   Fesor) и отправки их на анализ. Можно грабить корованы.    useradd www    Можно забыть детские кошмары о том, что веб-сервер кто-то взломает и получит доступ ко всей системе, если его запустить от рута. А еще не нужно больше искать следы взлома в страхе, что теперь надо всё переустанавливать. Просто обновите софт в образе, убейте контейнер (или всю машину целиком, только базу забекапьте, пожалуйста) и перезапустите сервис.    chroot, CGroups, LXC    Docker объединяет и абстрагирует сложные и разрозненные технологии в своих библиотеках libcontainer, runC, libnetwork. Если вы пробовали настроить LXC, то уже любите docker. Пока эти библиотеки как отдельные проекты еще свежи, но уже используется в самом docker'е.    Вкусности docker'а  Dockerfile    Может показаться, что Dockerfile это старый добрый Chef-конфиг, только на новый лад. А вот и нет: от конфигурации сервера в нем осталась только одна строка — имя базового образа операционной системы. Остальное же — часть архитектуры приложения. И это стоит воспринимать, как декларацию API и зависимостей именно сервиса, а не сервера. Эта часть пишется проектирующим приложение программистом по ходу дела естественным путём прямо в процессе разработки. Такой подход дает не только поразительную гибкость конфигурации, но и позволяет избежать испорченного телефона между разработчиком и админом. Больше про это ждите в статье про микросервисы.    Слоёные имаджи    Имаджи в docker не монолитны, а состоят из copy-on-write слоев. Это позволяет переиспользовать файлы базового readonly имаджа во всех контейнерах бесплатно, запускать контейнер, не копируя файловую систему имаджа, делать контейнеры readonly, а также кешировать разные этапы сборки имаджа. Сильно похоже на коммиты гита, если вы знакомы с его архитектурой.    Плагины и драйверы      Fesor:    Для него есть и драйвера логгеров, и плагины для волумов и сетей… словом… и это все еще будет развиваться но и уже немало.  Самое приятное, что никакой магии в плагинах нет. Простые, как выражаются разработчики докера, «водопроводные работы».    docker-daemon — это асинхронный HTTP-сервер    Да, ребята решили не играть в параллельность, а замутить Single Actor архитектуру через асинхронность, как в nginx. Только нам на радость выбрали язык попроще и запилили docker на Go, так что не нужно становиться Игорем Сысоевым, чтобы читать исходники docker'а.    docker in docker    Можно разрешать одним контейнерам управлять другими контейнерами. Таким образом получается, что на хост машине не будет установлено ничего, кроме docker'а. Делается это так:     docker run -v /var/run/docker.sock:/var/run/docker.sock \        -v $(which docker):/bin/docker \        -ti ubuntu  Но это не настоящий иерархичный docker-in-docker, а плоский зато стабильный вариант.    VXLAN    В последних версиях docker'а появилась поддержка распределенной сети. В статье это не раз упоминается, и не зря. С самого начала у docker'а была проблема с объединением нескольких машин в единый кластер, или даже нескольких контейнеров в группу. Предлагались кривые решения с проксированием, настройкой хост машины, и прочие некошерные вещи. Теперь же docker умеет сам настраивать на хост машине VXLAN, который объединяет несколько физических машин и docker'ов на них в единую виртуальную IP сеть. В такой сети контейнеры будут видеть друг друга как если бы они работали в одной сети. Само собой, такой кластер упирается в ширину сети между машинами, но и это уже прорыв для рядового девопса. А если еще и NAS запилить, то уже взрослый многофункциональный кластер получается.    Куда дальше?  На передовом крае запуск распределенных кластеров, docker как операционная система, docker-in-docker-in-docker. И, как любая открытая платформа, docker разделяется на составляющие части развиваемые отдельными группами, что очень ободряет коллектив.    Swarm? Machine! Compose!!!    Кроме самого docker'а, есть еще несколько интереснейших проектов внутри сообщества docker'а, которые отлично дополняют контейнеры: Machine, Compose и Swarm. О них — в следующих статьях.    Робота над ашипками  Благодарю покорно за холивар в комментах. Чтобы такое добро не пропадало, постараюсь раскрывать тему глубжее, отвечая прямо в статье.     VasilioRuzanni:    А аргументы Phusion в пользу использования baseimage-docker еще актуальны?  Вот проблемы, которые описаны в статье по ссылке:      Ubuntu не спроектирована для работы в контейнере.    Базовый образ убунты в docker'е это ее облачный вариант с правильно выключенным init и затюненным dpkg. Получается, что таки допроектировали. Для старых добрых OpenVZ контейнеров убунту тоже приходилось допиливать, и ничего, жили и живем годами в продакшне.      Нет init — будут зомби.    Да, зомби всё еще копятся. Если запустить fork-бомбу, то через 10 секунд на хост-машину с docker'ом уже не папость по SSH: can't fork. К чести docker'а, запуск docker ps покажет все запущенные контейнеры, а docker kill погасит бомбу и порипает всех зомби.    Однако, как часто бывает, если хотеть что-то сломать, то что-то таки сломается. Если в вашем приложении воркеры действительно часто внезапно мрут, ожидая завершения дочернего процесса, то у меня для вас плохие новости. Если же вы в таком поведении воркеров не видите ничего плохого, то дались вам эти зомби? Если же частые разрывы в иерархии процессов у вас by design, то да, стоит посмотреть в сторону, например, tini.      Syslog?    Так как docker предполагает, что микросервис шлет свои логи в STDOUT и STDERR, syslog не нужен. Docker сам управляет логами, и позволяет их сразу слать на анализ без сохранения. Если хотите хранить, то, во-первых, это не по облачному феншую, а во-вторых, логи таки могут занять весь диск.      Cron?    Что простите? А дженкинс нам на что? Как можно юзать крон в 2016 году? Однако, иногда очень-очень хочется, и, стоит надеяться, что docker инкапсулирует запуск сервисов по времени в самого себя. Но и сейчас никто не мешает запустить контейнер с произвольной реализацией крона и запускать контейнеры из него.      SSH?    По сути, консольная утилита docker уже и есть SSH. Она связывается с демоном по зашифрованному соединению, аутентифицируется по ключу, запускает шел и эмулирует терминал. Есть даже аналог scp.      Неправда, что docker рассчитан на запуск только одного процесса на контейнер!    Неправда, что неправда!      А давайте сделаем из непривычного docker'а привычный OpenVZ.    А давайте без давайте.          powerman:    Безопасность подразумевает необходимость ежедневных обновлений пакетов ОС…  Приличный тред получился.    Во-первых, если у вас живое приложение, то не будет никаких проблем с обновлением любого софта, и не только «пакетов ОС», а еще и версий фреймворков, баз данных, хитрожёлтых самопальных зависимостей, и еще чего угодно. Взяли — и пересобрали.    А во-вторых, работу с «пакетами ОС», если они на самом деле используются в каком-то вашем микросервисе, надо при обновлении тестировать. А готовить новый релиз к тестированию в мире docker'а принято с помощью — подожди-подожди — пересборки.    И главное, спросите любого владельца маломальски прибыльного сайта, что ему важнее: hardened gcc (с PIE и SSP) и потенцально невзламываемая до следующего shellshock интернет-шаурмятня или мгновенный выкат новой версии приложения без «этого тупого бага с оплатой», уносящего по 100500 денег в час?    Можно взглянуть на безопасность из будущей статьи про микросервисы. Современный сервер состоит из приложения и системы. Обе части надо защищать и обновлять. В предельном случае, к которому стремится сообщество docker'а, в контейнере не будет системы, а только приложение. И это приложение должно делать только одну функцию, например CQRS из базы данных. Тогда паника админов по поводу плохого базового образа убунты бесследно проходит, так как убунты в контейнере больше нет.      grossws:    Оверлейный vxlan, про который пишет автор публикации в предпоследнем абзаце, — без шифрования. Т. е. если использовать --link между контейнерами, то весь трафик внутри должен быть шифрованным и подписанным. Много ли людей будут это делать, а не воспринимать --link, как секьюрный оверлей? Я, например, сейчас использую tinc (p2p vpn), как паллиативное решение.  Прошу всех читателей не воспринимать --link, как секьюрный оверлей, и сам не буду. Если ваше приложение перезрело приватное облако в одном датацентре и дозрело до распределенного по глобусу монстра, пожалуйста, настройте шифрование.    @админы:    Docker админам не нужен, ведь уже же есть «правильная инфраструктура»™. Пусть всё остается как есть, а кодеры учатся кодить под «правильную инфраструктуру»™.  Рекомендую почитать «Механическое пианино» Воннегута. За известным исключением, все профессии, однажды, теряют свою маржу.      Scf:    Если вам нужны логи в кибане не в виде текста, а в виде структурированных данных, если у вас ненадежная сеть (привет, амазон), если вас волнуют потенциальные даунтаймы и узкие места в системе, если вы не можете себе позволить потерю логов из-за сетевых проблем, то…  Мы долго думали об этом, но варианта лучше и гибче, чем писать логи в каталог, смапленный наружу, где они подхватываются логсташем, который парсит их в зависимости от их типа и шлет в кибану json-документы, не нашли.  Да, с логами надо быть осторожными. Если лог напрямую слать по сети, то можно залочить процесс, пишущий лог. Полностью поддерживаю описанное решение, особенно в том случае, если логсташ тоже крутится в контейнере ;)      Fesor:    …есть множество кейсов когда это неудобно (shared memory например надо организовать между процессами)  Shared memory подразумевает выполнение процессов на одной машине. Если эти два процесса запущены на одном ядре, как в случае docker'а, то им достаточно иметь доступ к одному и тому же файлу (один inode в файловой системе), чтобы расшарить память. Рецепт для docker'а очень прост: делаете mmap файлу из общего volume'а и всё.    Полезно помнить, что docker всего лишь скрывает процессы друг от друга, но от этого они не перестают быть обычными юниксовыми процессами в обычной юниксовой среде.      JPEG:    Да, последний шаг осталось сделать докеру до гита: мердж имаджей с резолвом --theirs. Я серьёзно :)  Мержа нет потому, что рисковано мержить бинари, зависящие друг от друга. А ребейза нет потому, что нельзя проверить, не зависят ли верхние слои от нижних. В общем, полного гита не получится.          	 		  	  		Что для вас докер?  		 		  		 			 				 					 				   				 					Боевой друг 				 			 			 				 					 				   				 					Игрушка для ламеров 				 			 			 				 					 				   				 					Chef rules forever!!!11 				 			 			 				 					 				   				 					Интригующая новинка 				 			 			 				 					 				   				 					Я искал jQuery-плагин, отпустите меня домой… 				 			 		 		 		Проголосовало 1135 человек. Воздержалось 398 человек. 	      Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.","docker, chef, puppet, upstart, systemd, rbenv, dotenv, vpn, vxlan",Поняв Docker
"Привет Хабр! Я опубликовал уже три части из цикла статей (раз, два, три), а тут часть 0, как снег на голову. Как же так? Всё дело в том, что виртуализация является опциональной при построении нашего хостинга. Эта статья — самодостаточна, она не связана с другими частями из цикла. Вы вообще можете их не читать, если просто хотите разделить ваш выделенный сервер на несколько виртуальных машин.    Всё что я буду рассказывать может выполнить обычный программист в течение 5 минут, просто запустив набор сценариев для Ansible, которые я подготовил специально для вас и выложил на GitHub.    Содержание    Часть 0: Виртуализация  Часть 1: Ansible, Docker, Docker Swarm  Часть 2: Service Discovery  Часть 3: Consul, Registrator, Consul-Template  ...    Подготовка  Скачиваем набор сценариев или клонируем репозиторий:    » git clone https://github.com/vladkozlovski/ansible-virtualization » git checkout v1.x » cd ansible-virtualization   На этом подготовительные работы можно считать законченными.     Ах да, чуть не забыл, вам понадобится как минимум один выделенный сервер с авторизацией по ключу.     Конфигурация  Конфигурацию мы будем выполнять на примере Hetzner. Все конфигурационные файлы, которые мы будем редактировать, находятся в директории host_vars:      dc16-host1-vm1.yml – конфигурация виртуальной машины №1  dc16-host1-vm2.yml – конфигурация виртуальной машины №2  ..  dc16-host1.yml – конфигурация хостовой машины    В нашем примере мы создаём 6 виртуальных машин, ровно столько одиночных IP адресов на один сервер может выдать Hetzner. Давайте разберём, что тут у нас в конфигурационных файлах:    dc16-host1.yml  Конфигурация хостовой машины:    # Host 1  ansible_ssh_host: 5.9.45.106  # IPv4 адрес хостовой машины ansible_ssh_user: root  # Пользователь хостовой машины  # net vm_bridge: virbr0  ipv4: true ipv4_address: 5.9.45.106/27  # IPv4 адрес и маска хостовой машины ipv4_gateway: 5.9.45.97  # IPv4 шлюз хостовой машины ipv4_dns: 213.133.100.100 213.133.98.98 213.133.99.99  # Hetzner IPv4 DNS  ipv6: true ipv6_address: 2a01:4f8:163:326a::2  # IPv6 адрес хостовой машины ipv6_mask: 64 ipv6_gateway: fe80::1  # IPv6 шлюз хостовой машины ipv6_dns: 2a01:4f8:0:a0a1::add:1010 2a01:4f8:0:a102::add:9999 2a01:4f8:0:a111::add:9898  # Hetzner IPv6 DNS  # apt apt_host: ftp.de.debian.org   Hetzner присылает IPv4 и IPv6 адреса в письме при заказе сервера. Остальные значения переменных вы можете глянуть в личном кабинете. IPv4 и IPv6 адреса DNS-серверов я взял в wiki Hetzner'а.    dc16-host1-vm1.yml  Конфигурация виртуальной машины №1:    # Debian 1  # kvm-host ansible_ssh_host: 5.9.45.106  # IP адрес хостовой (не гостевой) машины ansible_ssh_user: root  # Пользователь хостовой (не гостевой) машины  # vnc (port: 5900) vnc_password: ""kBz4Yp3UyVEPMr""  # Пароль для подключения к VNC серверу  # vm vm_num: 1  # uniq 0-15 vm_name: debian1  # Уникальное название виртуальной машины vm_hdd_size: 10G  # Диск 10 гигабайт vm_memory: 2048  # Память в мегабайтах vm_swap_size: 2048  # Размер файла подкачки в мегабайтах vm_cpu: 2  # Количество ядер vm_bridge: virbr0 vm_root_password: ""3yMAqs3yTcuKvZ""  # Пароль для root пользователя виртуальной машины  # net vm_ipv4: true vm_ipv4_address: 5.9.244.210  # IPv4 адрес гостевой машины vm_ipv4_mask: 29 vm_ipv4_gateway: 5.9.244.209  # IPv4 шлюз гостевой машины vm_ipv4_dns: 213.133.98.98 213.133.99.99 213.133.100.100  # Hetzner IPv4 DNS  vm_ipv6: true vm_ipv6_address: 2a01:4f8:163:326a::d1  # IPv6 адрес гостевой машины vm_ipv6_mask: 64 vm_ipv6_gateway: fe80::1  # IPv6 шлюз гостевой машины vm_ipv6_dns: 2a01:4f8:0:a0a1::add:1010 2a01:4f8:0:a102::add:9999 2a01:4f8:0:a111::add:9898  # Hetzner IPv6 DNS  vm_mac: 00:52:54:56:88:88   Значения переменных vm_ipv4_address, vm_ipv4_mask и vm_ipv4_gateway Hetzner присылает при заказе дополнительного IP-адреса. IPv4 и IPv6 адреса DNS-серверов такие же, как и у хостовой машины. Дополнительный IPv4 адрес вы можете заказать в личном кабинете. Hetzner просит указывать цель, для которой вам нужен дополнительный адрес, я пишу туда одно слово – «Virtualization».    По поводу IPv6: каждый сервер получает подсеть /64. Соответственно вы можете взять любые адреса из неё. Например для 2a01:4f8:163:326a:: / 64:      2a01:4f8:163:326a::d1  2a01:4f8:163:326a::d2  ...  2a01:4f8:163:326a::d6    Что бы указать значение переменной vm_mac, вам необходимо получить отдельный MAC-адрес для указанного IP. Это можно сделать в личном кабинете.    На этом всё, можно приступать к запуску.    Запуск  Запуск производится двумя командами. Первая устанавливает необходимые пакеты и конфигурирует хостовую машину:    $ ansible-playbook -i prod kvm.yml   Вторая команда создаёт, конфигурирует и запускает виртуальные машины:    $ ansible-playbook -i prod guests.yml   После запуска и выполнения этих двух команд ваши виртуальные машины должны быть запущены и доступны снаружи. Ваш публичный ключ был скопирован на все виртуалки, поэтому авторизация будет происходить по ключу.    Итого  Мы используем этот набор сценариев довольно долго и всё отрабатывает как положено. Это значительно упростило жизнь и позволило быстро добавлять новые вычислительные ресурсы в наше скромное облако. Если у вас возникнут какие-либо вопросы – добро пожаловать в комментарии.    На этом всё. Всем спасибо за внимание. Стабильных вам облаков и удачи!    Подписывайтесь на меня в Twitter, я рассказываю о работе в стартапе, своих ошибках и правильных решениях, о python и всём, что касается веб-разработки.     P.S. Я ищу разработчиков в стартап, подробности у меня в профиле.","ansible, cloud hosting, hosting, qemu, qemu-kvm, qemu/kvm",Свой облачный хостинг за 5 минут. Часть 0: Виртуализация
"Пара часов из жизни математика-программиста или читаем википедию    Для начала в качестве эпиграфа цитирую  rocknrollnerd:  — Здравствуйте, меня зовут %username%, и втайне раскрываю суммы из сигма-нотации на листочке, чтобы понять, что там происходит.  — Привет, %username%!      Итак, как я и говорил в своей прошлой статье, у меня есть студенты, которые панически боятся математики, но в качестве хобби ковыряются паяльником и сейчас хотят собрать тележку-сигвей. Собрать-то собрали, а вот держать равновесие она не хочет. Они думали использовать ПИД-регулятор, да вот только не сумели подобрать коэффициенты, чтобы оно хорошо работало. Пришли ко мне за советом. А я ни бум-бум вообще в теории управления, никогда и близко не подходил. Но зато когда-то на хабре я видел статью, которая говорила про то, что линейно-квадратичный регулятор помог автору, а пид не помог.    Если ПИД я ещё себе худо-бедно на пальцах представляю (вот моя статья, которую с какого-то перепугу перенесли на гиктаймс), то про другие способы управления я даже и не слышал толком. Итак, моя задача — это представить себе (и объяснить студентам, а заодно и вам), что такое линейно-квадратичный регулятор. Пока что работы с железом не будет, я просто покажу, как я работаю с литературой, ведь именно это и составляет львиную долю моей работы.    Раз уж пошёл эксгибиционизм про мою работу, то вот вам моё рабочее место (кликабельно):          Используемые источники    Итак, первое, что я делаю, иду в википедию. Русская википедия жестока и беспощадна, поэтому читаем только английский текст. Он тоже отвратителен, но всё же не настолько. Итак, читаем. Из всех вводных кирпичей текста только одна фраза меня заинтересовала:  The theory of optimal control is concerned with operating a dynamic system at minimum cost. The case where the system dynamics are described by a set of linear differential equations and the cost is described by a quadratic function is called the LQ problem.     В переводе на русский они говорят, что моделируют некую динамическую систему дифференциальными уравнениями (о ужас), и выбирают непосредственно управление, минимизируя какую-то квадратичную функцию (о! я чувствую приближение наименьших квадратов). Так, хорошо. Пытаемся читать дальше:     Finite-horizon, continuous-time LQR,  [серия ужасных формул]    Пропускаем, ну его в болото такое читать, кроме того, явно будет противно руками считать непрерывные функции.    Infinite-horizon, continuous-time LQR,  [серия ещё более ужасных формул]    Час от часу не легче, ещё и несобственные интегралы пошли, пропускаем, вдруг дальше что интересного найдём.    Finite-horizon, discrete-time LQR    О, это я люблю. У нас дискретная система, смотрим её состояние через некие промежутки (некие промежутки в первом прочтении всегда равны одной секунде) времени. Производные из уравнения ушли, т.к. они теперь могут быть приближены как (x_{k+1}-x_{l})/1 секунду. Теперь неплохо было бы понять, что такое x_k.    Одномерный пример    Фейнман писал, как именно он читал все уравнения, что ему приходилось:  Actually, there was a certain amount of genuine quality to my guesses. I had a scheme, which I still use today when somebody is explaining  something that I'm trying to understand: I keep making up examples. For instance, the mathematicians would come in with a terrific theorem, and they're all excited. As they're telling me the conditions of the theorem, I construct something which fits all the conditions. You know, you have a set (one ball) — disjoint (two balls). Then the balls turn colors, grow hairs, or whatever, in my head as they put more conditions on. Finally they state the theorem, which is some dumb thing about the ball which isn't true for my hairy green ball thing, so I say, «False!»    Ну а мы что, круче Фейнмана? Лично я нет. Поэтому давайте так. Имеется автомобиль, едущий с некой начальной скоростью. Задача его разогнать до некой финальной скорости, при этом единственное, на что мы можем влиять, это на педаль газа, сиречь на ускорение автомобиля.    Давайте представим, автомобиль идеален и движется по такому закону:      x_k — это скорость автомобиля в секунду k, u_k — это положение педали газа, которое мы захотим, можно её интерпретировать как ускорение в секунду k. Итого, мы стартуем с некой скорости x_0, и затем проводим вот такое численное интегрирование (во какие слова пошли). Отметьте, что я не складываю м/с и м/с^2, u_k умножается на одну секунду интервала между измерениями. По умолчанию у меня все коэффициенты либо ноль, либо один.    Итак, чтобы понять, что происходит, я пишу вот такой код (я пишу очень много одноразового кода, который выкидываю сразу после написания). Привожу листинг здесь на всякий случай, как обычно, я использую OpenNL для решения больших разреженных линейных систем уравнений.    Скрытый текст#include <iostream> #include ""OpenNL_psm.h""  int main() {     const int N = 60;     const double xN = 2.3;     const double x0 = .5;     const double hard_penalty = 100.;      nlNewContext();     nlSolverParameteri(NL_NB_VARIABLES, N*2);     nlSolverParameteri(NL_LEAST_SQUARES, NL_TRUE);     nlBegin(NL_SYSTEM);     nlBegin(NL_MATRIX);      nlBegin(NL_ROW); // x0 = x0     nlCoefficient(0, 1);     nlRightHandSide(x0);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW); // xN = xN     nlCoefficient((N-1)*2, 1);     nlRightHandSide(xN);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW); // uN = 0, for convenience, normally uN is not defined     nlCoefficient((N-1)*2+1, 1);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      for (int i=0; i<N-1; i++) {         nlBegin(NL_ROW); // x{i+1} = xi + ui         nlCoefficient((i+1)*2  , -1);         nlCoefficient((i  )*2  ,  1);         nlCoefficient((i  )*2+1,  1);         nlScaleRow(hard_penalty);         nlEnd(NL_ROW);     }      for (int i=0; i<N; i++) {         nlBegin(NL_ROW); // xi = xN, soft         nlCoefficient(i*2, 1);         nlRightHandSide(xN);         nlEnd(NL_ROW);     }      nlEnd(NL_MATRIX);     nlEnd(NL_SYSTEM);     nlSolve();      for (int i=0; i<N; i++) {         std::cout << nlGetVariable(i*2) << "" "" <<  nlGetVariable(i*2+1) << std::endl;     }      nlDeleteContext(nlGetCurrent());     return 0; }     Итак, давайте разбираться, что я делаю. Для начала я говорю N=60, на всё отвожу 60 секунд. Затем говорю, что финальная скорость должна быть 2.3 метра в секунду, а начальная полметра в секунду, это выставлено от балды. Переменных у меня будет 60*2 — 60 значений скорости и 60 значений ускорения (строго говоря, ускорения должно быть 59, но мне проще сказать, что их 60, а последнее должно быть равно строго нулю).    Итого, у меня 2*N переменных (N=60), чётные переменные (я начинаю считать с нуля, как и всякий нормальный программист) — это скорость, а нечётные — это ускорение. Задаю начальную и конечную скорость вот этими строчками:  По факту, я сказал, что хочу, чтобы начальная скорость была равна x0 (.5m/s), а конечная — xN (2.3m/s).      nlBegin(NL_ROW); // x0 = x0     nlCoefficient(0, 1);     nlRightHandSide(x0);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW); // xN = xN     nlCoefficient((N-1)*2, 1);     nlRightHandSide(xN);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);     Мы знаем, что x{i+1} = xi + ui, поэтому давайте добавим N-1 такое уравнение в нашу систему:        for (int i=0; i<N-1; i++) {         nlBegin(NL_ROW); // x{i+1} = xi + ui         nlCoefficient((i+1)*2  , -1);         nlCoefficient((i  )*2  ,  1);         nlCoefficient((i  )*2+1,  1);         nlScaleRow(hard_penalty);         nlEnd(NL_ROW);     }     Итак, мы добавили всё жёсткие ограничения в систему, а что именно, собственно, мы можем хотеть оптимизировать? Давайте просто для начала скажем, что мы хотим, чтобы x_i как можно скорее достиг финальной скорости xN:        for (int i=0; i<N; i++) {         nlBegin(NL_ROW); // xi = xN, soft         nlCoefficient(i*2, 1);         nlRightHandSide(xN);         nlEnd(NL_ROW);     }     Ну вот наша система готова, жёсткие правила перехода между состояниями заданы, квадратичная функция качества системы тоже, давайте потрясём коробочку и посмотрим, что наименьшие квадраты нам дадут в качестве решения x_i, u_i (красная линия — это скорость, зелёная — ускорение):        Ооокей, мы действительно ускорились с полуметра в секунду до двух метров запятая трёх в секунду, но уж больно большое ускорение наша система выдала (что логично, т.к. мы потребовали сойтись как можно скорее).    А давайте изменим (вот коммит) целевую функцию, вместо наискорейшей сходимости попросим как можно меньшее ускорение:        for (int i=0; i<N; i++) {         nlBegin(NL_ROW); // ui = 0, soft         nlCoefficient(i*2+1, 1);         nlEnd(NL_ROW);     }         Мда, теперь машина ускоряется на два метра в секунду за целую минуту. Ок, давайте и быструю сходимость к финальной скорости, и маленькое ускорение попробуем (вот коммит)?        for (int i=0; i<N; i++) {         nlBegin(NL_ROW); // ui = 0, soft         nlCoefficient(i*2+1, 1);         nlEnd(NL_ROW);          nlBegin(NL_ROW); // xi = xN, soft         nlCoefficient(i*2, 1);         nlRightHandSide(xN);         nlEnd(NL_ROW);     }     Ага, супер, теперь становится красиво:        Итак, быстрая сходимость и ограничение величины управления — это конкурирующие цели, понятно. Обратите внимание, на данный момент я остановился на первой строчке параграфа википедии, дальше идут страшные формулы, я их не понимаю. Зачастую чтение статей сводится к поиску ключевых слов и полному выводу всех результатов, используя тот матаппарат, которым лично я владею.     Итак, что мы имеем на данный момент? То, что имея начальное состояние системы + имея конечное состояние системы + количество секунд, мы можем найти идеальное управление u_i. Это хорошо, только к сигвею слабо применимо. Чёрт, как же они делают-то? Так, читаем следующий текст в википедии:    with a performance index defined as        the optimal control sequence minimizing the performance index is given by        where [ААА, ЧТО ТАМ ДАЛЬШЕ ТАКОЕ?!]      Так. Я не понимаю что после знака равно в формуле «J = ...», но это явно квадратичная функция, что мы попробовали. Типа, скорейшей сходимости к цели плюс наименьшими затратами, это мы позже посмотрим ближе, сейчас мне достаточно моего понимания.     u_k = -F x_k. Оп-па. Они говорят, что для нашего одномерного примера в любой момент времени оптимальное ускорение — это некая константа -F, помноженная на текущую скорость? Ну-ка, ну-ка. А ведь и правда, зелёный и красный графики подозрительно друг на друга похожи!    Давайте-ка попробуем написать настоящие уравнения для нашего 1D примера.    Итак, у нас есть функция качества управления:        Мы хотим её минимизировать, при этом соблюдая ограничения на связь между соседними значениями скорости нашей машины:        Стоп-стоп-стоп, а какого хрена в википедии стоит x_k^T Q x_k? Ведь это же простой x_k^2 в нашем случае, а у нас (x_k-x_N)^2?! Ёлки, да ведь они предполагают, что финальное состояние, в которое мы хотим попасть, это нулевой вектор!!! КАКОГО [CENSORED] ОБ ЭТОМ НА ВСЕЙ СТРАНИЦЕ ВИКИПЕДИИ НИ СЛОВА?!     Окей, дышим глубоко, успокаиваемся. Теперь я все x_i в формулировке J выражу через u_i, чтобы не иметь ограничений. Теперь у меня переменными будет являться только вектор управления. Итак, мы хотим минимизировать функцию J, которая записывается вот так:        Зашибись. А теперь давайте раскрывать скобки (см. эпиграф):        Многоточие здесь означает всякую муть, которая от u_0 не зависит. Так как мы ищем минимум, то чтобы его найти, нужно приравнять нулю все частные производные, но для начала меня интересует частная производная по u_0:        Итого мы получаем, что оптимальное управление будет оптимально только тогда, когда u_0 имеет вот такое выражение:        Обратите внимание, что в это выражение входят и другие неизвестные u_i, но есть одно «но». Прикол в том, что я не хочу, чтобы машина ускорялась всю минуту на всего два метра в секунду. Минуту я ей дал просто как заведомо достаточное время. А могу и час дать. Член, зависящий от u_i, если грубо, то это вся работа по ускорению, от N она не зависит. Поэтому если N достаточно большой, то оптимальный u_0 линейно зависит только от того, насколько x0 далёк от конечного положения!    То есть, управление должно выглядеть следующим образом: мы моделируем систему, находим магический коэффициент, линейно связывающий u_i и x_i, записываем его, а затем в нашем роботе просто делаем линейный пропорциональный регулятор, используя найденный магический коэффициент!      Если быть предельно честным, то до вот этого момента я кода, конечно, не писал, всё вышеозначенное я проделал в уме и чуть-чуть на бумажке. Но это не отменяет факта, что одноразового кода я пишу действительно много.    2D пример    В качестве интуиции это прекрасно, а вот первый код, который я действительно написал:  Скрытый текст#include <iostream> #include <vector> #include ""OpenNL_psm.h""  int main() {     const int N = 60;     const double x0 = 3.1;     const double v0 = .5;     const double hard_penalty = 100.;     const double rho = 16.;      nlNewContext();     nlSolverParameteri(NL_NB_VARIABLES, N*3);     nlSolverParameteri(NL_LEAST_SQUARES, NL_TRUE);     nlBegin(NL_SYSTEM);     nlBegin(NL_MATRIX);      nlBegin(NL_ROW);     nlCoefficient(0, 1); // x0 = 3.1     nlRightHandSide(x0);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW);     nlCoefficient(1, 1); // v0 = .5     nlRightHandSide(v0);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW);     nlCoefficient((N-1)*3, 1); // xN = 0     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW);     nlCoefficient((N-1)*3+1, 1); // vN = 0     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      nlBegin(NL_ROW); // uN = 0, for convenience, normally uN is not defined     nlCoefficient((N-1)*3+2, 1);     nlScaleRow(hard_penalty);     nlEnd(NL_ROW);      for (int i=0; i<N-1; i++) {         nlBegin(NL_ROW); // x{N+1} = xN + vN         nlCoefficient((i+1)*3  , -1);         nlCoefficient((i  )*3  ,  1);         nlCoefficient((i  )*3+1,  1);         nlScaleRow(hard_penalty);         nlEnd(NL_ROW);          nlBegin(NL_ROW); // v{N+1} = vN + uN         nlCoefficient((i+1)*3+1, -1);         nlCoefficient((i  )*3+1,  1);         nlCoefficient((i  )*3+2,  1);         nlScaleRow(hard_penalty);         nlEnd(NL_ROW);     }      for (int i=0; i<N; i++) {         nlBegin(NL_ROW); // xi = 0, soft         nlCoefficient(i*3, 1);         nlEnd(NL_ROW);          nlBegin(NL_ROW); // vi = 0, soft         nlCoefficient(i*3+1, 1);         nlEnd(NL_ROW);          nlBegin(NL_ROW); // ui = 0, soft         nlCoefficient(i*3+2, 1);         nlScaleRow(rho);         nlEnd(NL_ROW);     }      nlEnd(NL_MATRIX);     nlEnd(NL_SYSTEM);     nlSolve();      std::vector<double> solution;     for (int i=0; i<3*N; i++) {         solution.push_back(nlGetVariable(i));     }      nlDeleteContext(nlGetCurrent());      for (int i=0; i<N; i++) {         for (int j=0; j<3; j++) {             std::cout << solution[i*3+j] << "" "";         }         std::cout << std::endl;     }      return 0; }       Здесь всё то же самое, только переменных системы теперь две: не только скорость, но и координата машины.  Итак, дана начальная позиция + начальная скорость (x0, v0), мне нужно достичь конечной позиции (0,0), остановиться в начале координат.  Переход из одного момента в следующий осуществляется как x_{k+1} = x_k + v_k, а v_{k+1} = v_k + u_k.    Я достаточно прокомментировал предыдущий код, этот не должен вызвать трудностей. Вот результат работы:        Красная линия — координата, зелёная — скорость, а синяя — это непосредственно положение «педали газа». Исходя из предыдущего, предполагается, что синяя кривая — это взвешенная сумма красной и зелёной. Хм, так ли это? Давайте попробуем посчитать!  То есть, теоретически, нам нужно найти два числа a и b, такие, что u_i = a*x_i + b*v_i. А это есть не что иное, как линейная регрессия, что мы делали в прошлой статье! Вот код.    В нём я сначала считаю кривые, что на картинке выше, а затем ищу такие a и b, что синяя кривая = a*красная + b*зелёная.    Вот разница между настоящими u_i и теми, что я получил, складывая зелёную и красную прямую:      Отклонение порядка одной сотой метра за секунду за секунду! Круто!!! Тот коммит, что я привёл, даёт a=-0.0513868, b=-0.347324. Отклонение действительно небольшое, но ведь это нормально, начальные данные-то я не менял.    А теперь давайте кардинально изменим начальное положение и скорость машины, оставив магические числа a и b из предыдущих вычислений.    Вот разница между настоящим оптимальным решением и тем, что мы получаем наитупейшей процедурой, давайте-ка я её ещё раз приведу полностью:      double xi = x0;     double vi = v0;     for (int i=0; i<N; i++) {         double ui = xi*a + vi*b;         xi = xi + vi;         vi = vi + ui;         std::cout << (ui-solution[i*3+2]) << std::endl;     }         И никаких дифференциальных уравнений Риккати, которые нам предлагала википедия. Возможно, что и про них мне придётся почитать, но это будет потом, когда припечёт. А пока что простые квадратичные функции меня устраивают более чем полностью.    Сухой остаток    Итого: чтобы это использовать, будут две основных трудности:  а) найти хорошие матрицы перехода A и B, это нужно будет записывать кинематические уравнения, так как, конечно, там всё будет зависеть от масс объектов и прочего  б) найти хорошие коэффициенты компромисса между целями наискорейшего схождения к цели, но при этом ещё и не делая сверхусилий.    Если а и б сделать, то метод выглядит многообещающим. Единственное, что он требует полного знания состояния системы, что не всегда получается. Например, положение сигвея мы не знаем, можем только догадываться о нём, исходя из данных датчиков типа гироскопа и акселерометра. Ну да это про это я расскажу своим студентам в следующий раз.    Итак, я хотел добиться трёх целей:   а) понять, что такое lqr  б) объяснить то, что понял, студентам и вам  в) показать вам и моим студентам, что я (как и большинство людей) ни хрена не понимаю в математических текстах, что печально, но совсем не катастрофично. Ищем ключевые слова, за которые зацепиться, выкидываем излишние, ненужные нам методы и абстракции, и пытаемся это впихнуть в рамки наших текущих знаний.    Надеюсь, мне удалось. Ещё раз, я совсем не специалист в теории управления, я её в глаза не видел, если у вас есть что дополнить и поправить, не стесняйтесь.    Enjoy!","математика на пальцах, наименьшие квадраты, математика для программистов",Математика на пальцах: линейно-квадратичный регулятор
"В этой небольшой статье речь пойдет о том, можно ли легко использовать Python для написания скриптов вместо Bash/Sh. Первый вопрос, который возникнет у читателя, пожалуй, а почему, собственно, не использовать Bash/Sh, которые специально были для этого созданы? Созданы они были достаточно давно и, на мой взгляд, имеют достаточно специфичный синтаксис, не сильно похожий на остальные языки, который достаточно сложно запомнить, если вы не администратор 50+ левела. Помните, ли вы навскидку как написать на нем простой if?    if [ $# -ne ""$ARGCOUNT"" ] then     echo ""Usage: `basename $0` filename""     exit $E_WRONGARGS fi  Элементарно правда? Интуитивно понятный синтаксис. :)    Тем не менее в python эти конструкции намного проще. Каждый раз когда я пишу что то на баше, то непременно лезу в поисковик чтобы вспомнить как писать простой if, switch или что-то еще. Присвоение я уже запомнил. :) В Python все иначе. Я хоть и не пишу на нем круглые сутки, но никогда не приходилось лезть и смотреть как там сделать простой цикл, потому что синтаксис языка простой и интуитивный. Плюс ко всему он намного ближе к остальным мейнстримовым языкам типа java или c++, чем Bash/Sh.    Также в стандартной и прочих библиотеках Python есть намного более удобные библиотеки чем консольные утилиты. Скажем, вы хотите распарсить json, xml, yaml. Знаете какой я недавно видел код в баше чтобы сделать это? Правильно:    python -c ""import json; json.loads..."" :)  И это был не мой код. Это был код баше/питоно нейтрального человека.    То же самое с регексом, sed бесспорно удобная утилита, но как много людей помнит как правильно ее использовать? Ну кроме Lee E. McMahon, который ее создал. Да впринципе многие помнят, даже я помню как делать простые вещи. Но, на мой взгляд, в Python модуль re намного удобнее.    В этой небольшой статье я хотел бы представить вам диалект Python который называется shellpy и служит для того, чтобы насколько это возможно заменить Bash на python в скриптах.     Велкам под кат.    Введение  Shell python ничем не отличается от простого Python кроме одной детали. Выражения внутри grave accent символов ( ` ) в отличие от Python не является eval, а обозначает выполнение команды в шелле. Например    `ls -l`  выполнит ls -l как shell команду. Также возможно написать все это без ` в конце строки    `ls -l  и это тоже будет корректным синтаксисом.     Можно выполнять сразу несколько команд на разных строках    ` echo test > test.txt cat test.txt `  и команды, занимающие несколько строк    `echo This is \   a very long \   line  Выполнение каждого выражения в shellpy возвращается объект класса Result    result = `ls -l  Это можно быть либо Result либо InteractiveResult (Ссылки на гитхаб с документацией, можно и потом посмотреть :) ). Давайте начнем с простого результата. Из него можно легко получить код возврата выполненной команды    result = `ls -l print result.returncode  И текст из stdout и stderr    result = `ls -l result_text = result.stdout result_error = result.stderr  Можно также пробежаться по всем строкам stdout выполненной команды в цикле    result = `ls -l for line in result:     print line.upper()  и так далее.     Для результата есть также еще очень много синтаксического сахара. Например, мы можем легко проверить, что код возврата выполняемой команды равен нулю    result = `ls -l if result:     print 'Return code for ls -l was 0'  Или же более простым способом получить текст из stdout    result = `ls -l print result  Все вышеперечисленное — это обзор синтаксиса вкратце, чтобы просто понять основную идею и не грузить вас всеми-всеми деталями. Там есть еще много чего и для интерактивного взаимодействия с выполняемыми командами, для управления исполнением команд. Но это все детали, в которые можно окунуться в документации (на английском языке), если сама идея вам покажется интересной.    Это ж не валидный синтаксис Python получается, как все работает то?  Магия конечно, как еще :) Да, друзья мои, мне пришлось использовать препроцессинг, каюсь, но другого способа я не нашел. Я видел другие библиотеки, которые делают нечто подобное, не нарушая синтаксиса языка вроде    from sh import ifconfig print(ifconfig(""wlan0""))  Но меня такой синтаксис не устраивал, поскольку даже несмотря на сложности, хотелось получить best user experience ©, а для меня это значит насколько это возможно простое и близкое к его величеству Шеллу написание команд.     Знакомый с темой читатель спросит, чем IPython то тебя не устроил, там ж почти как у тебя только значок другой ставить надо, может ты просто велосипедист, которому лень заглянуть в поисковик? И правда он выглядит вот так:    lines = !ls -l  Я его пытался использовать но встретил пару серьезных проблем, с которыми ужиться не смог. Самая главная из них, то что нет простого импорта как в Python. То есть ты не можешь написать какой-то код на самом ipython и легко его переиспользовать в других местах. Невозможно написать для своего ipython модуля    import myipythomodule  и чтобы все сразу заработало как в сказке. Единственный способ переиспользовать скрипт, это выполнить его. После выполнения в окружении у тебя появляются все функции и переменные, объявленные в выполняемом файле. Не кошерно на мой взгляд.    В shellpy код переиспользуется легко и импортируется точно так же как и в обычном python. Предположим у нас есть модуль common в котором мы храним очень полезный код. Заглянем в директорию с этим модулем    ls common/ common.spy  __init__.spy  Итак, что у нас тут есть, ну во первых init, но с расширением .spy. Это и является отличительной чертой spy модуля от обычного. Посмотрим также внутрь файла common.spy, что там интересного     def common_func():     return `echo 5  Мы видим что тут объявлена функция, которая внутри себя использует shellpy синтаксис чтобы вернуть результат выполнения `echo 5. Как этот модуль используется в коде? А вот как    from common.common import common_func  print('Result of imported function is ' + str(common_func()))  Видите? Как в обычном Python, просто взяли и заимпортировали.    Как же все работает. Это работает с помощью PEP 0302 — New Import Hooks. Когда вы импортируете что-то в своем коде то вначале Python спрашивает у хука, нет ли тут чего-то твоего, хук просматривает PYTHONPATH на наличие файлов *.spy или модулей shellpython. Если ничего нет, то так и говорит: ""Ничего нету, импортируй сам"". Если же он находит что-то там, то хук занимается импортом самостоятельно. А именно, он делает препроцессинг файла в обычный python и складывает все это добро в temp директорию операционной системы. Записав новый Python файл или модуль он добавляет его в PYTHONPATH и за дело берется уже самый обыкновенный импорт.    Давайте же скорее посмотрим на какой-нибудь пример  Этот скрипт скачивает аватар юзера Python с Github и кладет его в temp директорию        import json     import os     import tempfile      # с помощью curl получает ответ от апи гитхаба     answer = `curl https://api.github.com/users/python      # синтаксический сахар чтобы сравнить результат выполнение с нулем     if answer:         answer_json = json.loads(answer.stdout)         avatar_url = answer_json['avatar_url']          destination = os.path.join(tempfile.gettempdir(), 'python.png')          # в этот раз скачиваем саму картинку         result = `curl {avatar_url} > {destination}         if result:             # если проблем не возникло, то показываем картинку              p`ls -l {destination}         else:             print('Failed to download avatar')          print('Avatar downloaded')     else:         print('Failed to access github api')  Красота...    Установка  Shellpython можно установить двумя способами: pip install shellpy или склонировав репозиторий и выполнив setup.py install. После этого у вас появится утилита shellpy.    Запустим же что-нибудь  После установки можно потестировать shellpython на примерах, которые доступны прямо в репозитории.    shellpy example/curl.spy  shellpy example/git.spy  Также здесь есть allinone примеры, которые называются так, потому что тестируют все-все функции, которые есть в shellpy. Загляните туда, чтобы лучше узнать что же там еще такого есть, либо просто выполните    shellpy example/allinone/test.spy  Для третьего Python команда выглядит вот так    shellpy example/allinone/test3.spy  Совместимость  Это работает на Linux и должно работать на Mac для Python 2.x и 3.x. На виндовсе пока не работает, но проблем никаких для работы нет, так как все писалось с использованием кроссплатформенных библиотек и ничего платформоспецифичного в коде нет. Просто не дошли руки еще, чтобы потестировать на виндовсе. Мака у меня тоже нет, но вроде у друга работало :) Если у вас есть мак и у вас все нормально, скажите пожалуйста.    Если найдете проблемы — пишите в коммент, либо сюда  либо телеграфируйте как-нибудь :)    Документация (на английском)  Wiki    Можно ли законтрибьютить  Конечно :)    Оно мне ничего в продакшене не разломает?  Сейчас версия 0.4.0, это не стейбл и продакшн процессы пока лучше не завязывать на скрипт, подождав пока все отладится. Но в девелопменте, CI можно использовать вполне. Все это покрыто тестами и работает :)     P.s.  Пишите ваши отзывы об идее в целом и о реализации в частности, а также о проблемах, пожеланиях, всех рад услышать :) Заводите Issues еще в гитхабе, там их уже много :)","python, shell, bash, bash scripting",Пишем shell скрипты на Python и можно ли заменить им Bash
"Разработчики одного из известных дистрибутивов Linux под названием Mint сообщили в блоге, что их сервер был скомпрометирован, а ISO-дистрибутивы ОС подверглись модификации (backdoored). Указывается, что стоит обратить внимание на скачанные с сервера загрузки дистрибутивы 20 февраля. По данным разработчиков, скомпрометированным оказался дистрибутив версии Linux Mint 17.3 Cinnamon.         Вредоносные дистрибутивы были размещены по IP-адресу 5.104.175.212, а сам бэкдор обращается по URL-адресу absentvodka.com. Ниже указаны инструкции проверки скачанного дистрибутива.    Для проверки загруженного дистрибутива следует сравнить его сумму MD5 c соответствующим значением легитимного дистрибутива.    6e7f7e03500747c6c3bfece2c9c8394f linuxmint-17.3-cinnamon-32bit.iso  e71a2aad8b58605e906dbea444dc4983 linuxmint-17.3-cinnamon-64bit.iso  30fef1aa1134c5f3778c77c4417f7238 linuxmint-17.3-cinnamon-nocodecs-32bit.iso  3406350a87c201cdca0927b1bc7c2ccd linuxmint-17.3-cinnamon-nocodecs-64bit.iso  df38af96e99726bb0a1ef3e5cd47563d linuxmint-17.3-cinnamon-oem-64bit.iso    На вредоносный вариант дистрибутива также указывает присутствие файла /var/lib/man.cy в установленной ОС. В случае установленной вредоносной версии, следует отключить компьютер от сети, сделать резервное копирование необходимых данных, и переустановить ОС. После этого рекомендуется изменить учетные данные своих сервисов.","security, linux",Дистрибутивы Linux Mint оказались скомпрометированы
